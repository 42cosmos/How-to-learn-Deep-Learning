{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reindex(np.random.permutation(data.index))\n",
    "msk = np.random.rand(len(data)) < 0.5\n",
    "labels = []\n",
    "labels = data['Species'].copy()\n",
    "#test_labels = test['Species'].copy()\n",
    "#df = df.drop(df.columns[[0, 5]], axis=1)\n",
    "data = data.drop(data.columns[[0, 5]], axis=1)\n",
    "labels = pd.get_dummies(labels, columns=['Species'])\n",
    "train_labels = np.array(labels[msk], dtype=np.float32)\n",
    "test_labels = np.array(labels[~msk], dtype=np.float32)\n",
    "train_data = np.array(data[msk], dtype=np.float32)\n",
    "test_data = np.array(data[~msk], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tf.input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tf.input_data(shape=[None, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tf.fully_connected(net, 8, activation='relu')\n",
    "net = tf.fully_connected(net, 3, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tf.regression(net)\n",
    "model = tf.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: ZX5JWJ\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 76\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.064s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 76/76\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.98894\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 002 | loss: 0.98894 - acc: 0.2724 -- iter: 76/76\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m1.07861\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 003 | loss: 1.07861 - acc: 0.4478 -- iter: 76/76\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m1.09355\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 004 | loss: 1.09355 - acc: 0.3784 -- iter: 76/76\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m1.09666\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 005 | loss: 1.09666 - acc: 0.3624 -- iter: 76/76\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m1.09740\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 006 | loss: 1.09740 - acc: 0.3578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m1.09750\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 007 | loss: 1.09750 - acc: 0.3563 -- iter: 76/76\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m1.09741\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 008 | loss: 1.09741 - acc: 0.3557 -- iter: 76/76\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m1.09725\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 009 | loss: 1.09725 - acc: 0.3555 -- iter: 76/76\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m1.09705\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 010 | loss: 1.09705 - acc: 0.3554 -- iter: 76/76\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m1.09684\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 011 | loss: 1.09684 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m1.09661\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 012 | loss: 1.09661 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m1.09636\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 013 | loss: 1.09636 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m1.09610\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 014 | loss: 1.09610 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.09583\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 015 | loss: 1.09583 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m1.09554\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 016 | loss: 1.09554 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m1.09524\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 017 | loss: 1.09524 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.09492\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 018 | loss: 1.09492 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m1.09458\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 019 | loss: 1.09458 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m1.09422\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 020 | loss: 1.09422 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m1.09384\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 021 | loss: 1.09384 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m1.09345\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 022 | loss: 1.09345 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m1.09303\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 023 | loss: 1.09303 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m1.09379\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 024 | loss: 1.09379 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m1.09301\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 025 | loss: 1.09301 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m1.09231\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 026 | loss: 1.09231 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.09166\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 027 | loss: 1.09166 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.09103\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 028 | loss: 1.09103 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.09042\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 029 | loss: 1.09042 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m1.08979\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 030 | loss: 1.08979 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m1.08916\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 031 | loss: 1.08916 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m1.08852\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 032 | loss: 1.08852 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m1.08786\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 033 | loss: 1.08786 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m1.08718\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 034 | loss: 1.08718 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m1.08647\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 035 | loss: 1.08647 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m1.08575\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 036 | loss: 1.08575 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m1.08499\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 037 | loss: 1.08499 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m1.08421\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 038 | loss: 1.08421 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m1.08341\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 039 | loss: 1.08341 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m1.08257\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 040 | loss: 1.08257 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m1.08171\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 041 | loss: 1.08171 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m1.08082\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 042 | loss: 1.08082 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m1.07991\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 043 | loss: 1.07991 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m1.07896\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 044 | loss: 1.07896 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m1.07798\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 045 | loss: 1.07798 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.07698\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 046 | loss: 1.07698 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.07595\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 047 | loss: 1.07595 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.07932\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 048 | loss: 1.07932 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.07755\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 049 | loss: 1.07755 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.08075\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 050 | loss: 1.08075 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.07845\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 051 | loss: 1.07845 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.08098\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 052 | loss: 1.08098 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.07838\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 053 | loss: 1.07838 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.08051\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 054 | loss: 1.08051 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.07773\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 055 | loss: 1.07773 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.07521\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 056 | loss: 1.07521 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.07289\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 057 | loss: 1.07289 - acc: 0.3553 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.07075\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 058 | loss: 1.07075 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m1.06873\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 059 | loss: 1.06873 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m1.07231\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 060 | loss: 1.07231 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m1.06978\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 061 | loss: 1.06978 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m1.06742\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 062 | loss: 1.06742 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m1.06520\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 063 | loss: 1.06520 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m1.06309\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 064 | loss: 1.06309 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m1.06106\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 065 | loss: 1.06106 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m1.05910\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 066 | loss: 1.05910 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m1.05719\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 067 | loss: 1.05719 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m1.06128\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 068 | loss: 1.06128 - acc: 0.3553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m1.05875\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 069 | loss: 1.05875 - acc: 0.3583 -- iter: 76/76\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m1.05634\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 070 | loss: 1.05634 - acc: 0.3625 -- iter: 76/76\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m1.05402\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 071 | loss: 1.05402 - acc: 0.3677 -- iter: 76/76\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m1.05178\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 072 | loss: 1.05178 - acc: 0.3722 -- iter: 76/76\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m1.04960\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 073 | loss: 1.04960 - acc: 0.3762 -- iter: 76/76\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m1.04746\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 074 | loss: 1.04746 - acc: 0.3826 -- iter: 76/76\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m1.04535\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 075 | loss: 1.04535 - acc: 0.3896 -- iter: 76/76\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m1.04327\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 076 | loss: 1.04327 - acc: 0.4014 -- iter: 76/76\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m1.04120\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 077 | loss: 1.04120 - acc: 0.4146 -- iter: 76/76\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m1.03914\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 078 | loss: 1.03914 - acc: 0.4277 -- iter: 76/76\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m1.03707\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 079 | loss: 1.03707 - acc: 0.4393 -- iter: 76/76\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m1.04384\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 080 | loss: 1.04384 - acc: 0.4280 -- iter: 76/76\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m1.04090\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 081 | loss: 1.04090 - acc: 0.4472 -- iter: 76/76\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m1.03808\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 082 | loss: 1.03808 - acc: 0.4670 -- iter: 76/76\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m1.03531\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 083 | loss: 1.03531 - acc: 0.4848 -- iter: 76/76\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m1.03259\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 084 | loss: 1.03259 - acc: 0.5008 -- iter: 76/76\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m1.02991\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 085 | loss: 1.02991 - acc: 0.5152 -- iter: 76/76\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m1.02726\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 086 | loss: 1.02726 - acc: 0.5281 -- iter: 76/76\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m1.02463\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 087 | loss: 1.02463 - acc: 0.5398 -- iter: 76/76\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m1.02201\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 088 | loss: 1.02201 - acc: 0.5516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m1.01939\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 089 | loss: 1.01939 - acc: 0.5622 -- iter: 76/76\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m1.01678\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 090 | loss: 1.01678 - acc: 0.5718 -- iter: 76/76\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m1.01417\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 091 | loss: 1.01417 - acc: 0.5804 -- iter: 76/76\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m1.01155\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 092 | loss: 1.01155 - acc: 0.5881 -- iter: 76/76\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m1.00892\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 093 | loss: 1.00892 - acc: 0.5951 -- iter: 76/76\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m1.00628\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 094 | loss: 1.00628 - acc: 0.6014 -- iter: 76/76\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m1.00363\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 095 | loss: 1.00363 - acc: 0.6071 -- iter: 76/76\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m1.00096\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 096 | loss: 1.00096 - acc: 0.6121 -- iter: 76/76\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.99827\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 097 | loss: 0.99827 - acc: 0.6167 -- iter: 76/76\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m1.00864\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 098 | loss: 1.00864 - acc: 0.5919 -- iter: 76/76\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m1.00464\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 099 | loss: 1.00464 - acc: 0.5985 -- iter: 76/76\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m1.00078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 100 | loss: 1.00078 - acc: 0.6044 -- iter: 76/76\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.99703\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 101 | loss: 0.99703 - acc: 0.6098 -- iter: 76/76\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.99339\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 102 | loss: 0.99339 - acc: 0.6146 -- iter: 76/76\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.98983\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 103 | loss: 0.98983 - acc: 0.6189 -- iter: 76/76\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.98635\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 104 | loss: 0.98635 - acc: 0.6228 -- iter: 76/76\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.98292\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 105 | loss: 0.98292 - acc: 0.6263 -- iter: 76/76\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.99697\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 106 | loss: 0.99697 - acc: 0.5926 -- iter: 76/76\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.99194\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 107 | loss: 0.99194 - acc: 0.5992 -- iter: 76/76\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.98715\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 108 | loss: 0.98715 - acc: 0.6050 -- iter: 76/76\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.98257\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 109 | loss: 0.98257 - acc: 0.6103 -- iter: 76/76\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.97818\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 110 | loss: 0.97818 - acc: 0.6151 -- iter: 76/76\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.97395\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 111 | loss: 0.97395 - acc: 0.6194 -- iter: 76/76\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.96985\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 112 | loss: 0.96985 - acc: 0.6232 -- iter: 76/76\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.96587\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 113 | loss: 0.96587 - acc: 0.6267 -- iter: 76/76\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.96200\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 114 | loss: 0.96200 - acc: 0.6298 -- iter: 76/76\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.95822\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 115 | loss: 0.95822 - acc: 0.6326 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.97390\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 116 | loss: 0.97390 - acc: 0.6036 -- iter: 76/76\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.96836\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 117 | loss: 0.96836 - acc: 0.6090 -- iter: 76/76\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.96311\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 118 | loss: 0.96311 - acc: 0.6139 -- iter: 76/76\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.95810\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 119 | loss: 0.95810 - acc: 0.6183 -- iter: 76/76\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.95331\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 120 | loss: 0.95331 - acc: 0.6222 -- iter: 76/76\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.94870\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 121 | loss: 0.94870 - acc: 0.6258 -- iter: 76/76\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.94427\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 122 | loss: 0.94427 - acc: 0.6290 -- iter: 76/76\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.93998\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 123 | loss: 0.93998 - acc: 0.6319 -- iter: 76/76\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.93582\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 124 | loss: 0.93582 - acc: 0.6345 -- iter: 76/76\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.93177\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 125 | loss: 0.93177 - acc: 0.6368 -- iter: 76/76\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.92781\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 126 | loss: 0.92781 - acc: 0.6389 -- iter: 76/76\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.92394\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 127 | loss: 0.92394 - acc: 0.6408 -- iter: 76/76\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.92015\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 128 | loss: 0.92015 - acc: 0.6425 -- iter: 76/76\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.91642\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 129 | loss: 0.91642 - acc: 0.6441 -- iter: 76/76\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.91274\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 130 | loss: 0.91274 - acc: 0.6455 -- iter: 76/76\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.90912\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 131 | loss: 0.90912 - acc: 0.6467 -- iter: 76/76\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.90553\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 132 | loss: 0.90553 - acc: 0.6478 -- iter: 76/76\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.90198\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 133 | loss: 0.90198 - acc: 0.6488 -- iter: 76/76\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.89847\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 134 | loss: 0.89847 - acc: 0.6497 -- iter: 76/76\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.89498\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 135 | loss: 0.89498 - acc: 0.6506 -- iter: 76/76\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.89152\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 136 | loss: 0.89152 - acc: 0.6513 -- iter: 76/76\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.88808\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 137 | loss: 0.88808 - acc: 0.6519 -- iter: 76/76\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.88466\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 138 | loss: 0.88466 - acc: 0.6525 -- iter: 76/76\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.88126\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 139 | loss: 0.88126 - acc: 0.6531 -- iter: 76/76\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.87787\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 140 | loss: 0.87787 - acc: 0.6536 -- iter: 76/76\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.87449\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 141 | loss: 0.87449 - acc: 0.6540 -- iter: 76/76\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.87113\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 142 | loss: 0.87113 - acc: 0.6544 -- iter: 76/76\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.86778\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 143 | loss: 0.86778 - acc: 0.6547 -- iter: 76/76\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.86445\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 144 | loss: 0.86445 - acc: 0.6551 -- iter: 76/76\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.86112\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 145 | loss: 0.86112 - acc: 0.6553 -- iter: 76/76\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.85780\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 146 | loss: 0.85780 - acc: 0.6556 -- iter: 76/76\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.85449\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 147 | loss: 0.85449 - acc: 0.6558 -- iter: 76/76\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.85119\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 148 | loss: 0.85119 - acc: 0.6560 -- iter: 76/76\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.84790\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 149 | loss: 0.84790 - acc: 0.6562 -- iter: 76/76\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.84462\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 150 | loss: 0.84462 - acc: 0.6564 -- iter: 76/76\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.84135\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 151 | loss: 0.84135 - acc: 0.6565 -- iter: 76/76\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.83809\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 152 | loss: 0.83809 - acc: 0.6567 -- iter: 76/76\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.83484\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 153 | loss: 0.83484 - acc: 0.6568 -- iter: 76/76\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.83159\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 154 | loss: 0.83159 - acc: 0.6569 -- iter: 76/76\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.82836\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 155 | loss: 0.82836 - acc: 0.6570 -- iter: 76/76\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.82514\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 156 | loss: 0.82514 - acc: 0.6571 -- iter: 76/76\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.82192\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 157 | loss: 0.82192 - acc: 0.6572 -- iter: 76/76\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.81872\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 158 | loss: 0.81872 - acc: 0.6572 -- iter: 76/76\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.81553\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 159 | loss: 0.81553 - acc: 0.6573 -- iter: 76/76\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.81236\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 160 | loss: 0.81236 - acc: 0.6574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.80919\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 161 | loss: 0.80919 - acc: 0.6574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.80603\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 162 | loss: 0.80603 - acc: 0.6575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.80289\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 163 | loss: 0.80289 - acc: 0.6575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.79976\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 164 | loss: 0.79976 - acc: 0.6575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.79665\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 165 | loss: 0.79665 - acc: 0.6576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.79355\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 166 | loss: 0.79355 - acc: 0.6576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.79046\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 167 | loss: 0.79046 - acc: 0.6576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.78739\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 168 | loss: 0.78739 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.78433\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 169 | loss: 0.78433 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.78129\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 170 | loss: 0.78129 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.77826\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 171 | loss: 0.77826 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.77525\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 172 | loss: 0.77525 - acc: 0.6577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.77225\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 173 | loss: 0.77225 - acc: 0.6578 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.76927\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 174 | loss: 0.76927 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.76631\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 175 | loss: 0.76631 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.76336\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 176 | loss: 0.76336 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.76043\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 177 | loss: 0.76043 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.75752\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 178 | loss: 0.75752 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.75462\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 179 | loss: 0.75462 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.75175\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 180 | loss: 0.75175 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.74889\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 181 | loss: 0.74889 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.74605\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 182 | loss: 0.74605 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.74322\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 183 | loss: 0.74322 - acc: 0.6578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.79090\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 184 | loss: 0.79090 - acc: 0.6276 -- iter: 76/76\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.78312\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 185 | loss: 0.78312 - acc: 0.6306 -- iter: 76/76\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.82882\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 186 | loss: 0.82882 - acc: 0.6005 -- iter: 76/76\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.81686\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 187 | loss: 0.81686 - acc: 0.6062 -- iter: 76/76\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.80594\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 188 | loss: 0.80594 - acc: 0.6114 -- iter: 76/76\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.79592\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 189 | loss: 0.79592 - acc: 0.6160 -- iter: 76/76\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.78673\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 190 | loss: 0.78673 - acc: 0.6202 -- iter: 76/76\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.77826\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 191 | loss: 0.77826 - acc: 0.6240 -- iter: 76/76\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.82509\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 192 | loss: 0.82509 - acc: 0.5997 -- iter: 76/76\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.81245\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 193 | loss: 0.81245 - acc: 0.6056 -- iter: 76/76\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.80091\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 194 | loss: 0.80091 - acc: 0.6108 -- iter: 76/76\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.79036\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 195 | loss: 0.79036 - acc: 0.6155 -- iter: 76/76\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.78069\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 196 | loss: 0.78069 - acc: 0.6197 -- iter: 76/76\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.77181\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 197 | loss: 0.77181 - acc: 0.6236 -- iter: 76/76\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.76363\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 198 | loss: 0.76363 - acc: 0.6270 -- iter: 76/76\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.75607\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 199 | loss: 0.75607 - acc: 0.6301 -- iter: 76/76\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.74908\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 200 | loss: 0.74908 - acc: 0.6329 -- iter: 76/76\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.74258\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 201 | loss: 0.74258 - acc: 0.6354 -- iter: 76/76\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.73653\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 202 | loss: 0.73653 - acc: 0.6376 -- iter: 76/76\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.73089\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 203 | loss: 0.73089 - acc: 0.6396 -- iter: 76/76\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.72561\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 204 | loss: 0.72561 - acc: 0.6415 -- iter: 76/76\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.72065\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 205 | loss: 0.72065 - acc: 0.6431 -- iter: 76/76\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.71599\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 206 | loss: 0.71599 - acc: 0.6446 -- iter: 76/76\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.71158\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 207 | loss: 0.71158 - acc: 0.6459 -- iter: 76/76\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.70741\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 208 | loss: 0.70741 - acc: 0.6471 -- iter: 76/76\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.70345\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 209 | loss: 0.70345 - acc: 0.6482 -- iter: 76/76\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.69967\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 210 | loss: 0.69967 - acc: 0.6492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.69607\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 211 | loss: 0.69607 - acc: 0.6500 -- iter: 76/76\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.69261\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 212 | loss: 0.69261 - acc: 0.6508 -- iter: 76/76\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.68929\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 213 | loss: 0.68929 - acc: 0.6515 -- iter: 76/76\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.68610\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 214 | loss: 0.68610 - acc: 0.6522 -- iter: 76/76\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.68302\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 215 | loss: 0.68302 - acc: 0.6527 -- iter: 76/76\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.68004\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 216 | loss: 0.68004 - acc: 0.6533 -- iter: 76/76\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.67715\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 217 | loss: 0.67715 - acc: 0.6537 -- iter: 76/76\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.73812\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 218 | loss: 0.73812 - acc: 0.6212 -- iter: 76/76\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.72907\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 219 | loss: 0.72907 - acc: 0.6249 -- iter: 76/76\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.72077\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 220 | loss: 0.72077 - acc: 0.6295 -- iter: 76/76\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.71314\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 221 | loss: 0.71314 - acc: 0.6337 -- iter: 76/76\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.70612\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 222 | loss: 0.70612 - acc: 0.6374 -- iter: 76/76\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.69962\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 223 | loss: 0.69962 - acc: 0.6408 -- iter: 76/76\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.69361\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 224 | loss: 0.69361 - acc: 0.6451 -- iter: 76/76\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.68803\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 225 | loss: 0.68803 - acc: 0.6490 -- iter: 76/76\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.68283\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 226 | loss: 0.68283 - acc: 0.6525 -- iter: 76/76\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.67798\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 227 | loss: 0.67798 - acc: 0.6557 -- iter: 76/76\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.74360\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 228 | loss: 0.74360 - acc: 0.6244 -- iter: 76/76\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.73237\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 229 | loss: 0.73237 - acc: 0.6303 -- iter: 76/76\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.72213\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 230 | loss: 0.72213 - acc: 0.6357 -- iter: 76/76\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.71278\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 231 | loss: 0.71278 - acc: 0.6406 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.75514\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 232 | loss: 0.75514 - acc: 0.6199 -- iter: 76/76\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.74224\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 233 | loss: 0.74224 - acc: 0.6264 -- iter: 76/76\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.73051\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 234 | loss: 0.73051 - acc: 0.6321 -- iter: 76/76\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.71983\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 235 | loss: 0.71983 - acc: 0.6374 -- iter: 76/76\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.77720\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 236 | loss: 0.77720 - acc: 0.6118 -- iter: 76/76\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.76163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 237 | loss: 0.76163 - acc: 0.6190 -- iter: 76/76\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.74753\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 238 | loss: 0.74753 - acc: 0.6255 -- iter: 76/76\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.73473\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 239 | loss: 0.73473 - acc: 0.6314 -- iter: 76/76\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.72310\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 240 | loss: 0.72310 - acc: 0.6367 -- iter: 76/76\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.71251\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 241 | loss: 0.71251 - acc: 0.6414 -- iter: 76/76\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.70285\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 242 | loss: 0.70285 - acc: 0.6457 -- iter: 76/76\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.69402\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 243 | loss: 0.69402 - acc: 0.6496 -- iter: 76/76\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.76744\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 244 | loss: 0.76744 - acc: 0.6188 -- iter: 76/76\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.75193\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 245 | loss: 0.75193 - acc: 0.6254 -- iter: 76/76\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.81832\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 246 | loss: 0.81832 - acc: 0.5957 -- iter: 76/76\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.79758\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 247 | loss: 0.79758 - acc: 0.6046 -- iter: 76/76\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.77885\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 248 | loss: 0.77885 - acc: 0.6138 -- iter: 76/76\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.76193\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 249 | loss: 0.76193 - acc: 0.6222 -- iter: 76/76\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.74662\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 250 | loss: 0.74662 - acc: 0.6297 -- iter: 76/76\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.73275\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 251 | loss: 0.73275 - acc: 0.6365 -- iter: 76/76\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.72017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 252 | loss: 0.72017 - acc: 0.6426 -- iter: 76/76\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.70874\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 253 | loss: 0.70874 - acc: 0.6481 -- iter: 76/76\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.69834\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 254 | loss: 0.69834 - acc: 0.6530 -- iter: 76/76\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.68886\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 255 | loss: 0.68886 - acc: 0.6574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.75130\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 256 | loss: 0.75130 - acc: 0.6285 -- iter: 76/76\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.73632\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 257 | loss: 0.73632 - acc: 0.6394 -- iter: 76/76\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.72275\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 258 | loss: 0.72275 - acc: 0.6491 -- iter: 76/76\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.71043\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 259 | loss: 0.71043 - acc: 0.6579 -- iter: 76/76\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.69925\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 260 | loss: 0.69925 - acc: 0.6658 -- iter: 76/76\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.68907\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 261 | loss: 0.68907 - acc: 0.6729 -- iter: 76/76\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.76832\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 262 | loss: 0.76832 - acc: 0.6306 -- iter: 76/76\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.75106\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 263 | loss: 0.75106 - acc: 0.6534 -- iter: 76/76\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.73545\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 264 | loss: 0.73545 - acc: 0.6534 -- iter: 76/76\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.72132\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 265 | loss: 0.72132 - acc: 0.6644 -- iter: 76/76\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.70852\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 266 | loss: 0.70852 - acc: 0.6743 -- iter: 76/76\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.69690\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 267 | loss: 0.69690 - acc: 0.6832 -- iter: 76/76\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.68634\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 268 | loss: 0.68634 - acc: 0.6912 -- iter: 76/76\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.67673\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 269 | loss: 0.67673 - acc: 0.6984 -- iter: 76/76\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.66797\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 270 | loss: 0.66797 - acc: 0.7048 -- iter: 76/76\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.65996\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 271 | loss: 0.65996 - acc: 0.7107 -- iter: 76/76\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.65263\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 272 | loss: 0.65263 - acc: 0.7159 -- iter: 76/76\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.64591\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 273 | loss: 0.64591 - acc: 0.7220 -- iter: 76/76\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.70811\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 274 | loss: 0.70811 - acc: 0.6853 -- iter: 76/76\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.69562\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 275 | loss: 0.69562 - acc: 0.6957 -- iter: 76/76\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.76958\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 276 | loss: 0.76958 - acc: 0.6617 -- iter: 76/76\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.75081\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 277 | loss: 0.75081 - acc: 0.6744 -- iter: 76/76\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.83500\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 278 | loss: 0.83500 - acc: 0.6320 -- iter: 76/76\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.80963\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 279 | loss: 0.80963 - acc: 0.6517 -- iter: 76/76\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.78678\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 280 | loss: 0.78678 - acc: 0.6694 -- iter: 76/76\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.76619\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 281 | loss: 0.76619 - acc: 0.6854 -- iter: 76/76\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.74761\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 282 | loss: 0.74761 - acc: 0.6997 -- iter: 76/76\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.73083\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 283 | loss: 0.73083 - acc: 0.7127 -- iter: 76/76\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.71565\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 284 | loss: 0.71565 - acc: 0.7243 -- iter: 76/76\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.70192\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 285 | loss: 0.70192 - acc: 0.7347 -- iter: 76/76\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.68947\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 286 | loss: 0.68947 - acc: 0.7442 -- iter: 76/76\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.67816\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 287 | loss: 0.67816 - acc: 0.7526 -- iter: 76/76\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.75717\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 288 | loss: 0.75717 - acc: 0.7103 -- iter: 76/76\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.73894\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 289 | loss: 0.73894 - acc: 0.7221 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.72247\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 290 | loss: 0.72247 - acc: 0.7328 -- iter: 76/76\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.70758\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 291 | loss: 0.70758 - acc: 0.7424 -- iter: 76/76\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.69409\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 292 | loss: 0.69409 - acc: 0.7511 -- iter: 76/76\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.68188\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 293 | loss: 0.68188 - acc: 0.7589 -- iter: 76/76\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.67079\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 294 | loss: 0.67079 - acc: 0.7659 -- iter: 76/76\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.66072\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 295 | loss: 0.66072 - acc: 0.7722 -- iter: 76/76\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.65156\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 296 | loss: 0.65156 - acc: 0.7779 -- iter: 76/76\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.64321\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 297 | loss: 0.64321 - acc: 0.7830 -- iter: 76/76\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.63559\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 298 | loss: 0.63559 - acc: 0.7876 -- iter: 76/76\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.62863\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 299 | loss: 0.62863 - acc: 0.7917 -- iter: 76/76\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.62225\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 300 | loss: 0.62225 - acc: 0.7954 -- iter: 76/76\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.61639\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 301 | loss: 0.61639 - acc: 0.7988 -- iter: 76/76\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.61100\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 302 | loss: 0.61100 - acc: 0.8018 -- iter: 76/76\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.60604\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 303 | loss: 0.60604 - acc: 0.8045 -- iter: 76/76\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.60145\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 304 | loss: 0.60145 - acc: 0.8070 -- iter: 76/76\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.59720\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 305 | loss: 0.59720 - acc: 0.8092 -- iter: 76/76\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.66634\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 306 | loss: 0.66634 - acc: 0.7677 -- iter: 76/76\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.65539\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 307 | loss: 0.65539 - acc: 0.7738 -- iter: 76/76\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.64545\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 308 | loss: 0.64545 - acc: 0.7793 -- iter: 76/76\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.63640\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 309 | loss: 0.63640 - acc: 0.7843 -- iter: 76/76\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.62816\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 310 | loss: 0.62816 - acc: 0.7888 -- iter: 76/76\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.62064\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 311 | loss: 0.62064 - acc: 0.7928 -- iter: 76/76\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.61377\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 312 | loss: 0.61377 - acc: 0.7964 -- iter: 76/76\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.60748\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 313 | loss: 0.60748 - acc: 0.7997 -- iter: 76/76\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.60171\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 314 | loss: 0.60171 - acc: 0.8039 -- iter: 76/76\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.59640\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 315 | loss: 0.59640 - acc: 0.8077 -- iter: 76/76\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.59151\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 316 | loss: 0.59151 - acc: 0.8112 -- iter: 76/76\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.58700\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 317 | loss: 0.58700 - acc: 0.8143 -- iter: 76/76\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.58283\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 318 | loss: 0.58283 - acc: 0.8170 -- iter: 76/76\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.57895\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 319 | loss: 0.57895 - acc: 0.8195 -- iter: 76/76\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.57535\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 320 | loss: 0.57535 - acc: 0.8218 -- iter: 76/76\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.57198\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 321 | loss: 0.57198 - acc: 0.8238 -- iter: 76/76\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.56884\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 322 | loss: 0.56884 - acc: 0.8257 -- iter: 76/76\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.56589\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 323 | loss: 0.56589 - acc: 0.8273 -- iter: 76/76\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.56312\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 324 | loss: 0.56312 - acc: 0.8288 -- iter: 76/76\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.56050\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 325 | loss: 0.56050 - acc: 0.8301 -- iter: 76/76\n",
      "--\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.55803\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 326 | loss: 0.55803 - acc: 0.8313 -- iter: 76/76\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.55569\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 327 | loss: 0.55569 - acc: 0.8324 -- iter: 76/76\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.55346\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 328 | loss: 0.55346 - acc: 0.8334 -- iter: 76/76\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.55134\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 329 | loss: 0.55134 - acc: 0.8342 -- iter: 76/76\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.54930\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 330 | loss: 0.54930 - acc: 0.8350 -- iter: 76/76\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.54736\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 331 | loss: 0.54736 - acc: 0.8357 -- iter: 76/76\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.54549\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 332 | loss: 0.54549 - acc: 0.8364 -- iter: 76/76\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.54369\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 333 | loss: 0.54369 - acc: 0.8369 -- iter: 76/76\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.54195\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 334 | loss: 0.54195 - acc: 0.8375 -- iter: 76/76\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.54027\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 335 | loss: 0.54027 - acc: 0.8379 -- iter: 76/76\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.63361\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 336 | loss: 0.63361 - acc: 0.7936 -- iter: 76/76\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.62257\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 337 | loss: 0.62257 - acc: 0.7985 -- iter: 76/76\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.61256\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 338 | loss: 0.61256 - acc: 0.8028 -- iter: 76/76\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.60347\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 339 | loss: 0.60347 - acc: 0.8081 -- iter: 76/76\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.59521\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 340 | loss: 0.59521 - acc: 0.8128 -- iter: 76/76\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.58768\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 341 | loss: 0.58768 - acc: 0.8170 -- iter: 76/76\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.67791\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 342 | loss: 0.67791 - acc: 0.7682 -- iter: 76/76\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.66198\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 343 | loss: 0.66198 - acc: 0.7769 -- iter: 76/76\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.64759\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 344 | loss: 0.64759 - acc: 0.7861 -- iter: 76/76\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.63458\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 345 | loss: 0.63458 - acc: 0.7943 -- iter: 76/76\n",
      "--\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.62280\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 346 | loss: 0.62280 - acc: 0.8030 -- iter: 76/76\n",
      "--\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.61213\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 347 | loss: 0.61213 - acc: 0.8109 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.60245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 348 | loss: 0.60245 - acc: 0.8180 -- iter: 76/76\n",
      "--\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.59366\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 349 | loss: 0.59366 - acc: 0.8243 -- iter: 76/76\n",
      "--\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.58566\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 350 | loss: 0.58566 - acc: 0.8287 -- iter: 76/76\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.57837\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 351 | loss: 0.57837 - acc: 0.8327 -- iter: 76/76\n",
      "--\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.67419\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 352 | loss: 0.67419 - acc: 0.7850 -- iter: 76/76\n",
      "--\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.65791\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 353 | loss: 0.65791 - acc: 0.7933 -- iter: 76/76\n",
      "--\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.64320\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 354 | loss: 0.64320 - acc: 0.8008 -- iter: 76/76\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.62991\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 355 | loss: 0.62991 - acc: 0.8076 -- iter: 76/76\n",
      "--\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.61788\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 356 | loss: 0.61788 - acc: 0.8137 -- iter: 76/76\n",
      "--\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.60699\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 357 | loss: 0.60699 - acc: 0.8191 -- iter: 76/76\n",
      "--\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.59711\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 358 | loss: 0.59711 - acc: 0.8241 -- iter: 76/76\n",
      "--\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.58814\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 359 | loss: 0.58814 - acc: 0.8285 -- iter: 76/76\n",
      "--\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.68483\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 360 | loss: 0.68483 - acc: 0.7812 -- iter: 76/76\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.66698\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 361 | loss: 0.66698 - acc: 0.7912 -- iter: 76/76\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.65086\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 362 | loss: 0.65086 - acc: 0.8016 -- iter: 76/76\n",
      "--\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.63631\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 363 | loss: 0.63631 - acc: 0.8109 -- iter: 76/76\n",
      "--\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.62316\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 364 | loss: 0.62316 - acc: 0.8193 -- iter: 76/76\n",
      "--\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.61126\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 365 | loss: 0.61126 - acc: 0.8268 -- iter: 76/76\n",
      "--\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.60048\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 366 | loss: 0.60048 - acc: 0.8336 -- iter: 76/76\n",
      "--\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.59072\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 367 | loss: 0.59072 - acc: 0.8397 -- iter: 76/76\n",
      "--\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.58185\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 368 | loss: 0.58185 - acc: 0.8452 -- iter: 76/76\n",
      "--\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.57379\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 369 | loss: 0.57379 - acc: 0.8502 -- iter: 76/76\n",
      "--\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.56646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 370 | loss: 0.56646 - acc: 0.8546 -- iter: 76/76\n",
      "--\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.55977\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 371 | loss: 0.55977 - acc: 0.8586 -- iter: 76/76\n",
      "--\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.55367\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 372 | loss: 0.55367 - acc: 0.8623 -- iter: 76/76\n",
      "--\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.54809\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 373 | loss: 0.54809 - acc: 0.8655 -- iter: 76/76\n",
      "--\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.54299\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 374 | loss: 0.54299 - acc: 0.8671 -- iter: 76/76\n",
      "--\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.53830\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 375 | loss: 0.53830 - acc: 0.8686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.53400\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 376 | loss: 0.53400 - acc: 0.8699 -- iter: 76/76\n",
      "--\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.53003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 377 | loss: 0.53003 - acc: 0.8710 -- iter: 76/76\n",
      "--\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.52637\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 378 | loss: 0.52637 - acc: 0.8721 -- iter: 76/76\n",
      "--\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.52298\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 379 | loss: 0.52298 - acc: 0.8730 -- iter: 76/76\n",
      "--\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.51984\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 380 | loss: 0.51984 - acc: 0.8739 -- iter: 76/76\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.51692\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 381 | loss: 0.51692 - acc: 0.8760 -- iter: 76/76\n",
      "--\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.51419\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 382 | loss: 0.51419 - acc: 0.8778 -- iter: 76/76\n",
      "--\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.51165\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 383 | loss: 0.51165 - acc: 0.8795 -- iter: 76/76\n",
      "--\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.50926\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 384 | loss: 0.50926 - acc: 0.8811 -- iter: 76/76\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.50701\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 385 | loss: 0.50701 - acc: 0.8837 -- iter: 76/76\n",
      "--\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.50490\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 386 | loss: 0.50490 - acc: 0.8862 -- iter: 76/76\n",
      "--\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.50290\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 387 | loss: 0.50290 - acc: 0.8896 -- iter: 76/76\n",
      "--\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.50100\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 388 | loss: 0.50100 - acc: 0.8928 -- iter: 76/76\n",
      "--\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.49920\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 389 | loss: 0.49920 - acc: 0.8956 -- iter: 76/76\n",
      "--\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.49748\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 390 | loss: 0.49748 - acc: 0.8982 -- iter: 76/76\n",
      "--\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.49584\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 391 | loss: 0.49584 - acc: 0.9004 -- iter: 76/76\n",
      "--\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.62812\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 392 | loss: 0.62812 - acc: 0.8354 -- iter: 76/76\n",
      "--\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.61327\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 393 | loss: 0.61327 - acc: 0.8440 -- iter: 76/76\n",
      "--\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.59987\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 394 | loss: 0.59987 - acc: 0.8517 -- iter: 76/76\n",
      "--\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.58775\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 395 | loss: 0.58775 - acc: 0.8612 -- iter: 76/76\n",
      "--\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.57679\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 396 | loss: 0.57679 - acc: 0.8699 -- iter: 76/76\n",
      "--\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.56687\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 397 | loss: 0.56687 - acc: 0.8776 -- iter: 76/76\n",
      "--\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.55787\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 398 | loss: 0.55787 - acc: 0.8846 -- iter: 76/76\n",
      "--\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.54971\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 399 | loss: 0.54971 - acc: 0.8909 -- iter: 76/76\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.54230\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 400 | loss: 0.54230 - acc: 0.8978 -- iter: 76/76\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.53555\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 401 | loss: 0.53555 - acc: 0.9041 -- iter: 76/76\n",
      "--\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.52940\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 402 | loss: 0.52940 - acc: 0.9084 -- iter: 76/76\n",
      "--\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.52378\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 403 | loss: 0.52378 - acc: 0.9123 -- iter: 76/76\n",
      "--\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.51865\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 404 | loss: 0.51865 - acc: 0.9158 -- iter: 76/76\n",
      "--\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.51394\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 405 | loss: 0.51394 - acc: 0.9190 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.50962\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 406 | loss: 0.50962 - acc: 0.9218 -- iter: 76/76\n",
      "--\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.50564\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 407 | loss: 0.50564 - acc: 0.9244 -- iter: 76/76\n",
      "--\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.50198\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 408 | loss: 0.50198 - acc: 0.9267 -- iter: 76/76\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.49860\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 409 | loss: 0.49860 - acc: 0.9261 -- iter: 76/76\n",
      "--\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.49548\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 410 | loss: 0.49548 - acc: 0.9256 -- iter: 76/76\n",
      "--\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.49258\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 411 | loss: 0.49258 - acc: 0.9251 -- iter: 76/76\n",
      "--\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.48988\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 412 | loss: 0.48988 - acc: 0.9247 -- iter: 76/76\n",
      "--\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.48737\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 413 | loss: 0.48737 - acc: 0.9244 -- iter: 76/76\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.48503\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 414 | loss: 0.48503 - acc: 0.9240 -- iter: 76/76\n",
      "--\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.48283\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 415 | loss: 0.48283 - acc: 0.9237 -- iter: 76/76\n",
      "--\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.48077\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 416 | loss: 0.48077 - acc: 0.9235 -- iter: 76/76\n",
      "--\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.47882\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 417 | loss: 0.47882 - acc: 0.9232 -- iter: 76/76\n",
      "--\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.47699\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 418 | loss: 0.47699 - acc: 0.9230 -- iter: 76/76\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.47524\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 419 | loss: 0.47524 - acc: 0.9228 -- iter: 76/76\n",
      "--\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.47359\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 420 | loss: 0.47359 - acc: 0.9226 -- iter: 76/76\n",
      "--\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.47201\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 421 | loss: 0.47201 - acc: 0.9238 -- iter: 76/76\n",
      "--\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.47050\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 422 | loss: 0.47050 - acc: 0.9262 -- iter: 76/76\n",
      "--\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.46906\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 423 | loss: 0.46906 - acc: 0.9283 -- iter: 76/76\n",
      "--\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.46767\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 424 | loss: 0.46767 - acc: 0.9302 -- iter: 76/76\n",
      "--\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.46633\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 425 | loss: 0.46633 - acc: 0.9319 -- iter: 76/76\n",
      "--\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.46504\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 426 | loss: 0.46504 - acc: 0.9334 -- iter: 76/76\n",
      "--\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.46379\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 427 | loss: 0.46379 - acc: 0.9348 -- iter: 76/76\n",
      "--\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.46258\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 428 | loss: 0.46258 - acc: 0.9361 -- iter: 76/76\n",
      "--\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.46141\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 429 | loss: 0.46141 - acc: 0.9372 -- iter: 76/76\n",
      "--\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.46026\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 430 | loss: 0.46026 - acc: 0.9396 -- iter: 76/76\n",
      "--\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.45914\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 431 | loss: 0.45914 - acc: 0.9416 -- iter: 76/76\n",
      "--\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.45805\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 432 | loss: 0.45805 - acc: 0.9435 -- iter: 76/76\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.45698\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 433 | loss: 0.45698 - acc: 0.9452 -- iter: 76/76\n",
      "--\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.45593\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 434 | loss: 0.45593 - acc: 0.9468 -- iter: 76/76\n",
      "--\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.45490\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 435 | loss: 0.45490 - acc: 0.9481 -- iter: 76/76\n",
      "--\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.45388\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 436 | loss: 0.45388 - acc: 0.9494 -- iter: 76/76\n",
      "--\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.45289\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 437 | loss: 0.45289 - acc: 0.9505 -- iter: 76/76\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.45190\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 438 | loss: 0.45190 - acc: 0.9515 -- iter: 76/76\n",
      "--\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.45093\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 439 | loss: 0.45093 - acc: 0.9524 -- iter: 76/76\n",
      "--\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.44997\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 440 | loss: 0.44997 - acc: 0.9532 -- iter: 76/76\n",
      "--\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.44902\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 441 | loss: 0.44902 - acc: 0.9539 -- iter: 76/76\n",
      "--\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.44808\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 442 | loss: 0.44808 - acc: 0.9546 -- iter: 76/76\n",
      "--\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.44715\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 443 | loss: 0.44715 - acc: 0.9552 -- iter: 76/76\n",
      "--\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.44623\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 444 | loss: 0.44623 - acc: 0.9557 -- iter: 76/76\n",
      "--\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.44531\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 445 | loss: 0.44531 - acc: 0.9562 -- iter: 76/76\n",
      "--\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.44440\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 446 | loss: 0.44440 - acc: 0.9566 -- iter: 76/76\n",
      "--\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.44350\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 447 | loss: 0.44350 - acc: 0.9570 -- iter: 76/76\n",
      "--\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.44261\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 448 | loss: 0.44261 - acc: 0.9574 -- iter: 76/76\n",
      "--\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.44172\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 449 | loss: 0.44172 - acc: 0.9577 -- iter: 76/76\n",
      "--\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.44083\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 450 | loss: 0.44083 - acc: 0.9580 -- iter: 76/76\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.43995\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 451 | loss: 0.43995 - acc: 0.9595 -- iter: 76/76\n",
      "--\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.43908\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 452 | loss: 0.43908 - acc: 0.9610 -- iter: 76/76\n",
      "--\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.43820\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 453 | loss: 0.43820 - acc: 0.9622 -- iter: 76/76\n",
      "--\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.43734\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 454 | loss: 0.43734 - acc: 0.9634 -- iter: 76/76\n",
      "--\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.43647\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 455 | loss: 0.43647 - acc: 0.9644 -- iter: 76/76\n",
      "--\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.43561\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 456 | loss: 0.43561 - acc: 0.9653 -- iter: 76/76\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.43476\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 457 | loss: 0.43476 - acc: 0.9662 -- iter: 76/76\n",
      "--\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.43390\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 458 | loss: 0.43390 - acc: 0.9669 -- iter: 76/76\n",
      "--\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.43305\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 459 | loss: 0.43305 - acc: 0.9676 -- iter: 76/76\n",
      "--\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.43220\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 460 | loss: 0.43220 - acc: 0.9682 -- iter: 76/76\n",
      "--\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.43136\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 461 | loss: 0.43136 - acc: 0.9688 -- iter: 76/76\n",
      "--\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.43051\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 462 | loss: 0.43051 - acc: 0.9692 -- iter: 76/76\n",
      "--\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.42967\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 463 | loss: 0.42967 - acc: 0.9697 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.42883\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 464 | loss: 0.42883 - acc: 0.9701 -- iter: 76/76\n",
      "--\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.42800\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 465 | loss: 0.42800 - acc: 0.9705 -- iter: 76/76\n",
      "--\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.42716\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 466 | loss: 0.42716 - acc: 0.9708 -- iter: 76/76\n",
      "--\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.42633\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 467 | loss: 0.42633 - acc: 0.9711 -- iter: 76/76\n",
      "--\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.42550\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 468 | loss: 0.42550 - acc: 0.9713 -- iter: 76/76\n",
      "--\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.42467\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 469 | loss: 0.42467 - acc: 0.9716 -- iter: 76/76\n",
      "--\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.42385\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 470 | loss: 0.42385 - acc: 0.9718 -- iter: 76/76\n",
      "--\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.42302\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 471 | loss: 0.42302 - acc: 0.9720 -- iter: 76/76\n",
      "--\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.42220\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 472 | loss: 0.42220 - acc: 0.9721 -- iter: 76/76\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.42138\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 473 | loss: 0.42138 - acc: 0.9723 -- iter: 76/76\n",
      "--\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.42056\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 474 | loss: 0.42056 - acc: 0.9724 -- iter: 76/76\n",
      "--\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.41974\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 475 | loss: 0.41974 - acc: 0.9726 -- iter: 76/76\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.41892\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 476 | loss: 0.41892 - acc: 0.9727 -- iter: 76/76\n",
      "--\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.41811\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 477 | loss: 0.41811 - acc: 0.9728 -- iter: 76/76\n",
      "--\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.41729\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 478 | loss: 0.41729 - acc: 0.9729 -- iter: 76/76\n",
      "--\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.41648\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 479 | loss: 0.41648 - acc: 0.9729 -- iter: 76/76\n",
      "--\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.58590\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 480 | loss: 0.58590 - acc: 0.9033 -- iter: 76/76\n",
      "--\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.56811\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 481 | loss: 0.56811 - acc: 0.9103 -- iter: 76/76\n",
      "--\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.55207\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 482 | loss: 0.55207 - acc: 0.9167 -- iter: 76/76\n",
      "--\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.53760\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 483 | loss: 0.53760 - acc: 0.9224 -- iter: 76/76\n",
      "--\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.52453\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 484 | loss: 0.52453 - acc: 0.9275 -- iter: 76/76\n",
      "--\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.51272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 485 | loss: 0.51272 - acc: 0.9321 -- iter: 76/76\n",
      "--\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.50205\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 486 | loss: 0.50205 - acc: 0.9363 -- iter: 76/76\n",
      "--\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.49239\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 487 | loss: 0.49239 - acc: 0.9400 -- iter: 76/76\n",
      "--\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.62130\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 488 | loss: 0.62130 - acc: 0.8723 -- iter: 76/76\n",
      "--\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.59965\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 489 | loss: 0.59965 - acc: 0.8825 -- iter: 76/76\n",
      "--\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.58014\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 490 | loss: 0.58014 - acc: 0.8916 -- iter: 76/76\n",
      "--\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.56256\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 491 | loss: 0.56256 - acc: 0.8998 -- iter: 76/76\n",
      "--\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.54670\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 492 | loss: 0.54670 - acc: 0.9072 -- iter: 76/76\n",
      "--\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.53239\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 493 | loss: 0.53239 - acc: 0.9138 -- iter: 76/76\n",
      "--\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.71082\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 494 | loss: 0.71082 - acc: 0.8527 -- iter: 76/76\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.68007\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 495 | loss: 0.68007 - acc: 0.8648 -- iter: 76/76\n",
      "--\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.65241\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 496 | loss: 0.65241 - acc: 0.8757 -- iter: 76/76\n",
      "--\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.62751\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 497 | loss: 0.62751 - acc: 0.8855 -- iter: 76/76\n",
      "--\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.60509\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 498 | loss: 0.60509 - acc: 0.8943 -- iter: 76/76\n",
      "--\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.58488\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 499 | loss: 0.58488 - acc: 0.9023 -- iter: 76/76\n",
      "--\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.56667\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 500 | loss: 0.56667 - acc: 0.9094 -- iter: 76/76\n",
      "--\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m0.55023\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 501 | loss: 0.55023 - acc: 0.9158 -- iter: 76/76\n",
      "--\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m0.53539\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 502 | loss: 0.53539 - acc: 0.9216 -- iter: 76/76\n",
      "--\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m0.52199\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 503 | loss: 0.52199 - acc: 0.9268 -- iter: 76/76\n",
      "--\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m0.50987\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 504 | loss: 0.50987 - acc: 0.9315 -- iter: 76/76\n",
      "--\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m0.49891\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 505 | loss: 0.49891 - acc: 0.9357 -- iter: 76/76\n",
      "--\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m0.48900\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 506 | loss: 0.48900 - acc: 0.9395 -- iter: 76/76\n",
      "--\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m0.48002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 507 | loss: 0.48002 - acc: 0.9429 -- iter: 76/76\n",
      "--\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m0.47188\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 508 | loss: 0.47188 - acc: 0.9460 -- iter: 76/76\n",
      "--\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m0.46451\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 509 | loss: 0.46451 - acc: 0.9488 -- iter: 76/76\n",
      "--\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m0.45781\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 510 | loss: 0.45781 - acc: 0.9513 -- iter: 76/76\n",
      "--\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m0.45172\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 511 | loss: 0.45172 - acc: 0.9535 -- iter: 76/76\n",
      "--\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m0.44618\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 512 | loss: 0.44618 - acc: 0.9555 -- iter: 76/76\n",
      "--\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m0.44113\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 513 | loss: 0.44113 - acc: 0.9573 -- iter: 76/76\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m0.43652\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 514 | loss: 0.43652 - acc: 0.9590 -- iter: 76/76\n",
      "--\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m0.43230\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 515 | loss: 0.43230 - acc: 0.9604 -- iter: 76/76\n",
      "--\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m0.42843\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 516 | loss: 0.42843 - acc: 0.9618 -- iter: 76/76\n",
      "--\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m0.42488\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 517 | loss: 0.42488 - acc: 0.9630 -- iter: 76/76\n",
      "--\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m0.42161\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 518 | loss: 0.42161 - acc: 0.9640 -- iter: 76/76\n",
      "--\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m0.41860\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 519 | loss: 0.41860 - acc: 0.9650 -- iter: 76/76\n",
      "--\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m0.41582\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 520 | loss: 0.41582 - acc: 0.9659 -- iter: 76/76\n",
      "--\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m0.41324\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 521 | loss: 0.41324 - acc: 0.9666 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m0.41085\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 522 | loss: 0.41085 - acc: 0.9674 -- iter: 76/76\n",
      "--\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m0.40863\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 523 | loss: 0.40863 - acc: 0.9680 -- iter: 76/76\n",
      "--\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m0.40656\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 524 | loss: 0.40656 - acc: 0.9686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m0.40462\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 525 | loss: 0.40462 - acc: 0.9691 -- iter: 76/76\n",
      "--\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m0.40281\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 526 | loss: 0.40281 - acc: 0.9695 -- iter: 76/76\n",
      "--\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m0.40111\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 527 | loss: 0.40111 - acc: 0.9699 -- iter: 76/76\n",
      "--\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m0.39951\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 528 | loss: 0.39951 - acc: 0.9703 -- iter: 76/76\n",
      "--\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m0.39799\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 529 | loss: 0.39799 - acc: 0.9707 -- iter: 76/76\n",
      "--\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m0.39656\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 530 | loss: 0.39656 - acc: 0.9710 -- iter: 76/76\n",
      "--\n",
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m0.39519\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 531 | loss: 0.39519 - acc: 0.9712 -- iter: 76/76\n",
      "--\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m0.39389\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 532 | loss: 0.39389 - acc: 0.9715 -- iter: 76/76\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m0.39265\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 533 | loss: 0.39265 - acc: 0.9717 -- iter: 76/76\n",
      "--\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m0.39145\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 534 | loss: 0.39145 - acc: 0.9719 -- iter: 76/76\n",
      "--\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m0.39031\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 535 | loss: 0.39031 - acc: 0.9721 -- iter: 76/76\n",
      "--\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m0.38921\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 536 | loss: 0.38921 - acc: 0.9722 -- iter: 76/76\n",
      "--\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m0.38814\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 537 | loss: 0.38814 - acc: 0.9724 -- iter: 76/76\n",
      "--\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m0.38711\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 538 | loss: 0.38711 - acc: 0.9725 -- iter: 76/76\n",
      "--\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m0.38611\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 539 | loss: 0.38611 - acc: 0.9726 -- iter: 76/76\n",
      "--\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m0.38514\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 540 | loss: 0.38514 - acc: 0.9727 -- iter: 76/76\n",
      "--\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m0.38419\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 541 | loss: 0.38419 - acc: 0.9728 -- iter: 76/76\n",
      "--\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m0.56019\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 542 | loss: 0.56019 - acc: 0.9032 -- iter: 76/76\n",
      "--\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m0.54164\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 543 | loss: 0.54164 - acc: 0.9102 -- iter: 76/76\n",
      "--\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m0.52492\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 544 | loss: 0.52492 - acc: 0.9166 -- iter: 76/76\n",
      "--\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m0.50984\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 545 | loss: 0.50984 - acc: 0.9223 -- iter: 76/76\n",
      "--\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m0.49623\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 546 | loss: 0.49623 - acc: 0.9274 -- iter: 76/76\n",
      "--\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m0.48395\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 547 | loss: 0.48395 - acc: 0.9321 -- iter: 76/76\n",
      "--\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m0.47285\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 548 | loss: 0.47285 - acc: 0.9362 -- iter: 76/76\n",
      "--\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m0.46282\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 549 | loss: 0.46282 - acc: 0.9400 -- iter: 76/76\n",
      "--\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m0.45374\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 550 | loss: 0.45374 - acc: 0.9433 -- iter: 76/76\n",
      "--\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m0.44551\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 551 | loss: 0.44551 - acc: 0.9464 -- iter: 76/76\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m0.43806\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 552 | loss: 0.43806 - acc: 0.9491 -- iter: 76/76\n",
      "--\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m0.43129\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 553 | loss: 0.43129 - acc: 0.9516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m0.42514\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 554 | loss: 0.42514 - acc: 0.9538 -- iter: 76/76\n",
      "--\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m0.41954\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 555 | loss: 0.41954 - acc: 0.9558 -- iter: 76/76\n",
      "--\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m0.41444\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 556 | loss: 0.41444 - acc: 0.9576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m0.40980\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 557 | loss: 0.40980 - acc: 0.9592 -- iter: 76/76\n",
      "--\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m0.40555\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 558 | loss: 0.40555 - acc: 0.9606 -- iter: 76/76\n",
      "--\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m0.40167\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 559 | loss: 0.40167 - acc: 0.9619 -- iter: 76/76\n",
      "--\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m0.39812\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 560 | loss: 0.39812 - acc: 0.9631 -- iter: 76/76\n",
      "--\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m0.39486\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 561 | loss: 0.39486 - acc: 0.9642 -- iter: 76/76\n",
      "--\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m0.39186\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 562 | loss: 0.39186 - acc: 0.9651 -- iter: 76/76\n",
      "--\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m0.38910\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 563 | loss: 0.38910 - acc: 0.9660 -- iter: 76/76\n",
      "--\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m0.38654\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 564 | loss: 0.38654 - acc: 0.9667 -- iter: 76/76\n",
      "--\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m0.38418\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 565 | loss: 0.38418 - acc: 0.9674 -- iter: 76/76\n",
      "--\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m0.38199\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 566 | loss: 0.38199 - acc: 0.9681 -- iter: 76/76\n",
      "--\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m0.37995\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 567 | loss: 0.37995 - acc: 0.9686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m0.37805\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 568 | loss: 0.37805 - acc: 0.9691 -- iter: 76/76\n",
      "--\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m0.37627\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 569 | loss: 0.37627 - acc: 0.9696 -- iter: 76/76\n",
      "--\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m0.37461\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 570 | loss: 0.37461 - acc: 0.9700 -- iter: 76/76\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m0.37304\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 571 | loss: 0.37304 - acc: 0.9704 -- iter: 76/76\n",
      "--\n",
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m0.37156\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 572 | loss: 0.37156 - acc: 0.9707 -- iter: 76/76\n",
      "--\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m0.37017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 573 | loss: 0.37017 - acc: 0.9710 -- iter: 76/76\n",
      "--\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m0.58130\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 574 | loss: 0.58130 - acc: 0.9094 -- iter: 76/76\n",
      "--\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m0.55886\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 575 | loss: 0.55886 - acc: 0.9158 -- iter: 76/76\n",
      "--\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m0.53866\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 576 | loss: 0.53866 - acc: 0.9216 -- iter: 76/76\n",
      "--\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m0.52047\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 577 | loss: 0.52047 - acc: 0.9268 -- iter: 76/76\n",
      "--\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m0.50408\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 578 | loss: 0.50408 - acc: 0.9315 -- iter: 76/76\n",
      "--\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m0.48931\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 579 | loss: 0.48931 - acc: 0.9357 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m0.47599\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 580 | loss: 0.47599 - acc: 0.9395 -- iter: 76/76\n",
      "--\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m0.46396\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 581 | loss: 0.46396 - acc: 0.9429 -- iter: 76/76\n",
      "--\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m0.45309\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 582 | loss: 0.45309 - acc: 0.9460 -- iter: 76/76\n",
      "--\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m0.44325\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 583 | loss: 0.44325 - acc: 0.9488 -- iter: 76/76\n",
      "--\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m0.43433\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 584 | loss: 0.43433 - acc: 0.9513 -- iter: 76/76\n",
      "--\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m0.42625\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 585 | loss: 0.42625 - acc: 0.9535 -- iter: 76/76\n",
      "--\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m0.41891\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 586 | loss: 0.41891 - acc: 0.9555 -- iter: 76/76\n",
      "--\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m0.41225\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 587 | loss: 0.41225 - acc: 0.9573 -- iter: 76/76\n",
      "--\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m0.40620\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 588 | loss: 0.40620 - acc: 0.9590 -- iter: 76/76\n",
      "--\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m0.40070\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 589 | loss: 0.40070 - acc: 0.9605 -- iter: 76/76\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m0.39569\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 590 | loss: 0.39569 - acc: 0.9618 -- iter: 76/76\n",
      "--\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m0.39113\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 591 | loss: 0.39113 - acc: 0.9630 -- iter: 76/76\n",
      "--\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m0.38698\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 592 | loss: 0.38698 - acc: 0.9640 -- iter: 76/76\n",
      "--\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m0.38318\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 593 | loss: 0.38318 - acc: 0.9650 -- iter: 76/76\n",
      "--\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m0.37971\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 594 | loss: 0.37971 - acc: 0.9659 -- iter: 76/76\n",
      "--\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m0.37653\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 595 | loss: 0.37653 - acc: 0.9667 -- iter: 76/76\n",
      "--\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m0.37360\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 596 | loss: 0.37360 - acc: 0.9674 -- iter: 76/76\n",
      "--\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m0.37091\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 597 | loss: 0.37091 - acc: 0.9680 -- iter: 76/76\n",
      "--\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m0.36842\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 598 | loss: 0.36842 - acc: 0.9686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m0.36612\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 599 | loss: 0.36612 - acc: 0.9691 -- iter: 76/76\n",
      "--\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m0.36398\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 600 | loss: 0.36398 - acc: 0.9695 -- iter: 76/76\n",
      "--\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m0.36200\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 601 | loss: 0.36200 - acc: 0.9699 -- iter: 76/76\n",
      "--\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m0.36014\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 602 | loss: 0.36014 - acc: 0.9703 -- iter: 76/76\n",
      "--\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m0.35842\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 603 | loss: 0.35842 - acc: 0.9707 -- iter: 76/76\n",
      "--\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m0.53211\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 604 | loss: 0.53211 - acc: 0.9170 -- iter: 76/76\n",
      "--\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m0.51310\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 605 | loss: 0.51310 - acc: 0.9227 -- iter: 76/76\n",
      "--\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m0.49599\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 606 | loss: 0.49599 - acc: 0.9278 -- iter: 76/76\n",
      "--\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m0.48057\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 607 | loss: 0.48057 - acc: 0.9324 -- iter: 76/76\n",
      "--\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m0.46667\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 608 | loss: 0.46667 - acc: 0.9365 -- iter: 76/76\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m0.45414\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 609 | loss: 0.45414 - acc: 0.9402 -- iter: 76/76\n",
      "--\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m0.44283\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 610 | loss: 0.44283 - acc: 0.9449 -- iter: 76/76\n",
      "--\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m0.43260\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 611 | loss: 0.43260 - acc: 0.9491 -- iter: 76/76\n",
      "--\n",
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m0.42335\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 612 | loss: 0.42335 - acc: 0.9515 -- iter: 76/76\n",
      "--\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m0.41497\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 613 | loss: 0.41497 - acc: 0.9538 -- iter: 76/76\n",
      "--\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m0.40737\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 614 | loss: 0.40737 - acc: 0.9557 -- iter: 76/76\n",
      "--\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m0.40047\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 615 | loss: 0.40047 - acc: 0.9575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m0.39420\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 616 | loss: 0.39420 - acc: 0.9592 -- iter: 76/76\n",
      "--\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m0.38849\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 617 | loss: 0.38849 - acc: 0.9606 -- iter: 76/76\n",
      "--\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m0.38331\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 618 | loss: 0.38331 - acc: 0.9619 -- iter: 76/76\n",
      "--\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m0.37858\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 619 | loss: 0.37858 - acc: 0.9631 -- iter: 76/76\n",
      "--\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m0.37428\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 620 | loss: 0.37428 - acc: 0.9642 -- iter: 76/76\n",
      "--\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m0.37035\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 621 | loss: 0.37035 - acc: 0.9651 -- iter: 76/76\n",
      "--\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m0.36677\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 622 | loss: 0.36677 - acc: 0.9660 -- iter: 76/76\n",
      "--\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m0.36348\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 623 | loss: 0.36348 - acc: 0.9667 -- iter: 76/76\n",
      "--\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m0.36048\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 624 | loss: 0.36048 - acc: 0.9674 -- iter: 76/76\n",
      "--\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m0.35771\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 625 | loss: 0.35771 - acc: 0.9681 -- iter: 76/76\n",
      "--\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m0.57088\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 626 | loss: 0.57088 - acc: 0.9028 -- iter: 76/76\n",
      "--\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m0.54700\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 627 | loss: 0.54700 - acc: 0.9099 -- iter: 76/76\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m0.52548\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 628 | loss: 0.52548 - acc: 0.9163 -- iter: 76/76\n",
      "--\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m0.50611\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 629 | loss: 0.50611 - acc: 0.9220 -- iter: 76/76\n",
      "--\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m0.48865\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 630 | loss: 0.48865 - acc: 0.9272 -- iter: 76/76\n",
      "--\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m0.47292\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 631 | loss: 0.47292 - acc: 0.9318 -- iter: 76/76\n",
      "--\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m0.45874\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 632 | loss: 0.45874 - acc: 0.9360 -- iter: 76/76\n",
      "--\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m0.44595\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 633 | loss: 0.44595 - acc: 0.9398 -- iter: 76/76\n",
      "--\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m0.43441\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 634 | loss: 0.43441 - acc: 0.9432 -- iter: 76/76\n",
      "--\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m0.42398\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 635 | loss: 0.42398 - acc: 0.9462 -- iter: 76/76\n",
      "--\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m0.41455\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 636 | loss: 0.41455 - acc: 0.9490 -- iter: 76/76\n",
      "--\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m0.40602\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 637 | loss: 0.40602 - acc: 0.9514 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m0.39828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 638 | loss: 0.39828 - acc: 0.9537 -- iter: 76/76\n",
      "--\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m0.39127\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 639 | loss: 0.39127 - acc: 0.9557 -- iter: 76/76\n",
      "--\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m0.60912\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 640 | loss: 0.60912 - acc: 0.8969 -- iter: 76/76\n",
      "--\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m0.58098\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 641 | loss: 0.58098 - acc: 0.9046 -- iter: 76/76\n",
      "--\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m0.71645\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 642 | loss: 0.71645 - acc: 0.8471 -- iter: 76/76\n",
      "--\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m0.67762\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 643 | loss: 0.67762 - acc: 0.8597 -- iter: 76/76\n",
      "--\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m0.64272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 644 | loss: 0.64272 - acc: 0.8724 -- iter: 76/76\n",
      "--\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m0.61133\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 645 | loss: 0.61133 - acc: 0.8839 -- iter: 76/76\n",
      "--\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m0.58310\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 646 | loss: 0.58310 - acc: 0.8942 -- iter: 76/76\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m0.55769\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 647 | loss: 0.55769 - acc: 0.9034 -- iter: 76/76\n",
      "--\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m0.53481\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 648 | loss: 0.53481 - acc: 0.9118 -- iter: 76/76\n",
      "--\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m0.51420\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 649 | loss: 0.51420 - acc: 0.9193 -- iter: 76/76\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m0.49561\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 650 | loss: 0.49561 - acc: 0.9247 -- iter: 76/76\n",
      "--\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m0.47884\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 651 | loss: 0.47884 - acc: 0.9296 -- iter: 76/76\n",
      "--\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m0.66392\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 652 | loss: 0.66392 - acc: 0.8709 -- iter: 76/76\n",
      "--\n",
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m0.63031\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 653 | loss: 0.63031 - acc: 0.8811 -- iter: 76/76\n",
      "--\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m0.60006\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 654 | loss: 0.60006 - acc: 0.8904 -- iter: 76/76\n",
      "--\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m0.57285\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 655 | loss: 0.57285 - acc: 0.8987 -- iter: 76/76\n",
      "--\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m0.70320\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 656 | loss: 0.70320 - acc: 0.8536 -- iter: 76/76\n",
      "--\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m0.66571\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 657 | loss: 0.66571 - acc: 0.8656 -- iter: 76/76\n",
      "--\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m0.63199\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 658 | loss: 0.63199 - acc: 0.8764 -- iter: 76/76\n",
      "--\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m0.60167\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 659 | loss: 0.60167 - acc: 0.8861 -- iter: 76/76\n",
      "--\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.79052\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 660 | loss: 0.79052 - acc: 0.8265 -- iter: 76/76\n",
      "--\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m0.74443\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 661 | loss: 0.74443 - acc: 0.8425 -- iter: 76/76\n",
      "--\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m0.70301\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 662 | loss: 0.70301 - acc: 0.8569 -- iter: 76/76\n",
      "--\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m0.66579\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 663 | loss: 0.66579 - acc: 0.8699 -- iter: 76/76\n",
      "--\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m0.63232\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 664 | loss: 0.63232 - acc: 0.8816 -- iter: 76/76\n",
      "--\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m0.60220\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 665 | loss: 0.60220 - acc: 0.8921 -- iter: 76/76\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m0.57508\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 666 | loss: 0.57508 - acc: 0.9016 -- iter: 76/76\n",
      "--\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m0.55065\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 667 | loss: 0.55065 - acc: 0.9101 -- iter: 76/76\n",
      "--\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m0.52862\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 668 | loss: 0.52862 - acc: 0.9178 -- iter: 76/76\n",
      "--\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m0.50875\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 669 | loss: 0.50875 - acc: 0.9234 -- iter: 76/76\n",
      "--\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m0.70798\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 670 | loss: 0.70798 - acc: 0.8587 -- iter: 76/76\n",
      "--\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m0.67017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 671 | loss: 0.67017 - acc: 0.8702 -- iter: 76/76\n",
      "--\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m0.63616\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 672 | loss: 0.63616 - acc: 0.8805 -- iter: 76/76\n",
      "--\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m0.60556\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 673 | loss: 0.60556 - acc: 0.8899 -- iter: 76/76\n",
      "--\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m0.57803\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 674 | loss: 0.57803 - acc: 0.8982 -- iter: 76/76\n",
      "--\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m0.55323\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 675 | loss: 0.55323 - acc: 0.9058 -- iter: 76/76\n",
      "--\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m0.53091\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 676 | loss: 0.53091 - acc: 0.9126 -- iter: 76/76\n",
      "--\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m0.51079\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 677 | loss: 0.51079 - acc: 0.9187 -- iter: 76/76\n",
      "--\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m0.49265\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 678 | loss: 0.49265 - acc: 0.9242 -- iter: 76/76\n",
      "--\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m0.47630\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 679 | loss: 0.47630 - acc: 0.9291 -- iter: 76/76\n",
      "--\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m0.46155\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 680 | loss: 0.46155 - acc: 0.9336 -- iter: 76/76\n",
      "--\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m0.44824\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 681 | loss: 0.44824 - acc: 0.9376 -- iter: 76/76\n",
      "--\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m0.43622\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 682 | loss: 0.43622 - acc: 0.9412 -- iter: 76/76\n",
      "--\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m0.42536\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 683 | loss: 0.42536 - acc: 0.9445 -- iter: 76/76\n",
      "--\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m0.41555\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 684 | loss: 0.41555 - acc: 0.9474 -- iter: 76/76\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m0.40666\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 685 | loss: 0.40666 - acc: 0.9500 -- iter: 76/76\n",
      "--\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m0.39862\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 686 | loss: 0.39862 - acc: 0.9524 -- iter: 76/76\n",
      "--\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m0.39133\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 687 | loss: 0.39133 - acc: 0.9545 -- iter: 76/76\n",
      "--\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m0.38472\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 688 | loss: 0.38472 - acc: 0.9564 -- iter: 76/76\n",
      "--\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m0.37871\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 689 | loss: 0.37871 - acc: 0.9581 -- iter: 76/76\n",
      "--\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m0.37325\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 690 | loss: 0.37325 - acc: 0.9597 -- iter: 76/76\n",
      "--\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m0.36828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 691 | loss: 0.36828 - acc: 0.9611 -- iter: 76/76\n",
      "--\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m0.36374\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 692 | loss: 0.36374 - acc: 0.9624 -- iter: 76/76\n",
      "--\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m0.35961\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 693 | loss: 0.35961 - acc: 0.9635 -- iter: 76/76\n",
      "--\n",
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m0.35582\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 694 | loss: 0.35582 - acc: 0.9645 -- iter: 76/76\n",
      "--\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m0.35236\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 695 | loss: 0.35236 - acc: 0.9654 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m0.59950\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 696 | loss: 0.59950 - acc: 0.8965 -- iter: 76/76\n",
      "--\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m0.57162\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 697 | loss: 0.57162 - acc: 0.9042 -- iter: 76/76\n",
      "--\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m0.54653\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 698 | loss: 0.54653 - acc: 0.9112 -- iter: 76/76\n",
      "--\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m0.52395\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 699 | loss: 0.52395 - acc: 0.9187 -- iter: 76/76\n",
      "--\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m0.50364\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 700 | loss: 0.50364 - acc: 0.9256 -- iter: 76/76\n",
      "--\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m0.48535\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 701 | loss: 0.48535 - acc: 0.9317 -- iter: 76/76\n",
      "--\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m0.46887\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 702 | loss: 0.46887 - acc: 0.9372 -- iter: 76/76\n",
      "--\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m0.45401\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 703 | loss: 0.45401 - acc: 0.9422 -- iter: 76/76\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m0.44060\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 704 | loss: 0.44060 - acc: 0.9466 -- iter: 76/76\n",
      "--\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m0.42848\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 705 | loss: 0.42848 - acc: 0.9507 -- iter: 76/76\n",
      "--\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m0.41753\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 706 | loss: 0.41753 - acc: 0.9543 -- iter: 76/76\n",
      "--\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m0.40762\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 707 | loss: 0.40762 - acc: 0.9575 -- iter: 76/76\n",
      "--\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m0.39865\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 708 | loss: 0.39865 - acc: 0.9591 -- iter: 76/76\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m0.39052\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 709 | loss: 0.39052 - acc: 0.9606 -- iter: 76/76\n",
      "--\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m0.38315\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 710 | loss: 0.38315 - acc: 0.9619 -- iter: 76/76\n",
      "--\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m0.37647\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 711 | loss: 0.37647 - acc: 0.9631 -- iter: 76/76\n",
      "--\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m0.37040\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 712 | loss: 0.37040 - acc: 0.9641 -- iter: 76/76\n",
      "--\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m0.36490\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 713 | loss: 0.36490 - acc: 0.9651 -- iter: 76/76\n",
      "--\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m0.35991\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 714 | loss: 0.35991 - acc: 0.9660 -- iter: 76/76\n",
      "--\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m0.35536\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 715 | loss: 0.35536 - acc: 0.9667 -- iter: 76/76\n",
      "--\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m0.35122\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 716 | loss: 0.35122 - acc: 0.9674 -- iter: 76/76\n",
      "--\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m0.34745\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 717 | loss: 0.34745 - acc: 0.9681 -- iter: 76/76\n",
      "--\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m0.34401\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 718 | loss: 0.34401 - acc: 0.9686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m0.34085\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 719 | loss: 0.34085 - acc: 0.9691 -- iter: 76/76\n",
      "--\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m0.33796\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 720 | loss: 0.33796 - acc: 0.9696 -- iter: 76/76\n",
      "--\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m0.33530\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 721 | loss: 0.33530 - acc: 0.9700 -- iter: 76/76\n",
      "--\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m0.33286\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 722 | loss: 0.33286 - acc: 0.9704 -- iter: 76/76\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m0.33060\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 723 | loss: 0.33060 - acc: 0.9707 -- iter: 76/76\n",
      "--\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m0.32851\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 724 | loss: 0.32851 - acc: 0.9710 -- iter: 76/76\n",
      "--\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m0.32658\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 725 | loss: 0.32658 - acc: 0.9713 -- iter: 76/76\n",
      "--\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m0.32478\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 726 | loss: 0.32478 - acc: 0.9715 -- iter: 76/76\n",
      "--\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m0.32311\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 727 | loss: 0.32311 - acc: 0.9717 -- iter: 76/76\n",
      "--\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m0.32156\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 728 | loss: 0.32156 - acc: 0.9719 -- iter: 76/76\n",
      "--\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m0.32010\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 729 | loss: 0.32010 - acc: 0.9721 -- iter: 76/76\n",
      "--\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m0.31874\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 730 | loss: 0.31874 - acc: 0.9723 -- iter: 76/76\n",
      "--\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m0.31746\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 731 | loss: 0.31746 - acc: 0.9724 -- iter: 76/76\n",
      "--\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m0.31626\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 732 | loss: 0.31626 - acc: 0.9725 -- iter: 76/76\n",
      "--\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m0.31513\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 733 | loss: 0.31513 - acc: 0.9726 -- iter: 76/76\n",
      "--\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m0.31405\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 734 | loss: 0.31405 - acc: 0.9727 -- iter: 76/76\n",
      "--\n",
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m0.31303\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 735 | loss: 0.31303 - acc: 0.9728 -- iter: 76/76\n",
      "--\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m0.31205\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 736 | loss: 0.31205 - acc: 0.9729 -- iter: 76/76\n",
      "--\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m0.31113\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 737 | loss: 0.31113 - acc: 0.9730 -- iter: 76/76\n",
      "--\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m0.31023\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 738 | loss: 0.31023 - acc: 0.9731 -- iter: 76/76\n",
      "--\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m0.30938\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 739 | loss: 0.30938 - acc: 0.9731 -- iter: 76/76\n",
      "--\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m0.56262\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 740 | loss: 0.56262 - acc: 0.9074 -- iter: 76/76\n",
      "--\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m0.53648\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 741 | loss: 0.53648 - acc: 0.9140 -- iter: 76/76\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m0.51294\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 742 | loss: 0.51294 - acc: 0.9213 -- iter: 76/76\n",
      "--\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m0.49176\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 743 | loss: 0.49176 - acc: 0.9279 -- iter: 76/76\n",
      "--\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m0.47269\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 744 | loss: 0.47269 - acc: 0.9338 -- iter: 76/76\n",
      "--\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m0.45551\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 745 | loss: 0.45551 - acc: 0.9391 -- iter: 76/76\n",
      "--\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m0.44003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 746 | loss: 0.44003 - acc: 0.9438 -- iter: 76/76\n",
      "--\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m0.42607\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 747 | loss: 0.42607 - acc: 0.9481 -- iter: 76/76\n",
      "--\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m0.41347\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 748 | loss: 0.41347 - acc: 0.9520 -- iter: 76/76\n",
      "--\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m0.40210\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 749 | loss: 0.40210 - acc: 0.9555 -- iter: 76/76\n",
      "--\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m0.39182\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 750 | loss: 0.39182 - acc: 0.9586 -- iter: 76/76\n",
      "--\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m0.38254\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 751 | loss: 0.38254 - acc: 0.9615 -- iter: 76/76\n",
      "--\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m0.37414\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 752 | loss: 0.37414 - acc: 0.9627 -- iter: 76/76\n",
      "--\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m0.36654\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 753 | loss: 0.36654 - acc: 0.9638 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m0.35966\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 754 | loss: 0.35966 - acc: 0.9648 -- iter: 76/76\n",
      "--\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m0.35342\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 755 | loss: 0.35342 - acc: 0.9657 -- iter: 76/76\n",
      "--\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m0.34777\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 756 | loss: 0.34777 - acc: 0.9665 -- iter: 76/76\n",
      "--\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m0.34264\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 757 | loss: 0.34264 - acc: 0.9672 -- iter: 76/76\n",
      "--\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m0.33798\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 758 | loss: 0.33798 - acc: 0.9678 -- iter: 76/76\n",
      "--\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m0.33375\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 759 | loss: 0.33375 - acc: 0.9684 -- iter: 76/76\n",
      "--\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m0.32989\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 760 | loss: 0.32989 - acc: 0.9689 -- iter: 76/76\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m0.32638\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 761 | loss: 0.32638 - acc: 0.9694 -- iter: 76/76\n",
      "--\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m0.32317\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 762 | loss: 0.32317 - acc: 0.9698 -- iter: 76/76\n",
      "--\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m0.32024\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 763 | loss: 0.32024 - acc: 0.9702 -- iter: 76/76\n",
      "--\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m0.31755\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 764 | loss: 0.31755 - acc: 0.9706 -- iter: 76/76\n",
      "--\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m0.31508\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 765 | loss: 0.31508 - acc: 0.9709 -- iter: 76/76\n",
      "--\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m0.31281\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 766 | loss: 0.31281 - acc: 0.9712 -- iter: 76/76\n",
      "--\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m0.31072\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 767 | loss: 0.31072 - acc: 0.9714 -- iter: 76/76\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m0.30878\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 768 | loss: 0.30878 - acc: 0.9716 -- iter: 76/76\n",
      "--\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m0.30700\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 769 | loss: 0.30700 - acc: 0.9718 -- iter: 76/76\n",
      "--\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m0.30534\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 770 | loss: 0.30534 - acc: 0.9720 -- iter: 76/76\n",
      "--\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m0.30380\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 771 | loss: 0.30380 - acc: 0.9722 -- iter: 76/76\n",
      "--\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m0.30237\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 772 | loss: 0.30237 - acc: 0.9723 -- iter: 76/76\n",
      "--\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m0.30103\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 773 | loss: 0.30103 - acc: 0.9725 -- iter: 76/76\n",
      "--\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m0.29978\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 774 | loss: 0.29978 - acc: 0.9726 -- iter: 76/76\n",
      "--\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m0.29860\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 775 | loss: 0.29860 - acc: 0.9727 -- iter: 76/76\n",
      "--\n",
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m0.29749\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 776 | loss: 0.29749 - acc: 0.9728 -- iter: 76/76\n",
      "--\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m0.29645\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 777 | loss: 0.29645 - acc: 0.9729 -- iter: 76/76\n",
      "--\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m0.29546\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 778 | loss: 0.29546 - acc: 0.9730 -- iter: 76/76\n",
      "--\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m0.29452\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 779 | loss: 0.29452 - acc: 0.9730 -- iter: 76/76\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m0.29363\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 780 | loss: 0.29363 - acc: 0.9731 -- iter: 76/76\n",
      "--\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m0.29278\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 781 | loss: 0.29278 - acc: 0.9732 -- iter: 76/76\n",
      "--\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m0.29197\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 782 | loss: 0.29197 - acc: 0.9732 -- iter: 76/76\n",
      "--\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m0.29118\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 783 | loss: 0.29118 - acc: 0.9733 -- iter: 76/76\n",
      "--\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m0.29043\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 784 | loss: 0.29043 - acc: 0.9733 -- iter: 76/76\n",
      "--\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m0.28971\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 785 | loss: 0.28971 - acc: 0.9733 -- iter: 76/76\n",
      "--\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m0.28901\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 786 | loss: 0.28901 - acc: 0.9734 -- iter: 76/76\n",
      "--\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m0.28833\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 787 | loss: 0.28833 - acc: 0.9734 -- iter: 76/76\n",
      "--\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m0.28767\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 788 | loss: 0.28767 - acc: 0.9734 -- iter: 76/76\n",
      "--\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m0.28703\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 789 | loss: 0.28703 - acc: 0.9735 -- iter: 76/76\n",
      "--\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m0.28641\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 790 | loss: 0.28641 - acc: 0.9735 -- iter: 76/76\n",
      "--\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m0.28580\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 791 | loss: 0.28580 - acc: 0.9735 -- iter: 76/76\n",
      "--\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m0.28521\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 792 | loss: 0.28521 - acc: 0.9735 -- iter: 76/76\n",
      "--\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m0.28462\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 793 | loss: 0.28462 - acc: 0.9735 -- iter: 76/76\n",
      "--\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m0.28405\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 794 | loss: 0.28405 - acc: 0.9736 -- iter: 76/76\n",
      "--\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m0.28349\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 795 | loss: 0.28349 - acc: 0.9736 -- iter: 76/76\n",
      "--\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m0.28294\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 796 | loss: 0.28294 - acc: 0.9736 -- iter: 76/76\n",
      "--\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m0.28240\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 797 | loss: 0.28240 - acc: 0.9736 -- iter: 76/76\n",
      "--\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m0.55487\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 798 | loss: 0.55487 - acc: 0.9052 -- iter: 76/76\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m0.52709\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 799 | loss: 0.52709 - acc: 0.9133 -- iter: 76/76\n",
      "--\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m0.50209\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 800 | loss: 0.50209 - acc: 0.9207 -- iter: 76/76\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m0.47960\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 801 | loss: 0.47960 - acc: 0.9273 -- iter: 76/76\n",
      "--\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m0.45936\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 802 | loss: 0.45936 - acc: 0.9333 -- iter: 76/76\n",
      "--\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m0.44114\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 803 | loss: 0.44114 - acc: 0.9386 -- iter: 76/76\n",
      "--\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m0.42473\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 804 | loss: 0.42473 - acc: 0.9434 -- iter: 76/76\n",
      "--\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m0.40994\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 805 | loss: 0.40994 - acc: 0.9478 -- iter: 76/76\n",
      "--\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m0.39660\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 806 | loss: 0.39660 - acc: 0.9517 -- iter: 76/76\n",
      "--\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m0.38456\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 807 | loss: 0.38456 - acc: 0.9552 -- iter: 76/76\n",
      "--\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m0.37368\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 808 | loss: 0.37368 - acc: 0.9584 -- iter: 76/76\n",
      "--\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m0.36385\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 809 | loss: 0.36385 - acc: 0.9612 -- iter: 76/76\n",
      "--\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m0.35497\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 810 | loss: 0.35497 - acc: 0.9638 -- iter: 76/76\n",
      "--\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m0.34693\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 811 | loss: 0.34693 - acc: 0.9661 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m0.33966\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 812 | loss: 0.33966 - acc: 0.9682 -- iter: 76/76\n",
      "--\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m0.33308\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 813 | loss: 0.33308 - acc: 0.9687 -- iter: 76/76\n",
      "--\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m0.32713\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 814 | loss: 0.32713 - acc: 0.9692 -- iter: 76/76\n",
      "--\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m0.32174\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 815 | loss: 0.32174 - acc: 0.9697 -- iter: 76/76\n",
      "--\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m0.31685\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 816 | loss: 0.31685 - acc: 0.9701 -- iter: 76/76\n",
      "--\n",
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m0.31241\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 817 | loss: 0.31241 - acc: 0.9704 -- iter: 76/76\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m0.30838\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 818 | loss: 0.30838 - acc: 0.9707 -- iter: 76/76\n",
      "--\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m0.30472\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 819 | loss: 0.30472 - acc: 0.9710 -- iter: 76/76\n",
      "--\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m0.30138\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 820 | loss: 0.30138 - acc: 0.9713 -- iter: 76/76\n",
      "--\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m0.29833\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 821 | loss: 0.29833 - acc: 0.9715 -- iter: 76/76\n",
      "--\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m0.53872\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 822 | loss: 0.53872 - acc: 0.9165 -- iter: 76/76\n",
      "--\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m0.51189\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 823 | loss: 0.51189 - acc: 0.9222 -- iter: 76/76\n",
      "--\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m0.48773\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 824 | loss: 0.48773 - acc: 0.9287 -- iter: 76/76\n",
      "--\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m0.46599\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 825 | loss: 0.46599 - acc: 0.9345 -- iter: 76/76\n",
      "--\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m0.44641\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 826 | loss: 0.44641 - acc: 0.9397 -- iter: 76/76\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m0.42879\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 827 | loss: 0.42879 - acc: 0.9444 -- iter: 76/76\n",
      "--\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m0.41291\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 828 | loss: 0.41291 - acc: 0.9487 -- iter: 76/76\n",
      "--\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m0.39861\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 829 | loss: 0.39861 - acc: 0.9525 -- iter: 76/76\n",
      "--\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m0.38572\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 830 | loss: 0.38572 - acc: 0.9559 -- iter: 76/76\n",
      "--\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m0.37408\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 831 | loss: 0.37408 - acc: 0.9590 -- iter: 76/76\n",
      "--\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m0.36358\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 832 | loss: 0.36358 - acc: 0.9618 -- iter: 76/76\n",
      "--\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m0.35409\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 833 | loss: 0.35409 - acc: 0.9643 -- iter: 76/76\n",
      "--\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m0.34552\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 834 | loss: 0.34552 - acc: 0.9666 -- iter: 76/76\n",
      "--\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m0.33776\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 835 | loss: 0.33776 - acc: 0.9686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m0.33075\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 836 | loss: 0.33075 - acc: 0.9704 -- iter: 76/76\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m0.32439\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 837 | loss: 0.32439 - acc: 0.9721 -- iter: 76/76\n",
      "--\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m0.56426\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 838 | loss: 0.56426 - acc: 0.9012 -- iter: 76/76\n",
      "--\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m0.53453\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 839 | loss: 0.53453 - acc: 0.9097 -- iter: 76/76\n",
      "--\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m0.50777\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 840 | loss: 0.50777 - acc: 0.9174 -- iter: 76/76\n",
      "--\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m0.48368\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 841 | loss: 0.48368 - acc: 0.9244 -- iter: 76/76\n",
      "--\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m0.71811\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 842 | loss: 0.71811 - acc: 0.8635 -- iter: 76/76\n",
      "--\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m0.67303\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 843 | loss: 0.67303 - acc: 0.8759 -- iter: 76/76\n",
      "--\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m0.63250\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 844 | loss: 0.63250 - acc: 0.8870 -- iter: 76/76\n",
      "--\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m0.59605\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 845 | loss: 0.59605 - acc: 0.8969 -- iter: 76/76\n",
      "--\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m0.56327\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 846 | loss: 0.56327 - acc: 0.9059 -- iter: 76/76\n",
      "--\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m0.53378\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 847 | loss: 0.53378 - acc: 0.9140 -- iter: 76/76\n",
      "--\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m0.50725\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 848 | loss: 0.50725 - acc: 0.9213 -- iter: 76/76\n",
      "--\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m0.48336\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 849 | loss: 0.48336 - acc: 0.9279 -- iter: 76/76\n",
      "--\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m0.46184\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 850 | loss: 0.46184 - acc: 0.9338 -- iter: 76/76\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m0.44245\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 851 | loss: 0.44245 - acc: 0.9391 -- iter: 76/76\n",
      "--\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m0.42497\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 852 | loss: 0.42497 - acc: 0.9438 -- iter: 76/76\n",
      "--\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m0.40922\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 853 | loss: 0.40922 - acc: 0.9481 -- iter: 76/76\n",
      "--\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m0.39501\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 854 | loss: 0.39501 - acc: 0.9520 -- iter: 76/76\n",
      "--\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m0.38220\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 855 | loss: 0.38220 - acc: 0.9555 -- iter: 76/76\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m0.37064\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 856 | loss: 0.37064 - acc: 0.9586 -- iter: 76/76\n",
      "--\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m0.36021\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 857 | loss: 0.36021 - acc: 0.9601 -- iter: 76/76\n",
      "--\n",
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m0.35080\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 858 | loss: 0.35080 - acc: 0.9615 -- iter: 76/76\n",
      "--\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m0.34230\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 859 | loss: 0.34230 - acc: 0.9627 -- iter: 76/76\n",
      "--\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m0.33462\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 860 | loss: 0.33462 - acc: 0.9638 -- iter: 76/76\n",
      "--\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m0.32768\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 861 | loss: 0.32768 - acc: 0.9648 -- iter: 76/76\n",
      "--\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m0.32140\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 862 | loss: 0.32140 - acc: 0.9657 -- iter: 76/76\n",
      "--\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m0.31571\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 863 | loss: 0.31571 - acc: 0.9665 -- iter: 76/76\n",
      "--\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m0.54940\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 864 | loss: 0.54940 - acc: 0.9054 -- iter: 76/76\n",
      "--\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m0.52087\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 865 | loss: 0.52087 - acc: 0.9122 -- iter: 76/76\n",
      "--\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m0.49519\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 866 | loss: 0.49519 - acc: 0.9197 -- iter: 76/76\n",
      "--\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m0.47207\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 867 | loss: 0.47207 - acc: 0.9264 -- iter: 76/76\n",
      "--\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m0.45125\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 868 | loss: 0.45125 - acc: 0.9324 -- iter: 76/76\n",
      "--\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m0.43252\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 869 | loss: 0.43252 - acc: 0.9379 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m0.41564\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 870 | loss: 0.41564 - acc: 0.9428 -- iter: 76/76\n",
      "--\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m0.40044\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 871 | loss: 0.40044 - acc: 0.9472 -- iter: 76/76\n",
      "--\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m0.38674\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 872 | loss: 0.38674 - acc: 0.9511 -- iter: 76/76\n",
      "--\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m0.37439\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 873 | loss: 0.37439 - acc: 0.9547 -- iter: 76/76\n",
      "--\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m0.36324\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 874 | loss: 0.36324 - acc: 0.9579 -- iter: 76/76\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m0.35317\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 875 | loss: 0.35317 - acc: 0.9608 -- iter: 76/76\n",
      "--\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m0.34408\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 876 | loss: 0.34408 - acc: 0.9634 -- iter: 76/76\n",
      "--\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m0.33586\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 877 | loss: 0.33586 - acc: 0.9658 -- iter: 76/76\n",
      "--\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m0.32843\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 878 | loss: 0.32843 - acc: 0.9679 -- iter: 76/76\n",
      "--\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m0.32170\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 879 | loss: 0.32170 - acc: 0.9698 -- iter: 76/76\n",
      "--\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.31561\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 880 | loss: 0.31561 - acc: 0.9715 -- iter: 76/76\n",
      "--\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m0.31010\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 881 | loss: 0.31010 - acc: 0.9730 -- iter: 76/76\n",
      "--\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m0.30510\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 882 | loss: 0.30510 - acc: 0.9744 -- iter: 76/76\n",
      "--\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m0.30056\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 883 | loss: 0.30056 - acc: 0.9756 -- iter: 76/76\n",
      "--\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m0.29645\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 884 | loss: 0.29645 - acc: 0.9754 -- iter: 76/76\n",
      "--\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m0.29271\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 885 | loss: 0.29271 - acc: 0.9753 -- iter: 76/76\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m0.28930\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 886 | loss: 0.28930 - acc: 0.9751 -- iter: 76/76\n",
      "--\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m0.28620\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 887 | loss: 0.28620 - acc: 0.9750 -- iter: 76/76\n",
      "--\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m0.28337\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 888 | loss: 0.28337 - acc: 0.9748 -- iter: 76/76\n",
      "--\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m0.28079\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 889 | loss: 0.28079 - acc: 0.9747 -- iter: 76/76\n",
      "--\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m0.27842\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 890 | loss: 0.27842 - acc: 0.9759 -- iter: 76/76\n",
      "--\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m0.27625\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 891 | loss: 0.27625 - acc: 0.9770 -- iter: 76/76\n",
      "--\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m0.27425\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 892 | loss: 0.27425 - acc: 0.9780 -- iter: 76/76\n",
      "--\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m0.27242\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 893 | loss: 0.27242 - acc: 0.9789 -- iter: 76/76\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m0.27073\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 894 | loss: 0.27073 - acc: 0.9797 -- iter: 76/76\n",
      "--\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m0.26916\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 895 | loss: 0.26916 - acc: 0.9804 -- iter: 76/76\n",
      "--\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m0.26771\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 896 | loss: 0.26771 - acc: 0.9810 -- iter: 76/76\n",
      "--\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m0.26637\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 897 | loss: 0.26637 - acc: 0.9816 -- iter: 76/76\n",
      "--\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m0.26512\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 898 | loss: 0.26512 - acc: 0.9821 -- iter: 76/76\n",
      "--\n",
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m0.26396\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 899 | loss: 0.26396 - acc: 0.9826 -- iter: 76/76\n",
      "--\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m0.26288\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 900 | loss: 0.26288 - acc: 0.9830 -- iter: 76/76\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m0.26186\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 901 | loss: 0.26186 - acc: 0.9834 -- iter: 76/76\n",
      "--\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m0.26090\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 902 | loss: 0.26090 - acc: 0.9838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m0.26000\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 903 | loss: 0.26000 - acc: 0.9841 -- iter: 76/76\n",
      "--\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m0.25915\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 904 | loss: 0.25915 - acc: 0.9843 -- iter: 76/76\n",
      "--\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m0.25835\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 905 | loss: 0.25835 - acc: 0.9846 -- iter: 76/76\n",
      "--\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m0.25758\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 906 | loss: 0.25758 - acc: 0.9848 -- iter: 76/76\n",
      "--\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m0.25685\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 907 | loss: 0.25685 - acc: 0.9850 -- iter: 76/76\n",
      "--\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m0.25616\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 908 | loss: 0.25616 - acc: 0.9852 -- iter: 76/76\n",
      "--\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m0.25549\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 909 | loss: 0.25549 - acc: 0.9854 -- iter: 76/76\n",
      "--\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m0.25485\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 910 | loss: 0.25485 - acc: 0.9855 -- iter: 76/76\n",
      "--\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m0.25424\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 911 | loss: 0.25424 - acc: 0.9856 -- iter: 76/76\n",
      "--\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m0.25365\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 912 | loss: 0.25365 - acc: 0.9858 -- iter: 76/76\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m0.25307\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 913 | loss: 0.25307 - acc: 0.9859 -- iter: 76/76\n",
      "--\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m0.25252\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 914 | loss: 0.25252 - acc: 0.9860 -- iter: 76/76\n",
      "--\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m0.25198\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 915 | loss: 0.25198 - acc: 0.9861 -- iter: 76/76\n",
      "--\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m0.25146\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 916 | loss: 0.25146 - acc: 0.9861 -- iter: 76/76\n",
      "--\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m0.25095\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 917 | loss: 0.25095 - acc: 0.9862 -- iter: 76/76\n",
      "--\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m0.25045\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 918 | loss: 0.25045 - acc: 0.9863 -- iter: 76/76\n",
      "--\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m0.24996\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 919 | loss: 0.24996 - acc: 0.9863 -- iter: 76/76\n",
      "--\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m0.24948\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 920 | loss: 0.24948 - acc: 0.9864 -- iter: 76/76\n",
      "--\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m0.24902\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 921 | loss: 0.24902 - acc: 0.9864 -- iter: 76/76\n",
      "--\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m0.24856\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 922 | loss: 0.24856 - acc: 0.9865 -- iter: 76/76\n",
      "--\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m0.24810\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 923 | loss: 0.24810 - acc: 0.9865 -- iter: 76/76\n",
      "--\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m0.24766\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 924 | loss: 0.24766 - acc: 0.9865 -- iter: 76/76\n",
      "--\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m0.24722\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 925 | loss: 0.24722 - acc: 0.9866 -- iter: 76/76\n",
      "--\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m0.24678\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 926 | loss: 0.24678 - acc: 0.9866 -- iter: 76/76\n",
      "--\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m0.24636\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 927 | loss: 0.24636 - acc: 0.9866 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m0.24593\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 928 | loss: 0.24593 - acc: 0.9866 -- iter: 76/76\n",
      "--\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m0.24551\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 929 | loss: 0.24551 - acc: 0.9867 -- iter: 76/76\n",
      "--\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m0.24510\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 930 | loss: 0.24510 - acc: 0.9867 -- iter: 76/76\n",
      "--\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m0.24469\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 931 | loss: 0.24469 - acc: 0.9867 -- iter: 76/76\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m0.24428\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 932 | loss: 0.24428 - acc: 0.9867 -- iter: 76/76\n",
      "--\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m0.24387\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 933 | loss: 0.24387 - acc: 0.9867 -- iter: 76/76\n",
      "--\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m0.24347\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 934 | loss: 0.24347 - acc: 0.9867 -- iter: 76/76\n",
      "--\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m0.24307\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 935 | loss: 0.24307 - acc: 0.9867 -- iter: 76/76\n",
      "--\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m0.24268\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 936 | loss: 0.24268 - acc: 0.9868 -- iter: 76/76\n",
      "--\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m0.24228\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 937 | loss: 0.24228 - acc: 0.9868 -- iter: 76/76\n",
      "--\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m0.24189\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 938 | loss: 0.24189 - acc: 0.9868 -- iter: 76/76\n",
      "--\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m0.24150\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 939 | loss: 0.24150 - acc: 0.9868 -- iter: 76/76\n",
      "--\n",
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m0.48173\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 940 | loss: 0.48173 - acc: 0.9263 -- iter: 76/76\n",
      "--\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m0.45731\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 941 | loss: 0.45731 - acc: 0.9323 -- iter: 76/76\n",
      "--\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m0.43534\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 942 | loss: 0.43534 - acc: 0.9378 -- iter: 76/76\n",
      "--\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m0.41556\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 943 | loss: 0.41556 - acc: 0.9427 -- iter: 76/76\n",
      "--\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m0.68650\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 944 | loss: 0.68650 - acc: 0.8787 -- iter: 76/76\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m0.64165\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 945 | loss: 0.64165 - acc: 0.8895 -- iter: 76/76\n",
      "--\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m0.90298\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 946 | loss: 0.90298 - acc: 0.8308 -- iter: 76/76\n",
      "--\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m0.83666\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 947 | loss: 0.83666 - acc: 0.8451 -- iter: 76/76\n",
      "--\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m0.77707\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 948 | loss: 0.77707 - acc: 0.8580 -- iter: 76/76\n",
      "--\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m0.72354\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 949 | loss: 0.72354 - acc: 0.8695 -- iter: 76/76\n",
      "--\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m0.67541\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 950 | loss: 0.67541 - acc: 0.8799 -- iter: 76/76\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m0.63211\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 951 | loss: 0.63211 - acc: 0.8893 -- iter: 76/76\n",
      "--\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m0.59314\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 952 | loss: 0.59314 - acc: 0.8978 -- iter: 76/76\n",
      "--\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m0.55804\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 953 | loss: 0.55804 - acc: 0.9053 -- iter: 76/76\n",
      "--\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m0.52642\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 954 | loss: 0.52642 - acc: 0.9122 -- iter: 76/76\n",
      "--\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m0.49791\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 955 | loss: 0.49791 - acc: 0.9196 -- iter: 76/76\n",
      "--\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m0.47222\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 956 | loss: 0.47222 - acc: 0.9264 -- iter: 76/76\n",
      "--\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m0.44907\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 957 | loss: 0.44907 - acc: 0.9324 -- iter: 76/76\n",
      "--\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m0.42821\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 958 | loss: 0.42821 - acc: 0.9379 -- iter: 76/76\n",
      "--\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m0.40942\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 959 | loss: 0.40942 - acc: 0.9428 -- iter: 76/76\n",
      "--\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m0.39251\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 960 | loss: 0.39251 - acc: 0.9472 -- iter: 76/76\n",
      "--\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m0.37729\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 961 | loss: 0.37729 - acc: 0.9498 -- iter: 76/76\n",
      "--\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m0.36358\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 962 | loss: 0.36358 - acc: 0.9522 -- iter: 76/76\n",
      "--\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m0.35124\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 963 | loss: 0.35124 - acc: 0.9544 -- iter: 76/76\n",
      "--\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m0.61736\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 964 | loss: 0.61736 - acc: 0.9010 -- iter: 76/76\n",
      "--\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m0.57962\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 965 | loss: 0.57962 - acc: 0.9083 -- iter: 76/76\n",
      "--\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m0.54566\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 966 | loss: 0.54566 - acc: 0.9148 -- iter: 76/76\n",
      "--\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m0.51509\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 967 | loss: 0.51509 - acc: 0.9220 -- iter: 76/76\n",
      "--\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m0.48757\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 968 | loss: 0.48757 - acc: 0.9285 -- iter: 76/76\n",
      "--\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m0.46281\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 969 | loss: 0.46281 - acc: 0.9343 -- iter: 76/76\n",
      "--\n",
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m0.44052\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 970 | loss: 0.44052 - acc: 0.9396 -- iter: 76/76\n",
      "--\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m0.42046\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 971 | loss: 0.42046 - acc: 0.9443 -- iter: 76/76\n",
      "--\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m0.40240\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 972 | loss: 0.40240 - acc: 0.9486 -- iter: 76/76\n",
      "--\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m0.38614\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 973 | loss: 0.38614 - acc: 0.9524 -- iter: 76/76\n",
      "--\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m0.64345\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 974 | loss: 0.64345 - acc: 0.8914 -- iter: 76/76\n",
      "--\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m0.60312\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 975 | loss: 0.60312 - acc: 0.9009 -- iter: 76/76\n",
      "--\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m0.56686\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 976 | loss: 0.56686 - acc: 0.9095 -- iter: 76/76\n",
      "--\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m0.53426\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 977 | loss: 0.53426 - acc: 0.9172 -- iter: 76/76\n",
      "--\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m0.50494\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 978 | loss: 0.50494 - acc: 0.9242 -- iter: 76/76\n",
      "--\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m0.47855\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 979 | loss: 0.47855 - acc: 0.9305 -- iter: 76/76\n",
      "--\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m0.45479\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 980 | loss: 0.45479 - acc: 0.9361 -- iter: 76/76\n",
      "--\n",
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m0.43338\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 981 | loss: 0.43338 - acc: 0.9412 -- iter: 76/76\n",
      "--\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m0.41408\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 982 | loss: 0.41408 - acc: 0.9457 -- iter: 76/76\n",
      "--\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m0.39669\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 983 | loss: 0.39669 - acc: 0.9499 -- iter: 76/76\n",
      "--\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m0.38100\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 984 | loss: 0.38100 - acc: 0.9536 -- iter: 76/76\n",
      "--\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m0.36685\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 985 | loss: 0.36685 - acc: 0.9569 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m0.35408\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 986 | loss: 0.35408 - acc: 0.9599 -- iter: 76/76\n",
      "--\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m0.34257\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 987 | loss: 0.34257 - acc: 0.9626 -- iter: 76/76\n",
      "--\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m0.58795\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 988 | loss: 0.58795 - acc: 0.8992 -- iter: 76/76\n",
      "--\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m0.55304\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 989 | loss: 0.55304 - acc: 0.9080 -- iter: 76/76\n",
      "--\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m0.52164\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 990 | loss: 0.52164 - acc: 0.9159 -- iter: 76/76\n",
      "--\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m0.49339\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 991 | loss: 0.49339 - acc: 0.9230 -- iter: 76/76\n",
      "--\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m0.46796\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 992 | loss: 0.46796 - acc: 0.9293 -- iter: 76/76\n",
      "--\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m0.44507\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 993 | loss: 0.44507 - acc: 0.9351 -- iter: 76/76\n",
      "--\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m0.42447\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 994 | loss: 0.42447 - acc: 0.9403 -- iter: 76/76\n",
      "--\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m0.40591\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 995 | loss: 0.40591 - acc: 0.9449 -- iter: 76/76\n",
      "--\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m0.38920\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 996 | loss: 0.38920 - acc: 0.9491 -- iter: 76/76\n",
      "--\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m0.37415\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 997 | loss: 0.37415 - acc: 0.9529 -- iter: 76/76\n",
      "--\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m0.36058\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 998 | loss: 0.36058 - acc: 0.9563 -- iter: 76/76\n",
      "--\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m0.34835\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 999 | loss: 0.34835 - acc: 0.9593 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m0.62886\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1000 | loss: 0.62886 - acc: 0.8910 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m0.58981\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1001 | loss: 0.58981 - acc: 0.9006 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m0.55470\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1002 | loss: 0.55470 - acc: 0.9092 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m0.52313\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1003 | loss: 0.52313 - acc: 0.9170 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m0.78042\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1004 | loss: 0.78042 - acc: 0.8569 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m0.72639\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1005 | loss: 0.72639 - acc: 0.8699 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m0.67784\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1006 | loss: 0.67784 - acc: 0.8816 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m0.63422\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1007 | loss: 0.63422 - acc: 0.8908 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m0.59499\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1008 | loss: 0.59499 - acc: 0.8991 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m0.55972\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1009 | loss: 0.55972 - acc: 0.9065 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m0.52797\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1010 | loss: 0.52797 - acc: 0.9133 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m0.49938\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1011 | loss: 0.49938 - acc: 0.9206 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m0.47362\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1012 | loss: 0.47362 - acc: 0.9272 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m0.45040\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1013 | loss: 0.45040 - acc: 0.9332 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m0.42947\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1014 | loss: 0.42947 - acc: 0.9386 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m0.41060\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1015 | loss: 0.41060 - acc: 0.9434 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m0.39359\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1016 | loss: 0.39359 - acc: 0.9477 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m0.37826\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1017 | loss: 0.37826 - acc: 0.9516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m0.36444\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1018 | loss: 0.36444 - acc: 0.9552 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m0.35199\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1019 | loss: 0.35199 - acc: 0.9583 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m0.34077\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1020 | loss: 0.34077 - acc: 0.9599 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m0.33065\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1021 | loss: 0.33065 - acc: 0.9612 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m0.32152\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1022 | loss: 0.32152 - acc: 0.9625 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m0.31327\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1023 | loss: 0.31327 - acc: 0.9636 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m0.30583\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1024 | loss: 0.30583 - acc: 0.9646 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m0.29909\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1025 | loss: 0.29909 - acc: 0.9655 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m0.29299\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1026 | loss: 0.29299 - acc: 0.9663 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m0.28746\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1027 | loss: 0.28746 - acc: 0.9671 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m0.28244\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1028 | loss: 0.28244 - acc: 0.9691 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m0.27788\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1029 | loss: 0.27788 - acc: 0.9708 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m0.27374\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1030 | loss: 0.27374 - acc: 0.9724 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m0.26998\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1031 | loss: 0.26998 - acc: 0.9739 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m0.26655\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1032 | loss: 0.26655 - acc: 0.9752 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m0.26343\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1033 | loss: 0.26343 - acc: 0.9763 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m0.26058\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1034 | loss: 0.26058 - acc: 0.9774 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m0.25799\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1035 | loss: 0.25799 - acc: 0.9783 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m0.25561\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1036 | loss: 0.25561 - acc: 0.9792 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m0.25345\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1037 | loss: 0.25345 - acc: 0.9799 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m0.25146\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1038 | loss: 0.25146 - acc: 0.9806 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m0.24963\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1039 | loss: 0.24963 - acc: 0.9813 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m0.24795\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1040 | loss: 0.24795 - acc: 0.9818 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m0.24640\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1041 | loss: 0.24640 - acc: 0.9823 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m0.24497\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1042 | loss: 0.24497 - acc: 0.9828 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m0.24365\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1043 | loss: 0.24365 - acc: 0.9832 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m0.24241\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1044 | loss: 0.24241 - acc: 0.9835 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m0.24127\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1045 | loss: 0.24127 - acc: 0.9839 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m0.24020\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1046 | loss: 0.24020 - acc: 0.9842 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m0.23921\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1047 | loss: 0.23921 - acc: 0.9844 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m0.23827\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1048 | loss: 0.23827 - acc: 0.9847 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m0.23740\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1049 | loss: 0.23740 - acc: 0.9849 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m0.23658\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1050 | loss: 0.23658 - acc: 0.9851 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m0.23580\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1051 | loss: 0.23580 - acc: 0.9853 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m0.23507\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1052 | loss: 0.23507 - acc: 0.9854 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m0.23437\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1053 | loss: 0.23437 - acc: 0.9856 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m0.23371\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1054 | loss: 0.23371 - acc: 0.9857 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m0.23309\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1055 | loss: 0.23309 - acc: 0.9858 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m0.23249\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1056 | loss: 0.23249 - acc: 0.9859 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m0.23191\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1057 | loss: 0.23191 - acc: 0.9860 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m0.23136\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1058 | loss: 0.23136 - acc: 0.9861 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m0.23083\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1059 | loss: 0.23083 - acc: 0.9862 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m0.23032\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1060 | loss: 0.23032 - acc: 0.9862 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m0.22983\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1061 | loss: 0.22983 - acc: 0.9863 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m0.22935\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1062 | loss: 0.22935 - acc: 0.9863 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m0.22888\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1063 | loss: 0.22888 - acc: 0.9864 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m0.22843\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1064 | loss: 0.22843 - acc: 0.9864 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m0.22799\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1065 | loss: 0.22799 - acc: 0.9865 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m0.22756\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1066 | loss: 0.22756 - acc: 0.9865 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m0.22715\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1067 | loss: 0.22715 - acc: 0.9865 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m0.22674\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1068 | loss: 0.22674 - acc: 0.9866 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m0.22633\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1069 | loss: 0.22633 - acc: 0.9866 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m0.45381\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1070 | loss: 0.45381 - acc: 0.9248 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m0.43067\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1071 | loss: 0.43067 - acc: 0.9310 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m0.40984\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1072 | loss: 0.40984 - acc: 0.9366 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m0.39109\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1073 | loss: 0.39109 - acc: 0.9416 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m0.37421\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1074 | loss: 0.37421 - acc: 0.9461 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m0.35900\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1075 | loss: 0.35900 - acc: 0.9502 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m0.34530\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1076 | loss: 0.34530 - acc: 0.9539 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m0.33296\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1077 | loss: 0.33296 - acc: 0.9572 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m0.32183\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1078 | loss: 0.32183 - acc: 0.9601 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m0.31179\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1079 | loss: 0.31179 - acc: 0.9628 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m0.30274\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1080 | loss: 0.30274 - acc: 0.9652 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m0.29456\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1081 | loss: 0.29456 - acc: 0.9674 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m0.28718\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1082 | loss: 0.28718 - acc: 0.9693 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m0.28052\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1083 | loss: 0.28052 - acc: 0.9711 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m0.27450\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1084 | loss: 0.27450 - acc: 0.9726 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m0.26905\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1085 | loss: 0.26905 - acc: 0.9741 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m0.26412\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1086 | loss: 0.26412 - acc: 0.9753 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m0.25966\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1087 | loss: 0.25966 - acc: 0.9765 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m0.25562\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1088 | loss: 0.25562 - acc: 0.9775 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m0.25196\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1089 | loss: 0.25196 - acc: 0.9785 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m0.24864\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1090 | loss: 0.24864 - acc: 0.9793 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m0.24562\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1091 | loss: 0.24562 - acc: 0.9801 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m0.24287\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1092 | loss: 0.24287 - acc: 0.9807 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m0.24037\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1093 | loss: 0.24037 - acc: 0.9813 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m0.23809\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1094 | loss: 0.23809 - acc: 0.9819 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m0.23601\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1095 | loss: 0.23601 - acc: 0.9824 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m0.23411\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1096 | loss: 0.23411 - acc: 0.9828 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m0.23237\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1097 | loss: 0.23237 - acc: 0.9832 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m0.46852\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1098 | loss: 0.46852 - acc: 0.9204 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m0.44331\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1099 | loss: 0.44331 - acc: 0.9271 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.42062\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1100 | loss: 0.42062 - acc: 0.9331 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m0.40021\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1101 | loss: 0.40021 - acc: 0.9384 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m0.38183\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1102 | loss: 0.38183 - acc: 0.9433 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m0.36529\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1103 | loss: 0.36529 - acc: 0.9476 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m0.35040\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1104 | loss: 0.35040 - acc: 0.9516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m0.33698\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1105 | loss: 0.33698 - acc: 0.9551 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m0.32489\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1106 | loss: 0.32489 - acc: 0.9583 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m0.31398\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1107 | loss: 0.31398 - acc: 0.9611 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m0.30415\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1108 | loss: 0.30415 - acc: 0.9637 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m0.29527\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1109 | loss: 0.29527 - acc: 0.9660 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m0.28725\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1110 | loss: 0.28725 - acc: 0.9681 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m0.28002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1111 | loss: 0.28002 - acc: 0.9700 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m0.27348\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1112 | loss: 0.27348 - acc: 0.9717 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m0.26757\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1113 | loss: 0.26757 - acc: 0.9732 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m0.53441\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1114 | loss: 0.53441 - acc: 0.9114 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m0.50239\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1115 | loss: 0.50239 - acc: 0.9189 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m0.81148\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1116 | loss: 0.81148 - acc: 0.8533 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m0.75183\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1117 | loss: 0.75183 - acc: 0.8667 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m0.69821\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1118 | loss: 0.69821 - acc: 0.8787 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m0.65001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1119 | loss: 0.65001 - acc: 0.8895 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m0.60669\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1120 | loss: 0.60669 - acc: 0.8993 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m0.56773\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1121 | loss: 0.56773 - acc: 0.9080 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m0.53270\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1122 | loss: 0.53270 - acc: 0.9159 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m0.50117\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1123 | loss: 0.50117 - acc: 0.9230 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m0.47279\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1124 | loss: 0.47279 - acc: 0.9294 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m0.44723\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1125 | loss: 0.44723 - acc: 0.9351 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m0.42420\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1126 | loss: 0.42420 - acc: 0.9403 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m0.40345\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1127 | loss: 0.40345 - acc: 0.9450 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m0.38476\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1128 | loss: 0.38476 - acc: 0.9491 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m0.36791\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1129 | loss: 0.36791 - acc: 0.9529 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m0.35274\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1130 | loss: 0.35274 - acc: 0.9563 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m0.33906\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1131 | loss: 0.33906 - acc: 0.9594 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m0.32674\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1132 | loss: 0.32674 - acc: 0.9621 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m0.31564\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1133 | loss: 0.31564 - acc: 0.9646 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m0.30563\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1134 | loss: 0.30563 - acc: 0.9668 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m0.29661\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1135 | loss: 0.29661 - acc: 0.9688 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m0.28848\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1136 | loss: 0.28848 - acc: 0.9706 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m0.28113\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1137 | loss: 0.28113 - acc: 0.9722 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m0.68184\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1138 | loss: 0.68184 - acc: 0.8974 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m0.63514\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1139 | loss: 0.63514 - acc: 0.9063 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m0.59314\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1140 | loss: 0.59314 - acc: 0.9144 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m0.55536\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1141 | loss: 0.55536 - acc: 0.9216 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m0.52139\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1142 | loss: 0.52139 - acc: 0.9281 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m0.49084\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1143 | loss: 0.49084 - acc: 0.9340 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m0.46336\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1144 | loss: 0.46336 - acc: 0.9393 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m0.43864\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1145 | loss: 0.43864 - acc: 0.9441 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m0.41639\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1146 | loss: 0.41639 - acc: 0.9483 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m0.39637\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1147 | loss: 0.39637 - acc: 0.9522 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m0.70997\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1148 | loss: 0.70997 - acc: 0.8780 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m0.66064\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1149 | loss: 0.66064 - acc: 0.8889 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m0.61629\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1150 | loss: 0.61629 - acc: 0.8987 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m0.57640\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1151 | loss: 0.57640 - acc: 0.9075 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m0.54053\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1152 | loss: 0.54053 - acc: 0.9154 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m0.50824\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1153 | loss: 0.50824 - acc: 0.9226 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m0.47918\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1154 | loss: 0.47918 - acc: 0.9290 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m0.45301\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1155 | loss: 0.45301 - acc: 0.9348 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m0.42944\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1156 | loss: 0.42944 - acc: 0.9400 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m0.40820\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1157 | loss: 0.40820 - acc: 0.9447 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m0.63527\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1158 | loss: 0.63527 - acc: 0.8871 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m0.59346\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1159 | loss: 0.59346 - acc: 0.8970 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m0.55585\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1160 | loss: 0.55585 - acc: 0.9060 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m0.52202\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1161 | loss: 0.52202 - acc: 0.9141 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m0.49158\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1162 | loss: 0.49158 - acc: 0.9214 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m0.46419\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1163 | loss: 0.46419 - acc: 0.9279 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m0.70228\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1164 | loss: 0.70228 - acc: 0.8628 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m0.65387\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1165 | loss: 0.65387 - acc: 0.8752 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m0.61034\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1166 | loss: 0.61034 - acc: 0.8863 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m0.57119\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1167 | loss: 0.57119 - acc: 0.8964 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m0.53598\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1168 | loss: 0.53598 - acc: 0.9054 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m0.50431\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1169 | loss: 0.50431 - acc: 0.9136 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m0.47582\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1170 | loss: 0.47582 - acc: 0.9209 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m0.45018\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1171 | loss: 0.45018 - acc: 0.9275 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m0.42709\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1172 | loss: 0.42709 - acc: 0.9334 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m0.40630\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1173 | loss: 0.40630 - acc: 0.9388 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m0.69892\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1174 | loss: 0.69892 - acc: 0.8752 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m0.65099\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1175 | loss: 0.65099 - acc: 0.8863 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m0.60789\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1176 | loss: 0.60789 - acc: 0.8964 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m0.56914\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1177 | loss: 0.56914 - acc: 0.9054 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m0.53428\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1178 | loss: 0.53428 - acc: 0.9136 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m0.50293\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1179 | loss: 0.50293 - acc: 0.9209 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m0.47471\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1180 | loss: 0.47471 - acc: 0.9275 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m0.44931\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1181 | loss: 0.44931 - acc: 0.9334 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m0.42644\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1182 | loss: 0.42644 - acc: 0.9388 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m0.40584\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1183 | loss: 0.40584 - acc: 0.9436 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m0.38729\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1184 | loss: 0.38729 - acc: 0.9479 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m0.37057\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1185 | loss: 0.37057 - acc: 0.9518 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m0.68399\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1186 | loss: 0.68399 - acc: 0.8856 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m0.63762\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1187 | loss: 0.63762 - acc: 0.8957 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m0.59593\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1188 | loss: 0.59593 - acc: 0.9048 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m0.55844\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1189 | loss: 0.55844 - acc: 0.9130 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m0.52471\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1190 | loss: 0.52471 - acc: 0.9204 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m0.49437\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1191 | loss: 0.49437 - acc: 0.9270 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m0.73389\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1192 | loss: 0.73389 - acc: 0.8764 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m0.68271\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1193 | loss: 0.68271 - acc: 0.8875 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m0.63670\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1194 | loss: 0.63670 - acc: 0.8974 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m0.59533\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1195 | loss: 0.59533 - acc: 0.9064 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m0.85424\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1196 | loss: 0.85424 - acc: 0.8473 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m0.79126\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1197 | loss: 0.79126 - acc: 0.8613 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m0.98172\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1198 | loss: 0.98172 - acc: 0.8054 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m0.90623\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1199 | loss: 0.90623 - acc: 0.8222 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m1.12677\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1200 | loss: 1.12677 - acc: 0.7768 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m1.03711\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1201 | loss: 1.03711 - acc: 0.7952 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m0.95658\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1202 | loss: 0.95658 - acc: 0.8117 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m0.88422\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1203 | loss: 0.88422 - acc: 0.8266 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m0.81917\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1204 | loss: 0.81917 - acc: 0.8400 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m0.76065\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1205 | loss: 0.76065 - acc: 0.8521 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m0.98592\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1206 | loss: 0.98592 - acc: 0.7998 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m0.91082\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1207 | loss: 0.91082 - acc: 0.8158 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m0.84330\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1208 | loss: 0.84330 - acc: 0.8316 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m0.78257\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1209 | loss: 0.78257 - acc: 0.8458 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m0.72792\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1210 | loss: 0.72792 - acc: 0.8599 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m0.67874\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1211 | loss: 0.67874 - acc: 0.8726 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m0.63447\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1212 | loss: 0.63447 - acc: 0.8840 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m0.59462\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1213 | loss: 0.59462 - acc: 0.8943 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m0.55875\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1214 | loss: 0.55875 - acc: 0.9036 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m0.52646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1215 | loss: 0.52646 - acc: 0.9119 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m0.49740\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1216 | loss: 0.49740 - acc: 0.9194 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m0.47125\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1217 | loss: 0.47125 - acc: 0.9261 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m0.44771\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1218 | loss: 0.44771 - acc: 0.9322 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m0.42653\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1219 | loss: 0.42653 - acc: 0.9364 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m0.40745\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1220 | loss: 0.40745 - acc: 0.9401 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m0.39027\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1221 | loss: 0.39027 - acc: 0.9434 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m0.37478\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1222 | loss: 0.37478 - acc: 0.9465 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m0.36081\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1223 | loss: 0.36081 - acc: 0.9492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m0.34819\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1224 | loss: 0.34819 - acc: 0.9516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m0.33680\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1225 | loss: 0.33680 - acc: 0.9538 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m0.61182\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1226 | loss: 0.61182 - acc: 0.8900 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m0.57401\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1227 | loss: 0.57401 - acc: 0.8997 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m0.53998\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1228 | loss: 0.53998 - acc: 0.9084 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m0.50935\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1229 | loss: 0.50935 - acc: 0.9163 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m0.76336\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1230 | loss: 0.76336 - acc: 0.8536 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m0.71049\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1231 | loss: 0.71049 - acc: 0.8669 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m0.66300\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1232 | loss: 0.66300 - acc: 0.8789 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m0.62033\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1233 | loss: 0.62033 - acc: 0.8897 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m0.58198\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1234 | loss: 0.58198 - acc: 0.8994 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m0.54751\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1235 | loss: 0.54751 - acc: 0.9082 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m0.51649\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1236 | loss: 0.51649 - acc: 0.9160 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m0.48857\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1237 | loss: 0.48857 - acc: 0.9231 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m0.46340\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1238 | loss: 0.46340 - acc: 0.9295 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m0.44072\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1239 | loss: 0.44072 - acc: 0.9352 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m0.69705\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1240 | loss: 0.69705 - acc: 0.8706 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m0.65099\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1241 | loss: 0.65099 - acc: 0.8823 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m0.60955\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1242 | loss: 0.60955 - acc: 0.8927 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m0.57227\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1243 | loss: 0.57227 - acc: 0.9021 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m0.53871\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1244 | loss: 0.53871 - acc: 0.9106 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m0.50849\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1245 | loss: 0.50849 - acc: 0.9182 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m0.48127\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1246 | loss: 0.48127 - acc: 0.9251 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m0.45676\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1247 | loss: 0.45676 - acc: 0.9313 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m0.63867\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1248 | loss: 0.63867 - acc: 0.8816 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m0.59842\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1249 | loss: 0.59842 - acc: 0.8921 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m0.56221\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1250 | loss: 0.56221 - acc: 0.9016 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m0.52964\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1251 | loss: 0.52964 - acc: 0.9101 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m0.72254\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1252 | loss: 0.72254 - acc: 0.8559 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m0.67400\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1253 | loss: 0.67400 - acc: 0.8690 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m0.63036\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1254 | loss: 0.63036 - acc: 0.8808 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m0.59113\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1255 | loss: 0.59113 - acc: 0.8914 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m0.55584\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1256 | loss: 0.55584 - acc: 0.9009 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m0.52410\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1257 | loss: 0.52410 - acc: 0.9095 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m0.49555\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1258 | loss: 0.49555 - acc: 0.9173 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m0.46985\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1259 | loss: 0.46985 - acc: 0.9242 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m0.44672\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1260 | loss: 0.44672 - acc: 0.9305 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m0.42589\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1261 | loss: 0.42589 - acc: 0.9361 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m0.40712\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1262 | loss: 0.40712 - acc: 0.9412 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m0.39021\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1263 | loss: 0.39021 - acc: 0.9458 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m0.37497\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1264 | loss: 0.37497 - acc: 0.9499 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m0.36122\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1265 | loss: 0.36122 - acc: 0.9536 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m0.59918\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1266 | loss: 0.59918 - acc: 0.8872 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m0.56300\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1267 | loss: 0.56300 - acc: 0.8971 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m0.80827\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1268 | loss: 0.80827 - acc: 0.8443 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m0.75128\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1269 | loss: 0.75128 - acc: 0.8585 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m0.70007\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1270 | loss: 0.70007 - acc: 0.8713 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m0.65403\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1271 | loss: 0.65403 - acc: 0.8829 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m0.61265\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1272 | loss: 0.61265 - acc: 0.8933 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m0.57544\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1273 | loss: 0.57544 - acc: 0.9026 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m0.54196\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1274 | loss: 0.54196 - acc: 0.9111 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m0.51184\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1275 | loss: 0.51184 - acc: 0.9186 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m0.75466\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1276 | loss: 0.75466 - acc: 0.8570 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m0.70332\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1277 | loss: 0.70332 - acc: 0.8700 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m0.65718\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1278 | loss: 0.65718 - acc: 0.8817 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m0.61568\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1279 | loss: 0.61568 - acc: 0.8922 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m0.57835\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1280 | loss: 0.57835 - acc: 0.9017 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m0.54475\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1281 | loss: 0.54475 - acc: 0.9102 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m0.51450\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1282 | loss: 0.51450 - acc: 0.9179 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m0.48726\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1283 | loss: 0.48726 - acc: 0.9248 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m0.46272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1284 | loss: 0.46272 - acc: 0.9310 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m0.44060\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1285 | loss: 0.44060 - acc: 0.9366 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m0.66493\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1286 | loss: 0.66493 - acc: 0.8758 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m0.62260\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1287 | loss: 0.62260 - acc: 0.8869 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m0.58453\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1288 | loss: 0.58453 - acc: 0.8969 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m0.55028\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1289 | loss: 0.55028 - acc: 0.9059 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m0.51947\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1290 | loss: 0.51947 - acc: 0.9140 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m0.49175\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1291 | loss: 0.49175 - acc: 0.9213 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m0.46679\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1292 | loss: 0.46679 - acc: 0.9278 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m0.44432\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1293 | loss: 0.44432 - acc: 0.9337 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m0.70865\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1294 | loss: 0.70865 - acc: 0.8746 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m0.66203\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1295 | loss: 0.66203 - acc: 0.8858 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m0.62011\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1296 | loss: 0.62011 - acc: 0.8959 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m0.58241\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1297 | loss: 0.58241 - acc: 0.9050 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m0.54851\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1298 | loss: 0.54851 - acc: 0.9132 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m0.51801\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1299 | loss: 0.51801 - acc: 0.9205 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m0.49056\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1300 | loss: 0.49056 - acc: 0.9272 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1301  | total loss: \u001b[1m\u001b[32m0.46586\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1301 | loss: 0.46586 - acc: 0.9331 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1302  | total loss: \u001b[1m\u001b[32m0.44361\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1302 | loss: 0.44361 - acc: 0.9385 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1303  | total loss: \u001b[1m\u001b[32m0.42357\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1303 | loss: 0.42357 - acc: 0.9433 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1304  | total loss: \u001b[1m\u001b[32m0.40551\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1304 | loss: 0.40551 - acc: 0.9477 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1305  | total loss: \u001b[1m\u001b[32m0.38923\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1305 | loss: 0.38923 - acc: 0.9516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1306  | total loss: \u001b[1m\u001b[32m0.37454\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1306 | loss: 0.37454 - acc: 0.9551 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1307  | total loss: \u001b[1m\u001b[32m0.36128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1307 | loss: 0.36128 - acc: 0.9583 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1308  | total loss: \u001b[1m\u001b[32m0.34931\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1308 | loss: 0.34931 - acc: 0.9612 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1309  | total loss: \u001b[1m\u001b[32m0.33850\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1309 | loss: 0.33850 - acc: 0.9637 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1310  | total loss: \u001b[1m\u001b[32m0.55584\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1310 | loss: 0.55584 - acc: 0.9042 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1311  | total loss: \u001b[1m\u001b[32m0.52434\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1311 | loss: 0.52434 - acc: 0.9125 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1312  | total loss: \u001b[1m\u001b[32m0.49599\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1312 | loss: 0.49599 - acc: 0.9199 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1313  | total loss: \u001b[1m\u001b[32m0.47047\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1313 | loss: 0.47047 - acc: 0.9266 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1314  | total loss: \u001b[1m\u001b[32m0.44749\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1314 | loss: 0.44749 - acc: 0.9326 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1315  | total loss: \u001b[1m\u001b[32m0.42680\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1315 | loss: 0.42680 - acc: 0.9380 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1316  | total loss: \u001b[1m\u001b[32m0.40815\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1316 | loss: 0.40815 - acc: 0.9429 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1317  | total loss: \u001b[1m\u001b[32m0.39135\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1317 | loss: 0.39135 - acc: 0.9473 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1318  | total loss: \u001b[1m\u001b[32m0.37620\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1318 | loss: 0.37620 - acc: 0.9513 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1319  | total loss: \u001b[1m\u001b[32m0.36253\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1319 | loss: 0.36253 - acc: 0.9548 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.35019\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1320 | loss: 0.35019 - acc: 0.9580 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1321  | total loss: \u001b[1m\u001b[32m0.33904\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1321 | loss: 0.33904 - acc: 0.9609 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1322  | total loss: \u001b[1m\u001b[32m0.32898\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1322 | loss: 0.32898 - acc: 0.9635 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1323  | total loss: \u001b[1m\u001b[32m0.31987\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1323 | loss: 0.31987 - acc: 0.9658 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1324  | total loss: \u001b[1m\u001b[32m0.31164\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1324 | loss: 0.31164 - acc: 0.9679 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1325  | total loss: \u001b[1m\u001b[32m0.30418\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1325 | loss: 0.30418 - acc: 0.9698 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1326  | total loss: \u001b[1m\u001b[32m0.29743\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1326 | loss: 0.29743 - acc: 0.9715 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1327  | total loss: \u001b[1m\u001b[32m0.29130\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1327 | loss: 0.29130 - acc: 0.9731 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1328  | total loss: \u001b[1m\u001b[32m0.53789\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1328 | loss: 0.53789 - acc: 0.9113 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1329  | total loss: \u001b[1m\u001b[32m0.50767\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1329 | loss: 0.50767 - acc: 0.9188 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1330  | total loss: \u001b[1m\u001b[32m0.48048\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1330 | loss: 0.48048 - acc: 0.9256 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1331  | total loss: \u001b[1m\u001b[32m0.45600\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1331 | loss: 0.45600 - acc: 0.9318 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1332  | total loss: \u001b[1m\u001b[32m0.43395\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1332 | loss: 0.43395 - acc: 0.9373 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1333  | total loss: \u001b[1m\u001b[32m0.41410\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1333 | loss: 0.41410 - acc: 0.9422 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1334  | total loss: \u001b[1m\u001b[32m0.39621\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1334 | loss: 0.39621 - acc: 0.9467 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1335  | total loss: \u001b[1m\u001b[32m0.38009\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1335 | loss: 0.38009 - acc: 0.9507 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1336  | total loss: \u001b[1m\u001b[32m0.36556\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1336 | loss: 0.36556 - acc: 0.9543 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1337  | total loss: \u001b[1m\u001b[32m0.35245\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1337 | loss: 0.35245 - acc: 0.9576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1338  | total loss: \u001b[1m\u001b[32m0.34061\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1338 | loss: 0.34061 - acc: 0.9605 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1339  | total loss: \u001b[1m\u001b[32m0.32992\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1339 | loss: 0.32992 - acc: 0.9631 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1340  | total loss: \u001b[1m\u001b[32m0.32026\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1340 | loss: 0.32026 - acc: 0.9655 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1341  | total loss: \u001b[1m\u001b[32m0.31153\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1341 | loss: 0.31153 - acc: 0.9676 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1342  | total loss: \u001b[1m\u001b[32m0.30363\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1342 | loss: 0.30363 - acc: 0.9696 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1343  | total loss: \u001b[1m\u001b[32m0.29648\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1343 | loss: 0.29648 - acc: 0.9713 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1344  | total loss: \u001b[1m\u001b[32m0.29000\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1344 | loss: 0.29000 - acc: 0.9728 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1345  | total loss: \u001b[1m\u001b[32m0.28413\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1345 | loss: 0.28413 - acc: 0.9742 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1346  | total loss: \u001b[1m\u001b[32m0.27881\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1346 | loss: 0.27881 - acc: 0.9755 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1347  | total loss: \u001b[1m\u001b[32m0.27397\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1347 | loss: 0.27397 - acc: 0.9766 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1348  | total loss: \u001b[1m\u001b[32m0.26957\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1348 | loss: 0.26957 - acc: 0.9777 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1349  | total loss: \u001b[1m\u001b[32m0.26557\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1349 | loss: 0.26557 - acc: 0.9786 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1350  | total loss: \u001b[1m\u001b[32m0.26193\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1350 | loss: 0.26193 - acc: 0.9794 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1351  | total loss: \u001b[1m\u001b[32m0.25861\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1351 | loss: 0.25861 - acc: 0.9801 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1352  | total loss: \u001b[1m\u001b[32m0.25557\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1352 | loss: 0.25557 - acc: 0.9808 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1353  | total loss: \u001b[1m\u001b[32m0.25280\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1353 | loss: 0.25280 - acc: 0.9814 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1354  | total loss: \u001b[1m\u001b[32m0.25026\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1354 | loss: 0.25026 - acc: 0.9820 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1355  | total loss: \u001b[1m\u001b[32m0.24793\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1355 | loss: 0.24793 - acc: 0.9824 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1356  | total loss: \u001b[1m\u001b[32m0.57271\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1356 | loss: 0.57271 - acc: 0.9145 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1357  | total loss: \u001b[1m\u001b[32m0.53809\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1357 | loss: 0.53809 - acc: 0.9217 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1358  | total loss: \u001b[1m\u001b[32m0.50696\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1358 | loss: 0.50696 - acc: 0.9282 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1359  | total loss: \u001b[1m\u001b[32m0.47894\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1359 | loss: 0.47894 - acc: 0.9341 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1360  | total loss: \u001b[1m\u001b[32m0.45373\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1360 | loss: 0.45373 - acc: 0.9394 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1361  | total loss: \u001b[1m\u001b[32m0.43104\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1361 | loss: 0.43104 - acc: 0.9441 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1362  | total loss: \u001b[1m\u001b[32m0.41062\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1362 | loss: 0.41062 - acc: 0.9484 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1363  | total loss: \u001b[1m\u001b[32m0.39223\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1363 | loss: 0.39223 - acc: 0.9522 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1364  | total loss: \u001b[1m\u001b[32m0.37566\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1364 | loss: 0.37566 - acc: 0.9557 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1365  | total loss: \u001b[1m\u001b[32m0.36073\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1365 | loss: 0.36073 - acc: 0.9588 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1366  | total loss: \u001b[1m\u001b[32m0.34727\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1366 | loss: 0.34727 - acc: 0.9616 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1367  | total loss: \u001b[1m\u001b[32m0.33512\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1367 | loss: 0.33512 - acc: 0.9641 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1368  | total loss: \u001b[1m\u001b[32m0.32415\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1368 | loss: 0.32415 - acc: 0.9664 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1369  | total loss: \u001b[1m\u001b[32m0.31424\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1369 | loss: 0.31424 - acc: 0.9684 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1370  | total loss: \u001b[1m\u001b[32m0.30529\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1370 | loss: 0.30529 - acc: 0.9703 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1371  | total loss: \u001b[1m\u001b[32m0.29720\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1371 | loss: 0.29720 - acc: 0.9719 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1372  | total loss: \u001b[1m\u001b[32m0.28989\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1372 | loss: 0.28989 - acc: 0.9734 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1373  | total loss: \u001b[1m\u001b[32m0.28327\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1373 | loss: 0.28327 - acc: 0.9748 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1374  | total loss: \u001b[1m\u001b[32m0.27728\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1374 | loss: 0.27728 - acc: 0.9760 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1375  | total loss: \u001b[1m\u001b[32m0.27186\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1375 | loss: 0.27186 - acc: 0.9771 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1376  | total loss: \u001b[1m\u001b[32m0.26694\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1376 | loss: 0.26694 - acc: 0.9780 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1377  | total loss: \u001b[1m\u001b[32m0.26249\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1377 | loss: 0.26249 - acc: 0.9789 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1378  | total loss: \u001b[1m\u001b[32m0.25844\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1378 | loss: 0.25844 - acc: 0.9797 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1379  | total loss: \u001b[1m\u001b[32m0.25477\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1379 | loss: 0.25477 - acc: 0.9804 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1380  | total loss: \u001b[1m\u001b[32m0.25142\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1380 | loss: 0.25142 - acc: 0.9811 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1381  | total loss: \u001b[1m\u001b[32m0.24838\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1381 | loss: 0.24838 - acc: 0.9816 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1382  | total loss: \u001b[1m\u001b[32m0.24560\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1382 | loss: 0.24560 - acc: 0.9822 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1383  | total loss: \u001b[1m\u001b[32m0.24307\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1383 | loss: 0.24307 - acc: 0.9826 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1384  | total loss: \u001b[1m\u001b[32m0.24075\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1384 | loss: 0.24075 - acc: 0.9831 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1385  | total loss: \u001b[1m\u001b[32m0.23863\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1385 | loss: 0.23863 - acc: 0.9834 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1386  | total loss: \u001b[1m\u001b[32m0.23668\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1386 | loss: 0.23668 - acc: 0.9838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1387  | total loss: \u001b[1m\u001b[32m0.23489\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1387 | loss: 0.23489 - acc: 0.9841 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1388  | total loss: \u001b[1m\u001b[32m0.23324\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1388 | loss: 0.23324 - acc: 0.9844 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1389  | total loss: \u001b[1m\u001b[32m0.23172\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1389 | loss: 0.23172 - acc: 0.9846 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1390  | total loss: \u001b[1m\u001b[32m0.52980\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1390 | loss: 0.52980 - acc: 0.9230 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1391  | total loss: \u001b[1m\u001b[32m0.49860\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1391 | loss: 0.49860 - acc: 0.9294 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1392  | total loss: \u001b[1m\u001b[32m0.47053\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1392 | loss: 0.47053 - acc: 0.9351 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1393  | total loss: \u001b[1m\u001b[32m0.44529\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1393 | loss: 0.44529 - acc: 0.9403 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1394  | total loss: \u001b[1m\u001b[32m0.66871\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1394 | loss: 0.66871 - acc: 0.8910 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m0.62372\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1395 | loss: 0.62372 - acc: 0.9006 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m0.58328\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1396 | loss: 0.58328 - acc: 0.9092 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1397  | total loss: \u001b[1m\u001b[32m0.54694\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1397 | loss: 0.54694 - acc: 0.9170 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1398  | total loss: \u001b[1m\u001b[32m0.51425\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1398 | loss: 0.51425 - acc: 0.9240 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1399  | total loss: \u001b[1m\u001b[32m0.48485\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1399 | loss: 0.48485 - acc: 0.9302 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1400  | total loss: \u001b[1m\u001b[32m0.45839\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1400 | loss: 0.45839 - acc: 0.9359 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1401  | total loss: \u001b[1m\u001b[32m0.43456\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1401 | loss: 0.43456 - acc: 0.9410 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1402  | total loss: \u001b[1m\u001b[32m0.41308\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1402 | loss: 0.41308 - acc: 0.9456 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1403  | total loss: \u001b[1m\u001b[32m0.39373\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1403 | loss: 0.39373 - acc: 0.9497 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1404  | total loss: \u001b[1m\u001b[32m0.37628\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1404 | loss: 0.37628 - acc: 0.9534 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1405  | total loss: \u001b[1m\u001b[32m0.36054\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1405 | loss: 0.36054 - acc: 0.9568 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1406  | total loss: \u001b[1m\u001b[32m0.34634\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1406 | loss: 0.34634 - acc: 0.9598 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1407  | total loss: \u001b[1m\u001b[32m0.33354\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1407 | loss: 0.33354 - acc: 0.9625 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1408  | total loss: \u001b[1m\u001b[32m0.65042\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1408 | loss: 0.65042 - acc: 0.8952 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1409  | total loss: \u001b[1m\u001b[32m0.60722\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1409 | loss: 0.60722 - acc: 0.9043 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m0.56836\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1410 | loss: 0.56836 - acc: 0.9126 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1411  | total loss: \u001b[1m\u001b[32m0.53340\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1411 | loss: 0.53340 - acc: 0.9200 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1412  | total loss: \u001b[1m\u001b[32m0.50195\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1412 | loss: 0.50195 - acc: 0.9267 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1413  | total loss: \u001b[1m\u001b[32m0.47365\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1413 | loss: 0.47365 - acc: 0.9327 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1414  | total loss: \u001b[1m\u001b[32m0.44819\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1414 | loss: 0.44819 - acc: 0.9381 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1415  | total loss: \u001b[1m\u001b[32m0.42527\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1415 | loss: 0.42527 - acc: 0.9430 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1416  | total loss: \u001b[1m\u001b[32m0.65096\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1416 | loss: 0.65096 - acc: 0.8842 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1417  | total loss: \u001b[1m\u001b[32m0.60779\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1417 | loss: 0.60779 - acc: 0.8945 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1418  | total loss: \u001b[1m\u001b[32m0.56897\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1418 | loss: 0.56897 - acc: 0.9037 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1419  | total loss: \u001b[1m\u001b[32m0.53405\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1419 | loss: 0.53405 - acc: 0.9120 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1420  | total loss: \u001b[1m\u001b[32m0.50264\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1420 | loss: 0.50264 - acc: 0.9195 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1421  | total loss: \u001b[1m\u001b[32m0.47437\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1421 | loss: 0.47437 - acc: 0.9262 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1422  | total loss: \u001b[1m\u001b[32m0.44894\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1422 | loss: 0.44894 - acc: 0.9323 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1423  | total loss: \u001b[1m\u001b[32m0.42605\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1423 | loss: 0.42605 - acc: 0.9378 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1424  | total loss: \u001b[1m\u001b[32m0.40545\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1424 | loss: 0.40545 - acc: 0.9427 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1425  | total loss: \u001b[1m\u001b[32m0.38690\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1425 | loss: 0.38690 - acc: 0.9471 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1426  | total loss: \u001b[1m\u001b[32m0.37019\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1426 | loss: 0.37019 - acc: 0.9511 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1427  | total loss: \u001b[1m\u001b[32m0.35513\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1427 | loss: 0.35513 - acc: 0.9546 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1428  | total loss: \u001b[1m\u001b[32m0.34157\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1428 | loss: 0.34157 - acc: 0.9579 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1429  | total loss: \u001b[1m\u001b[32m0.32934\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1429 | loss: 0.32934 - acc: 0.9608 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1430  | total loss: \u001b[1m\u001b[32m0.61434\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1430 | loss: 0.61434 - acc: 0.8897 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1431  | total loss: \u001b[1m\u001b[32m0.57484\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1431 | loss: 0.57484 - acc: 0.8994 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1432  | total loss: \u001b[1m\u001b[32m0.53932\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1432 | loss: 0.53932 - acc: 0.9081 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1433  | total loss: \u001b[1m\u001b[32m0.50736\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1433 | loss: 0.50736 - acc: 0.9160 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1434  | total loss: \u001b[1m\u001b[32m0.47861\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1434 | loss: 0.47861 - acc: 0.9231 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1435  | total loss: \u001b[1m\u001b[32m0.45274\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1435 | loss: 0.45274 - acc: 0.9295 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1436  | total loss: \u001b[1m\u001b[32m0.42945\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1436 | loss: 0.42945 - acc: 0.9352 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1437  | total loss: \u001b[1m\u001b[32m0.40849\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1437 | loss: 0.40849 - acc: 0.9404 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1438  | total loss: \u001b[1m\u001b[32m0.38962\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1438 | loss: 0.38962 - acc: 0.9450 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1439  | total loss: \u001b[1m\u001b[32m0.37262\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1439 | loss: 0.37262 - acc: 0.9492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1440  | total loss: \u001b[1m\u001b[32m0.35730\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1440 | loss: 0.35730 - acc: 0.9530 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1441  | total loss: \u001b[1m\u001b[32m0.34350\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1441 | loss: 0.34350 - acc: 0.9564 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1442  | total loss: \u001b[1m\u001b[32m0.33105\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1442 | loss: 0.33105 - acc: 0.9594 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1443  | total loss: \u001b[1m\u001b[32m0.31983\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1443 | loss: 0.31983 - acc: 0.9621 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1444  | total loss: \u001b[1m\u001b[32m0.30970\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1444 | loss: 0.30970 - acc: 0.9646 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1445  | total loss: \u001b[1m\u001b[32m0.30056\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1445 | loss: 0.30056 - acc: 0.9668 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1446  | total loss: \u001b[1m\u001b[32m0.29231\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1446 | loss: 0.29231 - acc: 0.9688 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1447  | total loss: \u001b[1m\u001b[32m0.28486\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1447 | loss: 0.28486 - acc: 0.9706 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1448  | total loss: \u001b[1m\u001b[32m0.27813\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1448 | loss: 0.27813 - acc: 0.9723 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1449  | total loss: \u001b[1m\u001b[32m0.27204\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1449 | loss: 0.27204 - acc: 0.9737 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1450  | total loss: \u001b[1m\u001b[32m0.26653\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1450 | loss: 0.26653 - acc: 0.9750 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1451  | total loss: \u001b[1m\u001b[32m0.26154\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1451 | loss: 0.26154 - acc: 0.9762 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1452  | total loss: \u001b[1m\u001b[32m0.25703\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1452 | loss: 0.25703 - acc: 0.9773 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1453  | total loss: \u001b[1m\u001b[32m0.25293\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1453 | loss: 0.25293 - acc: 0.9782 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1454  | total loss: \u001b[1m\u001b[32m0.24921\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1454 | loss: 0.24921 - acc: 0.9791 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1455  | total loss: \u001b[1m\u001b[32m0.24583\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1455 | loss: 0.24583 - acc: 0.9799 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1456  | total loss: \u001b[1m\u001b[32m0.24276\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1456 | loss: 0.24276 - acc: 0.9806 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1457  | total loss: \u001b[1m\u001b[32m0.23996\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1457 | loss: 0.23996 - acc: 0.9812 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1458  | total loss: \u001b[1m\u001b[32m0.23741\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1458 | loss: 0.23741 - acc: 0.9818 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1459  | total loss: \u001b[1m\u001b[32m0.23508\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1459 | loss: 0.23508 - acc: 0.9823 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1460  | total loss: \u001b[1m\u001b[32m0.23295\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1460 | loss: 0.23295 - acc: 0.9827 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1461  | total loss: \u001b[1m\u001b[32m0.23101\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1461 | loss: 0.23101 - acc: 0.9831 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1462  | total loss: \u001b[1m\u001b[32m0.22922\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1462 | loss: 0.22922 - acc: 0.9835 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1463  | total loss: \u001b[1m\u001b[32m0.22758\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1463 | loss: 0.22758 - acc: 0.9838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1464  | total loss: \u001b[1m\u001b[32m0.22607\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1464 | loss: 0.22607 - acc: 0.9841 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1465  | total loss: \u001b[1m\u001b[32m0.22468\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1465 | loss: 0.22468 - acc: 0.9844 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1466  | total loss: \u001b[1m\u001b[32m0.22339\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1466 | loss: 0.22339 - acc: 0.9847 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1467  | total loss: \u001b[1m\u001b[32m0.22221\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1467 | loss: 0.22221 - acc: 0.9849 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1468  | total loss: \u001b[1m\u001b[32m0.54256\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1468 | loss: 0.54256 - acc: 0.9166 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1469  | total loss: \u001b[1m\u001b[32m0.50945\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1469 | loss: 0.50945 - acc: 0.9237 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1470  | total loss: \u001b[1m\u001b[32m0.70607\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1470 | loss: 0.70607 - acc: 0.8642 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1471  | total loss: \u001b[1m\u001b[32m0.65668\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1471 | loss: 0.65668 - acc: 0.8765 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1472  | total loss: \u001b[1m\u001b[32m0.61228\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1472 | loss: 0.61228 - acc: 0.8875 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1473  | total loss: \u001b[1m\u001b[32m0.57236\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1473 | loss: 0.57236 - acc: 0.8974 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1474  | total loss: \u001b[1m\u001b[32m0.53648\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1474 | loss: 0.53648 - acc: 0.9064 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1475  | total loss: \u001b[1m\u001b[32m0.50420\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1475 | loss: 0.50420 - acc: 0.9144 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1476  | total loss: \u001b[1m\u001b[32m0.47517\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1476 | loss: 0.47517 - acc: 0.9217 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1477  | total loss: \u001b[1m\u001b[32m0.44904\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1477 | loss: 0.44904 - acc: 0.9282 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1478  | total loss: \u001b[1m\u001b[32m0.42551\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1478 | loss: 0.42551 - acc: 0.9340 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1479  | total loss: \u001b[1m\u001b[32m0.40432\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1479 | loss: 0.40432 - acc: 0.9393 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1480  | total loss: \u001b[1m\u001b[32m0.38523\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1480 | loss: 0.38523 - acc: 0.9441 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1481  | total loss: \u001b[1m\u001b[32m0.36803\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1481 | loss: 0.36803 - acc: 0.9484 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1482  | total loss: \u001b[1m\u001b[32m0.35252\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1482 | loss: 0.35252 - acc: 0.9522 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1483  | total loss: \u001b[1m\u001b[32m0.33855\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1483 | loss: 0.33855 - acc: 0.9557 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1484  | total loss: \u001b[1m\u001b[32m0.32595\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1484 | loss: 0.32595 - acc: 0.9588 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1485  | total loss: \u001b[1m\u001b[32m0.31459\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1485 | loss: 0.31459 - acc: 0.9616 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1486  | total loss: \u001b[1m\u001b[32m0.30434\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1486 | loss: 0.30434 - acc: 0.9641 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1487  | total loss: \u001b[1m\u001b[32m0.29511\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1487 | loss: 0.29511 - acc: 0.9664 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1488  | total loss: \u001b[1m\u001b[32m0.28677\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1488 | loss: 0.28677 - acc: 0.9684 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1489  | total loss: \u001b[1m\u001b[32m0.27926\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1489 | loss: 0.27926 - acc: 0.9703 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1490  | total loss: \u001b[1m\u001b[32m0.27247\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1490 | loss: 0.27247 - acc: 0.9719 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1491  | total loss: \u001b[1m\u001b[32m0.26634\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1491 | loss: 0.26634 - acc: 0.9734 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1492  | total loss: \u001b[1m\u001b[32m0.26080\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1492 | loss: 0.26080 - acc: 0.9748 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1493  | total loss: \u001b[1m\u001b[32m0.25578\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1493 | loss: 0.25578 - acc: 0.9760 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1494  | total loss: \u001b[1m\u001b[32m0.25124\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1494 | loss: 0.25124 - acc: 0.9771 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1495  | total loss: \u001b[1m\u001b[32m0.24713\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1495 | loss: 0.24713 - acc: 0.9780 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1496  | total loss: \u001b[1m\u001b[32m0.52510\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1496 | loss: 0.52510 - acc: 0.9171 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1497  | total loss: \u001b[1m\u001b[32m0.49357\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1497 | loss: 0.49357 - acc: 0.9241 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1498  | total loss: \u001b[1m\u001b[32m0.46520\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1498 | loss: 0.46520 - acc: 0.9303 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1499  | total loss: \u001b[1m\u001b[32m0.43968\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1499 | loss: 0.43968 - acc: 0.9360 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1500  | total loss: \u001b[1m\u001b[32m0.41671\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1500 | loss: 0.41671 - acc: 0.9411 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1501  | total loss: \u001b[1m\u001b[32m0.39604\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1501 | loss: 0.39604 - acc: 0.9456 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1502  | total loss: \u001b[1m\u001b[32m0.37743\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1502 | loss: 0.37743 - acc: 0.9498 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1503  | total loss: \u001b[1m\u001b[32m0.36068\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1503 | loss: 0.36068 - acc: 0.9535 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1504  | total loss: \u001b[1m\u001b[32m0.34560\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1504 | loss: 0.34560 - acc: 0.9568 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1505  | total loss: \u001b[1m\u001b[32m0.33202\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1505 | loss: 0.33202 - acc: 0.9598 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1506  | total loss: \u001b[1m\u001b[32m0.31978\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1506 | loss: 0.31978 - acc: 0.9625 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1507  | total loss: \u001b[1m\u001b[32m0.30874\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1507 | loss: 0.30874 - acc: 0.9649 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1508  | total loss: \u001b[1m\u001b[32m0.29878\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1508 | loss: 0.29878 - acc: 0.9671 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1509  | total loss: \u001b[1m\u001b[32m0.28980\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1509 | loss: 0.28980 - acc: 0.9691 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1510  | total loss: \u001b[1m\u001b[32m0.28169\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1510 | loss: 0.28169 - acc: 0.9709 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1511  | total loss: \u001b[1m\u001b[32m0.27437\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1511 | loss: 0.27437 - acc: 0.9725 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1512  | total loss: \u001b[1m\u001b[32m0.26775\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1512 | loss: 0.26775 - acc: 0.9739 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1513  | total loss: \u001b[1m\u001b[32m0.26178\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1513 | loss: 0.26178 - acc: 0.9752 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1514  | total loss: \u001b[1m\u001b[32m0.25637\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1514 | loss: 0.25637 - acc: 0.9764 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1515  | total loss: \u001b[1m\u001b[32m0.25148\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1515 | loss: 0.25148 - acc: 0.9774 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1516  | total loss: \u001b[1m\u001b[32m0.24705\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1516 | loss: 0.24705 - acc: 0.9784 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1517  | total loss: \u001b[1m\u001b[32m0.24304\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1517 | loss: 0.24304 - acc: 0.9792 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1518  | total loss: \u001b[1m\u001b[32m0.45693\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1518 | loss: 0.45693 - acc: 0.9247 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m0.43191\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1519 | loss: 0.43191 - acc: 0.9309 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m0.40939\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1520 | loss: 0.40939 - acc: 0.9365 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1521  | total loss: \u001b[1m\u001b[32m0.38912\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1521 | loss: 0.38912 - acc: 0.9415 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1522  | total loss: \u001b[1m\u001b[32m0.37088\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1522 | loss: 0.37088 - acc: 0.9461 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1523  | total loss: \u001b[1m\u001b[32m0.35445\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1523 | loss: 0.35445 - acc: 0.9502 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1524  | total loss: \u001b[1m\u001b[32m0.69162\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1524 | loss: 0.69162 - acc: 0.8841 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1525  | total loss: \u001b[1m\u001b[32m0.64315\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1525 | loss: 0.64315 - acc: 0.8944 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1526  | total loss: \u001b[1m\u001b[32m0.59957\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1526 | loss: 0.59957 - acc: 0.9036 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1527  | total loss: \u001b[1m\u001b[32m0.56038\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1527 | loss: 0.56038 - acc: 0.9119 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1528  | total loss: \u001b[1m\u001b[32m0.52514\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1528 | loss: 0.52514 - acc: 0.9194 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1529  | total loss: \u001b[1m\u001b[32m0.49345\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1529 | loss: 0.49345 - acc: 0.9262 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1530  | total loss: \u001b[1m\u001b[32m0.46493\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1530 | loss: 0.46493 - acc: 0.9322 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1531  | total loss: \u001b[1m\u001b[32m0.43928\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1531 | loss: 0.43928 - acc: 0.9377 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1532  | total loss: \u001b[1m\u001b[32m0.41618\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1532 | loss: 0.41618 - acc: 0.9426 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1533  | total loss: \u001b[1m\u001b[32m0.39538\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1533 | loss: 0.39538 - acc: 0.9470 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1534  | total loss: \u001b[1m\u001b[32m0.37665\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1534 | loss: 0.37665 - acc: 0.9510 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1535  | total loss: \u001b[1m\u001b[32m0.35978\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1535 | loss: 0.35978 - acc: 0.9546 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1536  | total loss: \u001b[1m\u001b[32m0.34457\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1536 | loss: 0.34457 - acc: 0.9578 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1537  | total loss: \u001b[1m\u001b[32m0.33087\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1537 | loss: 0.33087 - acc: 0.9607 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1538  | total loss: \u001b[1m\u001b[32m0.31851\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1538 | loss: 0.31851 - acc: 0.9633 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1539  | total loss: \u001b[1m\u001b[32m0.30738\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1539 | loss: 0.30738 - acc: 0.9657 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.29733\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1540 | loss: 0.29733 - acc: 0.9678 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1541  | total loss: \u001b[1m\u001b[32m0.28828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1541 | loss: 0.28828 - acc: 0.9697 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1542  | total loss: \u001b[1m\u001b[32m0.28011\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1542 | loss: 0.28011 - acc: 0.9714 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1543  | total loss: \u001b[1m\u001b[32m0.27273\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1543 | loss: 0.27273 - acc: 0.9730 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1544  | total loss: \u001b[1m\u001b[32m0.26608\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1544 | loss: 0.26608 - acc: 0.9743 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1545  | total loss: \u001b[1m\u001b[32m0.26007\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1545 | loss: 0.26007 - acc: 0.9756 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1546  | total loss: \u001b[1m\u001b[32m0.25463\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1546 | loss: 0.25463 - acc: 0.9767 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1547  | total loss: \u001b[1m\u001b[32m0.24972\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1547 | loss: 0.24972 - acc: 0.9777 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1548  | total loss: \u001b[1m\u001b[32m0.24527\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1548 | loss: 0.24527 - acc: 0.9786 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1549  | total loss: \u001b[1m\u001b[32m0.24124\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1549 | loss: 0.24124 - acc: 0.9795 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1550  | total loss: \u001b[1m\u001b[32m0.23759\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1550 | loss: 0.23759 - acc: 0.9802 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1551  | total loss: \u001b[1m\u001b[32m0.23427\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1551 | loss: 0.23427 - acc: 0.9809 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1552  | total loss: \u001b[1m\u001b[32m0.23126\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1552 | loss: 0.23126 - acc: 0.9815 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1553  | total loss: \u001b[1m\u001b[32m0.22852\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1553 | loss: 0.22852 - acc: 0.9820 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1554  | total loss: \u001b[1m\u001b[32m0.22602\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1554 | loss: 0.22602 - acc: 0.9825 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1555  | total loss: \u001b[1m\u001b[32m0.22375\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1555 | loss: 0.22375 - acc: 0.9829 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1556  | total loss: \u001b[1m\u001b[32m0.55553\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1556 | loss: 0.55553 - acc: 0.9123 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1557  | total loss: \u001b[1m\u001b[32m0.52029\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1557 | loss: 0.52029 - acc: 0.9197 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1558  | total loss: \u001b[1m\u001b[32m0.48860\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1558 | loss: 0.48860 - acc: 0.9264 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1559  | total loss: \u001b[1m\u001b[32m0.46009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1559 | loss: 0.46009 - acc: 0.9325 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1560  | total loss: \u001b[1m\u001b[32m0.43444\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1560 | loss: 0.43444 - acc: 0.9379 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1561  | total loss: \u001b[1m\u001b[32m0.41136\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1561 | loss: 0.41136 - acc: 0.9428 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1562  | total loss: \u001b[1m\u001b[32m0.39059\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1562 | loss: 0.39059 - acc: 0.9472 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1563  | total loss: \u001b[1m\u001b[32m0.37190\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1563 | loss: 0.37190 - acc: 0.9512 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1564  | total loss: \u001b[1m\u001b[32m0.35506\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1564 | loss: 0.35506 - acc: 0.9547 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1565  | total loss: \u001b[1m\u001b[32m0.33990\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1565 | loss: 0.33990 - acc: 0.9579 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1566  | total loss: \u001b[1m\u001b[32m0.32623\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1566 | loss: 0.32623 - acc: 0.9608 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1567  | total loss: \u001b[1m\u001b[32m0.31392\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1567 | loss: 0.31392 - acc: 0.9634 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1568  | total loss: \u001b[1m\u001b[32m0.30281\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1568 | loss: 0.30281 - acc: 0.9658 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m0.29280\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1569 | loss: 0.29280 - acc: 0.9679 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m0.28376\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1570 | loss: 0.28376 - acc: 0.9698 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1571  | total loss: \u001b[1m\u001b[32m0.27561\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1571 | loss: 0.27561 - acc: 0.9715 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1572  | total loss: \u001b[1m\u001b[32m0.26825\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1572 | loss: 0.26825 - acc: 0.9730 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1573  | total loss: \u001b[1m\u001b[32m0.26160\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1573 | loss: 0.26160 - acc: 0.9744 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1574  | total loss: \u001b[1m\u001b[32m0.25560\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1574 | loss: 0.25560 - acc: 0.9756 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1575  | total loss: \u001b[1m\u001b[32m0.25017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1575 | loss: 0.25017 - acc: 0.9768 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1576  | total loss: \u001b[1m\u001b[32m0.24527\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1576 | loss: 0.24527 - acc: 0.9778 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1577  | total loss: \u001b[1m\u001b[32m0.24084\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1577 | loss: 0.24084 - acc: 0.9787 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1578  | total loss: \u001b[1m\u001b[32m0.23683\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1578 | loss: 0.23683 - acc: 0.9795 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1579  | total loss: \u001b[1m\u001b[32m0.23319\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1579 | loss: 0.23319 - acc: 0.9802 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1580  | total loss: \u001b[1m\u001b[32m0.49853\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1580 | loss: 0.49853 - acc: 0.9151 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1581  | total loss: \u001b[1m\u001b[32m0.46870\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1581 | loss: 0.46870 - acc: 0.9223 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1582  | total loss: \u001b[1m\u001b[32m0.44187\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1582 | loss: 0.44187 - acc: 0.9287 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1583  | total loss: \u001b[1m\u001b[32m0.41772\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1583 | loss: 0.41772 - acc: 0.9345 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1584  | total loss: \u001b[1m\u001b[32m0.39599\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1584 | loss: 0.39599 - acc: 0.9398 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1585  | total loss: \u001b[1m\u001b[32m0.37644\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1585 | loss: 0.37644 - acc: 0.9445 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1586  | total loss: \u001b[1m\u001b[32m0.35883\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1586 | loss: 0.35883 - acc: 0.9487 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1587  | total loss: \u001b[1m\u001b[32m0.34299\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1587 | loss: 0.34299 - acc: 0.9525 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1588  | total loss: \u001b[1m\u001b[32m0.32872\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1588 | loss: 0.32872 - acc: 0.9560 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m0.31586\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1589 | loss: 0.31586 - acc: 0.9590 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m0.30428\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1590 | loss: 0.30428 - acc: 0.9618 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1591  | total loss: \u001b[1m\u001b[32m0.29384\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1591 | loss: 0.29384 - acc: 0.9643 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1592  | total loss: \u001b[1m\u001b[32m0.28443\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1592 | loss: 0.28443 - acc: 0.9666 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1593  | total loss: \u001b[1m\u001b[32m0.27594\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1593 | loss: 0.27594 - acc: 0.9686 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1594  | total loss: \u001b[1m\u001b[32m0.26828\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1594 | loss: 0.26828 - acc: 0.9704 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1595  | total loss: \u001b[1m\u001b[32m0.26137\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1595 | loss: 0.26137 - acc: 0.9721 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1596  | total loss: \u001b[1m\u001b[32m0.25512\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1596 | loss: 0.25512 - acc: 0.9735 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1597  | total loss: \u001b[1m\u001b[32m0.24948\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1597 | loss: 0.24948 - acc: 0.9749 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1598  | total loss: \u001b[1m\u001b[32m0.24438\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1598 | loss: 0.24438 - acc: 0.9761 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m0.23977\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1599 | loss: 0.23977 - acc: 0.9772 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m0.23560\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1600 | loss: 0.23560 - acc: 0.9781 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1601  | total loss: \u001b[1m\u001b[32m0.23182\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1601 | loss: 0.23182 - acc: 0.9790 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1602  | total loss: \u001b[1m\u001b[32m0.22839\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1602 | loss: 0.22839 - acc: 0.9798 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1603  | total loss: \u001b[1m\u001b[32m0.22529\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1603 | loss: 0.22529 - acc: 0.9805 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1604  | total loss: \u001b[1m\u001b[32m0.22247\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1604 | loss: 0.22247 - acc: 0.9811 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1605  | total loss: \u001b[1m\u001b[32m0.21991\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1605 | loss: 0.21991 - acc: 0.9817 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1606  | total loss: \u001b[1m\u001b[32m0.21758\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1606 | loss: 0.21758 - acc: 0.9822 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1607  | total loss: \u001b[1m\u001b[32m0.21546\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1607 | loss: 0.21546 - acc: 0.9827 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1608  | total loss: \u001b[1m\u001b[32m0.21353\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1608 | loss: 0.21353 - acc: 0.9831 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1609  | total loss: \u001b[1m\u001b[32m0.21177\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1609 | loss: 0.21177 - acc: 0.9835 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1610  | total loss: \u001b[1m\u001b[32m0.21015\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1610 | loss: 0.21015 - acc: 0.9838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1611  | total loss: \u001b[1m\u001b[32m0.20868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1611 | loss: 0.20868 - acc: 0.9841 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1612  | total loss: \u001b[1m\u001b[32m0.20732\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1612 | loss: 0.20732 - acc: 0.9844 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1613  | total loss: \u001b[1m\u001b[32m0.20608\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1613 | loss: 0.20608 - acc: 0.9846 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1614  | total loss: \u001b[1m\u001b[32m0.20493\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1614 | loss: 0.20493 - acc: 0.9848 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1615  | total loss: \u001b[1m\u001b[32m0.20387\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1615 | loss: 0.20387 - acc: 0.9850 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1616  | total loss: \u001b[1m\u001b[32m0.49261\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1616 | loss: 0.49261 - acc: 0.9247 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1617  | total loss: \u001b[1m\u001b[32m0.46277\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1617 | loss: 0.46277 - acc: 0.9309 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1618  | total loss: \u001b[1m\u001b[32m0.43592\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1618 | loss: 0.43592 - acc: 0.9365 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1619  | total loss: \u001b[1m\u001b[32m0.41175\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1619 | loss: 0.41175 - acc: 0.9415 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1620  | total loss: \u001b[1m\u001b[32m0.39001\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1620 | loss: 0.39001 - acc: 0.9461 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1621  | total loss: \u001b[1m\u001b[32m0.37044\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1621 | loss: 0.37044 - acc: 0.9501 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1622  | total loss: \u001b[1m\u001b[32m0.35283\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1622 | loss: 0.35283 - acc: 0.9538 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1623  | total loss: \u001b[1m\u001b[32m0.33696\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1623 | loss: 0.33696 - acc: 0.9571 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1624  | total loss: \u001b[1m\u001b[32m0.32267\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1624 | loss: 0.32267 - acc: 0.9601 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1625  | total loss: \u001b[1m\u001b[32m0.30980\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1625 | loss: 0.30980 - acc: 0.9628 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1626  | total loss: \u001b[1m\u001b[32m0.29820\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1626 | loss: 0.29820 - acc: 0.9652 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1627  | total loss: \u001b[1m\u001b[32m0.28774\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1627 | loss: 0.28774 - acc: 0.9673 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1628  | total loss: \u001b[1m\u001b[32m0.27830\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1628 | loss: 0.27830 - acc: 0.9693 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1629  | total loss: \u001b[1m\u001b[32m0.26980\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1629 | loss: 0.26980 - acc: 0.9710 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1630  | total loss: \u001b[1m\u001b[32m0.55828\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1630 | loss: 0.55828 - acc: 0.9160 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1631  | total loss: \u001b[1m\u001b[32m0.52177\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1631 | loss: 0.52177 - acc: 0.9231 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1632  | total loss: \u001b[1m\u001b[32m0.48893\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1632 | loss: 0.48893 - acc: 0.9295 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1633  | total loss: \u001b[1m\u001b[32m0.45938\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1633 | loss: 0.45938 - acc: 0.9352 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1634  | total loss: \u001b[1m\u001b[32m0.43279\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1634 | loss: 0.43279 - acc: 0.9404 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1635  | total loss: \u001b[1m\u001b[32m0.40887\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1635 | loss: 0.40887 - acc: 0.9450 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1636  | total loss: \u001b[1m\u001b[32m0.38733\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1636 | loss: 0.38733 - acc: 0.9492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1637  | total loss: \u001b[1m\u001b[32m0.36795\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1637 | loss: 0.36795 - acc: 0.9530 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1638  | total loss: \u001b[1m\u001b[32m0.35049\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1638 | loss: 0.35049 - acc: 0.9564 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1639  | total loss: \u001b[1m\u001b[32m0.33478\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1639 | loss: 0.33478 - acc: 0.9594 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1640  | total loss: \u001b[1m\u001b[32m0.32062\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1640 | loss: 0.32062 - acc: 0.9622 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1641  | total loss: \u001b[1m\u001b[32m0.30787\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1641 | loss: 0.30787 - acc: 0.9646 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1642  | total loss: \u001b[1m\u001b[32m0.61440\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1642 | loss: 0.61440 - acc: 0.9011 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1643  | total loss: \u001b[1m\u001b[32m0.57229\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1643 | loss: 0.57229 - acc: 0.9096 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1644  | total loss: \u001b[1m\u001b[32m0.53440\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1644 | loss: 0.53440 - acc: 0.9174 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1645  | total loss: \u001b[1m\u001b[32m0.50033\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1645 | loss: 0.50033 - acc: 0.9243 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1646  | total loss: \u001b[1m\u001b[32m0.46968\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1646 | loss: 0.46968 - acc: 0.9306 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1647  | total loss: \u001b[1m\u001b[32m0.44210\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1647 | loss: 0.44210 - acc: 0.9362 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1648  | total loss: \u001b[1m\u001b[32m0.41729\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1648 | loss: 0.41729 - acc: 0.9413 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1649  | total loss: \u001b[1m\u001b[32m0.39495\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1649 | loss: 0.39495 - acc: 0.9458 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1650  | total loss: \u001b[1m\u001b[32m0.65689\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1650 | loss: 0.65689 - acc: 0.8894 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1651  | total loss: \u001b[1m\u001b[32m0.61063\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1651 | loss: 0.61063 - acc: 0.8991 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1652  | total loss: \u001b[1m\u001b[32m0.56902\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1652 | loss: 0.56902 - acc: 0.9079 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1653  | total loss: \u001b[1m\u001b[32m0.53161\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1653 | loss: 0.53161 - acc: 0.9158 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1654  | total loss: \u001b[1m\u001b[32m0.49795\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1654 | loss: 0.49795 - acc: 0.9229 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1655  | total loss: \u001b[1m\u001b[32m0.46766\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1655 | loss: 0.46766 - acc: 0.9293 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1656  | total loss: \u001b[1m\u001b[32m0.44040\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1656 | loss: 0.44040 - acc: 0.9351 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1657  | total loss: \u001b[1m\u001b[32m0.41587\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1657 | loss: 0.41587 - acc: 0.9402 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1658  | total loss: \u001b[1m\u001b[32m0.39379\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1658 | loss: 0.39379 - acc: 0.9449 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1659  | total loss: \u001b[1m\u001b[32m0.37390\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1659 | loss: 0.37390 - acc: 0.9491 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1660  | total loss: \u001b[1m\u001b[32m0.35599\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1660 | loss: 0.35599 - acc: 0.9529 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1661  | total loss: \u001b[1m\u001b[32m0.33986\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1661 | loss: 0.33986 - acc: 0.9563 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1662  | total loss: \u001b[1m\u001b[32m0.32534\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1662 | loss: 0.32534 - acc: 0.9593 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1663  | total loss: \u001b[1m\u001b[32m0.31225\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1663 | loss: 0.31225 - acc: 0.9621 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1664  | total loss: \u001b[1m\u001b[32m0.30046\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1664 | loss: 0.30046 - acc: 0.9645 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1665  | total loss: \u001b[1m\u001b[32m0.28984\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1665 | loss: 0.28984 - acc: 0.9668 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1666  | total loss: \u001b[1m\u001b[32m0.28026\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1666 | loss: 0.28026 - acc: 0.9688 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1667  | total loss: \u001b[1m\u001b[32m0.27163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1667 | loss: 0.27163 - acc: 0.9706 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1668  | total loss: \u001b[1m\u001b[32m0.26385\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1668 | loss: 0.26385 - acc: 0.9722 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1669  | total loss: \u001b[1m\u001b[32m0.25682\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1669 | loss: 0.25682 - acc: 0.9737 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1670  | total loss: \u001b[1m\u001b[32m0.25049\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1670 | loss: 0.25049 - acc: 0.9750 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1671  | total loss: \u001b[1m\u001b[32m0.24476\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1671 | loss: 0.24476 - acc: 0.9762 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1672  | total loss: \u001b[1m\u001b[32m0.23959\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1672 | loss: 0.23959 - acc: 0.9772 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1673  | total loss: \u001b[1m\u001b[32m0.23491\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1673 | loss: 0.23491 - acc: 0.9782 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1674  | total loss: \u001b[1m\u001b[32m0.23068\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1674 | loss: 0.23068 - acc: 0.9791 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1675  | total loss: \u001b[1m\u001b[32m0.22685\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1675 | loss: 0.22685 - acc: 0.9798 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1676  | total loss: \u001b[1m\u001b[32m0.22337\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1676 | loss: 0.22337 - acc: 0.9805 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1677  | total loss: \u001b[1m\u001b[32m0.22023\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1677 | loss: 0.22023 - acc: 0.9812 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1678  | total loss: \u001b[1m\u001b[32m0.57608\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1678 | loss: 0.57608 - acc: 0.9120 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1679  | total loss: \u001b[1m\u001b[32m0.53766\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1679 | loss: 0.53766 - acc: 0.9195 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1680  | total loss: \u001b[1m\u001b[32m0.50310\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1680 | loss: 0.50310 - acc: 0.9262 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1681  | total loss: \u001b[1m\u001b[32m0.47202\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1681 | loss: 0.47202 - acc: 0.9323 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1682  | total loss: \u001b[1m\u001b[32m0.44406\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1682 | loss: 0.44406 - acc: 0.9377 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1683  | total loss: \u001b[1m\u001b[32m0.41892\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1683 | loss: 0.41892 - acc: 0.9427 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1684  | total loss: \u001b[1m\u001b[32m0.73788\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1684 | loss: 0.73788 - acc: 0.8760 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1685  | total loss: \u001b[1m\u001b[32m0.68342\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1685 | loss: 0.68342 - acc: 0.8871 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1686  | total loss: \u001b[1m\u001b[32m0.63446\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1686 | loss: 0.63446 - acc: 0.8971 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1687  | total loss: \u001b[1m\u001b[32m0.59045\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1687 | loss: 0.59045 - acc: 0.9047 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1688  | total loss: \u001b[1m\u001b[32m0.55086\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1688 | loss: 0.55086 - acc: 0.9116 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1689  | total loss: \u001b[1m\u001b[32m0.51525\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1689 | loss: 0.51525 - acc: 0.9178 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1690  | total loss: \u001b[1m\u001b[32m0.48321\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1690 | loss: 0.48321 - acc: 0.9234 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1691  | total loss: \u001b[1m\u001b[32m0.45436\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1691 | loss: 0.45436 - acc: 0.9298 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1692  | total loss: \u001b[1m\u001b[32m0.42839\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1692 | loss: 0.42839 - acc: 0.9355 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1693  | total loss: \u001b[1m\u001b[32m0.40500\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1693 | loss: 0.40500 - acc: 0.9406 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1694  | total loss: \u001b[1m\u001b[32m0.38392\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1694 | loss: 0.38392 - acc: 0.9452 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1695  | total loss: \u001b[1m\u001b[32m0.36494\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1695 | loss: 0.36494 - acc: 0.9494 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1696  | total loss: \u001b[1m\u001b[32m0.34783\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1696 | loss: 0.34783 - acc: 0.9531 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1697  | total loss: \u001b[1m\u001b[32m0.33243\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1697 | loss: 0.33243 - acc: 0.9565 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1698  | total loss: \u001b[1m\u001b[32m0.31855\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1698 | loss: 0.31855 - acc: 0.9595 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1699  | total loss: \u001b[1m\u001b[32m0.30605\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1699 | loss: 0.30605 - acc: 0.9623 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1700  | total loss: \u001b[1m\u001b[32m0.29479\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1700 | loss: 0.29479 - acc: 0.9647 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1701  | total loss: \u001b[1m\u001b[32m0.28465\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1701 | loss: 0.28465 - acc: 0.9669 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1702  | total loss: \u001b[1m\u001b[32m0.27550\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1702 | loss: 0.27550 - acc: 0.9689 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1703  | total loss: \u001b[1m\u001b[32m0.26726\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1703 | loss: 0.26726 - acc: 0.9707 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1704  | total loss: \u001b[1m\u001b[32m0.25983\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1704 | loss: 0.25983 - acc: 0.9723 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1705  | total loss: \u001b[1m\u001b[32m0.25313\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1705 | loss: 0.25313 - acc: 0.9738 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1706  | total loss: \u001b[1m\u001b[32m0.24707\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1706 | loss: 0.24707 - acc: 0.9751 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1707  | total loss: \u001b[1m\u001b[32m0.24159\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1707 | loss: 0.24159 - acc: 0.9763 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1708  | total loss: \u001b[1m\u001b[32m0.23664\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1708 | loss: 0.23664 - acc: 0.9773 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1709  | total loss: \u001b[1m\u001b[32m0.23216\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1709 | loss: 0.23216 - acc: 0.9783 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1710  | total loss: \u001b[1m\u001b[32m0.54609\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1710 | loss: 0.54609 - acc: 0.9186 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1711  | total loss: \u001b[1m\u001b[32m0.51065\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1711 | loss: 0.51065 - acc: 0.9254 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1712  | total loss: \u001b[1m\u001b[32m0.47877\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1712 | loss: 0.47877 - acc: 0.9316 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1713  | total loss: \u001b[1m\u001b[32m0.45009\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1713 | loss: 0.45009 - acc: 0.9371 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1714  | total loss: \u001b[1m\u001b[32m0.42429\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1714 | loss: 0.42429 - acc: 0.9421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1715  | total loss: \u001b[1m\u001b[32m0.40109\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1715 | loss: 0.40109 - acc: 0.9465 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1716  | total loss: \u001b[1m\u001b[32m0.72777\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1716 | loss: 0.72777 - acc: 0.8874 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1717  | total loss: \u001b[1m\u001b[32m0.67429\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1717 | loss: 0.67429 - acc: 0.8974 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1718  | total loss: \u001b[1m\u001b[32m0.62621\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1718 | loss: 0.62621 - acc: 0.9063 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1719  | total loss: \u001b[1m\u001b[32m0.58298\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1719 | loss: 0.58298 - acc: 0.9130 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1720  | total loss: \u001b[1m\u001b[32m0.54410\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1720 | loss: 0.54410 - acc: 0.9191 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1721  | total loss: \u001b[1m\u001b[32m0.50913\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1721 | loss: 0.50913 - acc: 0.9246 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1722  | total loss: \u001b[1m\u001b[32m0.47766\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1722 | loss: 0.47766 - acc: 0.9295 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1723  | total loss: \u001b[1m\u001b[32m0.44933\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1723 | loss: 0.44933 - acc: 0.9352 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1724  | total loss: \u001b[1m\u001b[32m0.42383\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1724 | loss: 0.42383 - acc: 0.9404 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1725  | total loss: \u001b[1m\u001b[32m0.40086\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1725 | loss: 0.40086 - acc: 0.9450 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1726  | total loss: \u001b[1m\u001b[32m0.38017\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1726 | loss: 0.38017 - acc: 0.9492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1727  | total loss: \u001b[1m\u001b[32m0.36153\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1727 | loss: 0.36153 - acc: 0.9530 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1728  | total loss: \u001b[1m\u001b[32m0.34474\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1728 | loss: 0.34474 - acc: 0.9564 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1729  | total loss: \u001b[1m\u001b[32m0.32962\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1729 | loss: 0.32962 - acc: 0.9594 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1730  | total loss: \u001b[1m\u001b[32m0.31599\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1730 | loss: 0.31599 - acc: 0.9621 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1731  | total loss: \u001b[1m\u001b[32m0.30372\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1731 | loss: 0.30372 - acc: 0.9646 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1732  | total loss: \u001b[1m\u001b[32m0.29266\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1732 | loss: 0.29266 - acc: 0.9668 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1733  | total loss: \u001b[1m\u001b[32m0.28270\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1733 | loss: 0.28270 - acc: 0.9688 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1734  | total loss: \u001b[1m\u001b[32m0.27372\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1734 | loss: 0.27372 - acc: 0.9706 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1735  | total loss: \u001b[1m\u001b[32m0.26562\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1735 | loss: 0.26562 - acc: 0.9723 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1736  | total loss: \u001b[1m\u001b[32m0.25832\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1736 | loss: 0.25832 - acc: 0.9737 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1737  | total loss: \u001b[1m\u001b[32m0.25173\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1737 | loss: 0.25173 - acc: 0.9750 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1738  | total loss: \u001b[1m\u001b[32m0.24578\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1738 | loss: 0.24578 - acc: 0.9762 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1739  | total loss: \u001b[1m\u001b[32m0.24040\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1739 | loss: 0.24040 - acc: 0.9773 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1740  | total loss: \u001b[1m\u001b[32m0.23553\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1740 | loss: 0.23553 - acc: 0.9782 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1741  | total loss: \u001b[1m\u001b[32m0.23112\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1741 | loss: 0.23112 - acc: 0.9791 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1742  | total loss: \u001b[1m\u001b[32m0.22713\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1742 | loss: 0.22713 - acc: 0.9799 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1743  | total loss: \u001b[1m\u001b[32m0.22352\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1743 | loss: 0.22352 - acc: 0.9806 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1744  | total loss: \u001b[1m\u001b[32m0.22024\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1744 | loss: 0.22024 - acc: 0.9812 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1745  | total loss: \u001b[1m\u001b[32m0.21726\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1745 | loss: 0.21726 - acc: 0.9818 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1746  | total loss: \u001b[1m\u001b[32m0.21456\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 1746 | loss: 0.21456 - acc: 0.9823 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1747  | total loss: \u001b[1m\u001b[32m0.21210\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1747 | loss: 0.21210 - acc: 0.9827 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1748  | total loss: \u001b[1m\u001b[32m0.20986\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1748 | loss: 0.20986 - acc: 0.9831 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1749  | total loss: \u001b[1m\u001b[32m0.20783\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1749 | loss: 0.20783 - acc: 0.9835 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1750  | total loss: \u001b[1m\u001b[32m0.20597\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1750 | loss: 0.20597 - acc: 0.9838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1751  | total loss: \u001b[1m\u001b[32m0.20428\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1751 | loss: 0.20428 - acc: 0.9841 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1752  | total loss: \u001b[1m\u001b[32m0.20273\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1752 | loss: 0.20273 - acc: 0.9844 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1753  | total loss: \u001b[1m\u001b[32m0.20131\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1753 | loss: 0.20131 - acc: 0.9847 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1754  | total loss: \u001b[1m\u001b[32m0.20001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1754 | loss: 0.20001 - acc: 0.9849 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1755  | total loss: \u001b[1m\u001b[32m0.19881\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1755 | loss: 0.19881 - acc: 0.9851 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1756  | total loss: \u001b[1m\u001b[32m0.19771\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1756 | loss: 0.19771 - acc: 0.9852 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1757  | total loss: \u001b[1m\u001b[32m0.19669\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1757 | loss: 0.19669 - acc: 0.9854 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1758  | total loss: \u001b[1m\u001b[32m0.19576\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1758 | loss: 0.19576 - acc: 0.9855 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1759  | total loss: \u001b[1m\u001b[32m0.19488\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1759 | loss: 0.19488 - acc: 0.9857 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.55851\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1760 | loss: 0.55851 - acc: 0.9187 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1761  | total loss: \u001b[1m\u001b[32m0.52136\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1761 | loss: 0.52136 - acc: 0.9255 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1762  | total loss: \u001b[1m\u001b[32m0.48794\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1762 | loss: 0.48794 - acc: 0.9316 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1763  | total loss: \u001b[1m\u001b[32m0.45789\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1763 | loss: 0.45789 - acc: 0.9372 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1764  | total loss: \u001b[1m\u001b[32m0.43085\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1764 | loss: 0.43085 - acc: 0.9421 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1765  | total loss: \u001b[1m\u001b[32m0.40652\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1765 | loss: 0.40652 - acc: 0.9466 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1766  | total loss: \u001b[1m\u001b[32m0.74691\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1766 | loss: 0.74691 - acc: 0.8875 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1767  | total loss: \u001b[1m\u001b[32m0.69104\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1767 | loss: 0.69104 - acc: 0.8974 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1768  | total loss: \u001b[1m\u001b[32m0.64080\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1768 | loss: 0.64080 - acc: 0.9063 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1769  | total loss: \u001b[1m\u001b[32m0.59562\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1769 | loss: 0.59562 - acc: 0.9144 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1770  | total loss: \u001b[1m\u001b[32m0.55499\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1770 | loss: 0.55499 - acc: 0.9203 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1771  | total loss: \u001b[1m\u001b[32m0.51844\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1771 | loss: 0.51844 - acc: 0.9257 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1772  | total loss: \u001b[1m\u001b[32m0.48555\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1772 | loss: 0.48555 - acc: 0.9318 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1773  | total loss: \u001b[1m\u001b[32m0.45595\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1773 | loss: 0.45595 - acc: 0.9373 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1774  | total loss: \u001b[1m\u001b[32m0.42929\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1774 | loss: 0.42929 - acc: 0.9422 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1775  | total loss: \u001b[1m\u001b[32m0.40529\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1775 | loss: 0.40529 - acc: 0.9467 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1776  | total loss: \u001b[1m\u001b[32m0.38368\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1776 | loss: 0.38368 - acc: 0.9507 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1777  | total loss: \u001b[1m\u001b[32m0.36421\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1777 | loss: 0.36421 - acc: 0.9543 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1778  | total loss: \u001b[1m\u001b[32m0.34667\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1778 | loss: 0.34667 - acc: 0.9576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1779  | total loss: \u001b[1m\u001b[32m0.33088\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1779 | loss: 0.33088 - acc: 0.9605 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1780  | total loss: \u001b[1m\u001b[32m0.31665\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1780 | loss: 0.31665 - acc: 0.9631 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1781  | total loss: \u001b[1m\u001b[32m0.30384\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1781 | loss: 0.30384 - acc: 0.9655 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1782  | total loss: \u001b[1m\u001b[32m0.29230\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1782 | loss: 0.29230 - acc: 0.9676 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1783  | total loss: \u001b[1m\u001b[32m0.28190\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1783 | loss: 0.28190 - acc: 0.9696 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1784  | total loss: \u001b[1m\u001b[32m0.27253\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1784 | loss: 0.27253 - acc: 0.9713 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1785  | total loss: \u001b[1m\u001b[32m0.26408\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1785 | loss: 0.26408 - acc: 0.9728 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1786  | total loss: \u001b[1m\u001b[32m0.25647\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1786 | loss: 0.25647 - acc: 0.9742 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1787  | total loss: \u001b[1m\u001b[32m0.24959\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1787 | loss: 0.24959 - acc: 0.9755 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1788  | total loss: \u001b[1m\u001b[32m0.24339\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1788 | loss: 0.24339 - acc: 0.9766 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1789  | total loss: \u001b[1m\u001b[32m0.23778\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1789 | loss: 0.23778 - acc: 0.9777 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1790  | total loss: \u001b[1m\u001b[32m0.23272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1790 | loss: 0.23272 - acc: 0.9786 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1791  | total loss: \u001b[1m\u001b[32m0.22814\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1791 | loss: 0.22814 - acc: 0.9794 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1792  | total loss: \u001b[1m\u001b[32m0.22399\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1792 | loss: 0.22399 - acc: 0.9801 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1793  | total loss: \u001b[1m\u001b[32m0.22023\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1793 | loss: 0.22023 - acc: 0.9808 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1794  | total loss: \u001b[1m\u001b[32m0.21683\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1794 | loss: 0.21683 - acc: 0.9814 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1795  | total loss: \u001b[1m\u001b[32m0.21375\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1795 | loss: 0.21375 - acc: 0.9820 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1796  | total loss: \u001b[1m\u001b[32m0.21095\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1796 | loss: 0.21095 - acc: 0.9824 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1797  | total loss: \u001b[1m\u001b[32m0.20841\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1797 | loss: 0.20841 - acc: 0.9829 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1798  | total loss: \u001b[1m\u001b[32m0.52166\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1798 | loss: 0.52166 - acc: 0.9149 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1799  | total loss: \u001b[1m\u001b[32m0.48804\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1799 | loss: 0.48804 - acc: 0.9221 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1800  | total loss: \u001b[1m\u001b[32m0.45781\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1800 | loss: 0.45781 - acc: 0.9285 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1801  | total loss: \u001b[1m\u001b[32m0.43062\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1801 | loss: 0.43062 - acc: 0.9344 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1802  | total loss: \u001b[1m\u001b[32m0.40616\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1802 | loss: 0.40616 - acc: 0.9396 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1803  | total loss: \u001b[1m\u001b[32m0.38415\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1803 | loss: 0.38415 - acc: 0.9443 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1804  | total loss: \u001b[1m\u001b[32m0.36435\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1804 | loss: 0.36435 - acc: 0.9486 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1805  | total loss: \u001b[1m\u001b[32m0.34652\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1805 | loss: 0.34652 - acc: 0.9524 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1806  | total loss: \u001b[1m\u001b[32m0.33047\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1806 | loss: 0.33047 - acc: 0.9559 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1807  | total loss: \u001b[1m\u001b[32m0.31601\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1807 | loss: 0.31601 - acc: 0.9590 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1808  | total loss: \u001b[1m\u001b[32m0.30298\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1808 | loss: 0.30298 - acc: 0.9617 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1809  | total loss: \u001b[1m\u001b[32m0.29123\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1809 | loss: 0.29123 - acc: 0.9643 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1810  | total loss: \u001b[1m\u001b[32m0.28064\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1810 | loss: 0.28064 - acc: 0.9665 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1811  | total loss: \u001b[1m\u001b[32m0.27109\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1811 | loss: 0.27109 - acc: 0.9685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1812  | total loss: \u001b[1m\u001b[32m0.26248\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1812 | loss: 0.26248 - acc: 0.9704 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1813  | total loss: \u001b[1m\u001b[32m0.25471\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1813 | loss: 0.25471 - acc: 0.9720 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1814  | total loss: \u001b[1m\u001b[32m0.24770\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1814 | loss: 0.24770 - acc: 0.9735 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1815  | total loss: \u001b[1m\u001b[32m0.24137\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1815 | loss: 0.24137 - acc: 0.9748 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1816  | total loss: \u001b[1m\u001b[32m0.23567\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1816 | loss: 0.23567 - acc: 0.9760 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1817  | total loss: \u001b[1m\u001b[32m0.23051\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1817 | loss: 0.23051 - acc: 0.9771 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1818  | total loss: \u001b[1m\u001b[32m0.22586\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1818 | loss: 0.22586 - acc: 0.9781 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1819  | total loss: \u001b[1m\u001b[32m0.22165\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1819 | loss: 0.22165 - acc: 0.9790 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1820  | total loss: \u001b[1m\u001b[32m0.50350\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1820 | loss: 0.50350 - acc: 0.9205 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1821  | total loss: \u001b[1m\u001b[32m0.47151\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1821 | loss: 0.47151 - acc: 0.9272 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1822  | total loss: \u001b[1m\u001b[32m0.44273\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1822 | loss: 0.44273 - acc: 0.9331 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1823  | total loss: \u001b[1m\u001b[32m0.41684\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1823 | loss: 0.41684 - acc: 0.9385 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1824  | total loss: \u001b[1m\u001b[32m0.39353\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1824 | loss: 0.39353 - acc: 0.9433 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1825  | total loss: \u001b[1m\u001b[32m0.37257\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1825 | loss: 0.37257 - acc: 0.9477 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1826  | total loss: \u001b[1m\u001b[32m0.35370\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1826 | loss: 0.35370 - acc: 0.9516 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1827  | total loss: \u001b[1m\u001b[32m0.33671\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1827 | loss: 0.33671 - acc: 0.9551 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1828  | total loss: \u001b[1m\u001b[32m0.60718\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1828 | loss: 0.60718 - acc: 0.8991 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1829  | total loss: \u001b[1m\u001b[32m0.56488\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1829 | loss: 0.56488 - acc: 0.9079 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1830  | total loss: \u001b[1m\u001b[32m0.52684\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1830 | loss: 0.52684 - acc: 0.9158 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1831  | total loss: \u001b[1m\u001b[32m0.49263\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1831 | loss: 0.49263 - acc: 0.9229 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1832  | total loss: \u001b[1m\u001b[32m0.77239\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1832 | loss: 0.77239 - acc: 0.8622 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1833  | total loss: \u001b[1m\u001b[32m0.71370\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1833 | loss: 0.71370 - acc: 0.8733 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1834  | total loss: \u001b[1m\u001b[32m0.92434\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1834 | loss: 0.92434 - acc: 0.8294 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1835  | total loss: \u001b[1m\u001b[32m0.85060\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1835 | loss: 0.85060 - acc: 0.8438 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1836  | total loss: \u001b[1m\u001b[32m0.78430\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1836 | loss: 0.78430 - acc: 0.8568 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1837  | total loss: \u001b[1m\u001b[32m0.72468\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1837 | loss: 0.72468 - acc: 0.8685 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1838  | total loss: \u001b[1m\u001b[32m0.67105\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1838 | loss: 0.67105 - acc: 0.8790 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1839  | total loss: \u001b[1m\u001b[32m0.62280\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1839 | loss: 0.62280 - acc: 0.8885 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1840  | total loss: \u001b[1m\u001b[32m0.57938\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1840 | loss: 0.57938 - acc: 0.8970 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1841  | total loss: \u001b[1m\u001b[32m0.54029\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1841 | loss: 0.54029 - acc: 0.9047 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1842  | total loss: \u001b[1m\u001b[32m0.50509\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1842 | loss: 0.50509 - acc: 0.9129 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1843  | total loss: \u001b[1m\u001b[32m0.47340\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1843 | loss: 0.47340 - acc: 0.9203 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1844  | total loss: \u001b[1m\u001b[32m0.44486\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1844 | loss: 0.44486 - acc: 0.9269 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1845  | total loss: \u001b[1m\u001b[32m0.41917\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1845 | loss: 0.41917 - acc: 0.9329 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1846  | total loss: \u001b[1m\u001b[32m0.39604\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1846 | loss: 0.39604 - acc: 0.9383 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1847  | total loss: \u001b[1m\u001b[32m0.37521\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1847 | loss: 0.37521 - acc: 0.9432 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1848  | total loss: \u001b[1m\u001b[32m0.35647\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1848 | loss: 0.35647 - acc: 0.9475 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1849  | total loss: \u001b[1m\u001b[32m0.33960\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1849 | loss: 0.33960 - acc: 0.9515 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1850  | total loss: \u001b[1m\u001b[32m0.32442\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1850 | loss: 0.32442 - acc: 0.9550 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1851  | total loss: \u001b[1m\u001b[32m0.31075\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1851 | loss: 0.31075 - acc: 0.9582 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1852  | total loss: \u001b[1m\u001b[32m0.29844\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1852 | loss: 0.29844 - acc: 0.9611 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1853  | total loss: \u001b[1m\u001b[32m0.28734\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1853 | loss: 0.28734 - acc: 0.9636 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1854  | total loss: \u001b[1m\u001b[32m0.58656\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1854 | loss: 0.58656 - acc: 0.8989 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1855  | total loss: \u001b[1m\u001b[32m0.54665\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1855 | loss: 0.54665 - acc: 0.9077 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1856  | total loss: \u001b[1m\u001b[32m0.51073\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1856 | loss: 0.51073 - acc: 0.9156 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1857  | total loss: \u001b[1m\u001b[32m0.47841\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1857 | loss: 0.47841 - acc: 0.9227 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1858  | total loss: \u001b[1m\u001b[32m0.44932\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1858 | loss: 0.44932 - acc: 0.9291 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1859  | total loss: \u001b[1m\u001b[32m0.42315\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1859 | loss: 0.42315 - acc: 0.9349 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1860  | total loss: \u001b[1m\u001b[32m0.39960\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1860 | loss: 0.39960 - acc: 0.9401 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1861  | total loss: \u001b[1m\u001b[32m0.37841\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1861 | loss: 0.37841 - acc: 0.9448 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1862  | total loss: \u001b[1m\u001b[32m0.35934\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1862 | loss: 0.35934 - acc: 0.9490 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1863  | total loss: \u001b[1m\u001b[32m0.34217\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1863 | loss: 0.34217 - acc: 0.9528 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1864  | total loss: \u001b[1m\u001b[32m0.32671\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1864 | loss: 0.32671 - acc: 0.9562 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1865  | total loss: \u001b[1m\u001b[32m0.31279\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1865 | loss: 0.31279 - acc: 0.9592 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1866  | total loss: \u001b[1m\u001b[32m0.66834\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1866 | loss: 0.66834 - acc: 0.8909 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1867  | total loss: \u001b[1m\u001b[32m0.62028\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1867 | loss: 0.62028 - acc: 0.9005 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1868  | total loss: \u001b[1m\u001b[32m0.87717\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1868 | loss: 0.87717 - acc: 0.8434 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1869  | total loss: \u001b[1m\u001b[32m0.80834\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1869 | loss: 0.80834 - acc: 0.8564 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1870  | total loss: \u001b[1m\u001b[32m0.74646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1870 | loss: 0.74646 - acc: 0.8681 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1871  | total loss: \u001b[1m\u001b[32m0.69083\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1871 | loss: 0.69083 - acc: 0.8787 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1872  | total loss: \u001b[1m\u001b[32m0.64080\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1872 | loss: 0.64080 - acc: 0.8882 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1873  | total loss: \u001b[1m\u001b[32m0.59580\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1873 | loss: 0.59580 - acc: 0.8967 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1874  | total loss: \u001b[1m\u001b[32m0.83975\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1874 | loss: 0.83975 - acc: 0.8439 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1875  | total loss: \u001b[1m\u001b[32m0.77492\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1875 | loss: 0.77492 - acc: 0.8569 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1876  | total loss: \u001b[1m\u001b[32m0.95600\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1876 | loss: 0.95600 - acc: 0.8094 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1877  | total loss: \u001b[1m\u001b[32m0.87966\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1877 | loss: 0.87966 - acc: 0.8258 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1878  | total loss: \u001b[1m\u001b[32m0.81102\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1878 | loss: 0.81102 - acc: 0.8406 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1879  | total loss: \u001b[1m\u001b[32m0.74928\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1879 | loss: 0.74928 - acc: 0.8539 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1880  | total loss: \u001b[1m\u001b[32m0.69374\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1880 | loss: 0.69374 - acc: 0.8659 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1881  | total loss: \u001b[1m\u001b[32m0.64377\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1881 | loss: 0.64377 - acc: 0.8766 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1882  | total loss: \u001b[1m\u001b[32m0.92151\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1882 | loss: 0.92151 - acc: 0.8258 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1883  | total loss: \u001b[1m\u001b[32m0.84884\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1883 | loss: 0.84884 - acc: 0.8419 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1884  | total loss: \u001b[1m\u001b[32m0.78349\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1884 | loss: 0.78349 - acc: 0.8564 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1885  | total loss: \u001b[1m\u001b[32m0.72471\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1885 | loss: 0.72471 - acc: 0.8695 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1886  | total loss: \u001b[1m\u001b[32m0.67184\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1886 | loss: 0.67184 - acc: 0.8812 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1887  | total loss: \u001b[1m\u001b[32m0.62428\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1887 | loss: 0.62428 - acc: 0.8918 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1888  | total loss: \u001b[1m\u001b[32m0.58150\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1888 | loss: 0.58150 - acc: 0.9013 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1889  | total loss: \u001b[1m\u001b[32m0.54300\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1889 | loss: 0.54300 - acc: 0.9098 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1890  | total loss: \u001b[1m\u001b[32m0.50837\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1890 | loss: 0.50837 - acc: 0.9175 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1891  | total loss: \u001b[1m\u001b[32m0.47720\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1891 | loss: 0.47720 - acc: 0.9245 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1892  | total loss: \u001b[1m\u001b[32m0.44915\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1892 | loss: 0.44915 - acc: 0.9307 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1893  | total loss: \u001b[1m\u001b[32m0.42391\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1893 | loss: 0.42391 - acc: 0.9363 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1894  | total loss: \u001b[1m\u001b[32m0.40119\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1894 | loss: 0.40119 - acc: 0.9414 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1895  | total loss: \u001b[1m\u001b[32m0.38074\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1895 | loss: 0.38074 - acc: 0.9459 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1896  | total loss: \u001b[1m\u001b[32m0.36233\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1896 | loss: 0.36233 - acc: 0.9500 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1897  | total loss: \u001b[1m\u001b[32m0.34576\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1897 | loss: 0.34576 - acc: 0.9537 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1898  | total loss: \u001b[1m\u001b[32m0.33083\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1898 | loss: 0.33083 - acc: 0.9570 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1899  | total loss: \u001b[1m\u001b[32m0.31737\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1899 | loss: 0.31737 - acc: 0.9600 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1900  | total loss: \u001b[1m\u001b[32m0.30525\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1900 | loss: 0.30525 - acc: 0.9627 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1901  | total loss: \u001b[1m\u001b[32m0.29432\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1901 | loss: 0.29432 - acc: 0.9651 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1902  | total loss: \u001b[1m\u001b[32m0.56228\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1902 | loss: 0.56228 - acc: 0.9002 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1903  | total loss: \u001b[1m\u001b[32m0.52564\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1903 | loss: 0.52564 - acc: 0.9088 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1904  | total loss: \u001b[1m\u001b[32m0.49267\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1904 | loss: 0.49267 - acc: 0.9166 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1905  | total loss: \u001b[1m\u001b[32m0.46300\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1905 | loss: 0.46300 - acc: 0.9237 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1906  | total loss: \u001b[1m\u001b[32m0.43631\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1906 | loss: 0.43631 - acc: 0.9300 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1907  | total loss: \u001b[1m\u001b[32m0.41229\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1907 | loss: 0.41229 - acc: 0.9357 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1908  | total loss: \u001b[1m\u001b[32m0.39067\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1908 | loss: 0.39067 - acc: 0.9408 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1909  | total loss: \u001b[1m\u001b[32m0.37121\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1909 | loss: 0.37121 - acc: 0.9454 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1910  | total loss: \u001b[1m\u001b[32m0.35368\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1910 | loss: 0.35368 - acc: 0.9495 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1911  | total loss: \u001b[1m\u001b[32m0.33790\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1911 | loss: 0.33790 - acc: 0.9533 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1912  | total loss: \u001b[1m\u001b[32m0.32369\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1912 | loss: 0.32369 - acc: 0.9566 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1913  | total loss: \u001b[1m\u001b[32m0.31088\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1913 | loss: 0.31088 - acc: 0.9596 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1914  | total loss: \u001b[1m\u001b[32m0.29933\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1914 | loss: 0.29933 - acc: 0.9624 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1915  | total loss: \u001b[1m\u001b[32m0.28891\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1915 | loss: 0.28891 - acc: 0.9648 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1916  | total loss: \u001b[1m\u001b[32m0.27952\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1916 | loss: 0.27952 - acc: 0.9670 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1917  | total loss: \u001b[1m\u001b[32m0.27104\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1917 | loss: 0.27104 - acc: 0.9690 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1918  | total loss: \u001b[1m\u001b[32m0.26338\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1918 | loss: 0.26338 - acc: 0.9708 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1919  | total loss: \u001b[1m\u001b[32m0.25646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1919 | loss: 0.25646 - acc: 0.9724 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1920  | total loss: \u001b[1m\u001b[32m0.58702\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1920 | loss: 0.58702 - acc: 0.9080 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1921  | total loss: \u001b[1m\u001b[32m0.54774\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1921 | loss: 0.54774 - acc: 0.9159 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1922  | total loss: \u001b[1m\u001b[32m0.51241\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1922 | loss: 0.51241 - acc: 0.9230 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1923  | total loss: \u001b[1m\u001b[32m0.48063\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1923 | loss: 0.48063 - acc: 0.9294 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1924  | total loss: \u001b[1m\u001b[32m0.45204\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1924 | loss: 0.45204 - acc: 0.9351 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1925  | total loss: \u001b[1m\u001b[32m0.42631\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1925 | loss: 0.42631 - acc: 0.9403 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1926  | total loss: \u001b[1m\u001b[32m0.40316\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1926 | loss: 0.40316 - acc: 0.9450 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1927  | total loss: \u001b[1m\u001b[32m0.38232\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1927 | loss: 0.38232 - acc: 0.9492 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1928  | total loss: \u001b[1m\u001b[32m0.36356\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1928 | loss: 0.36356 - acc: 0.9529 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1929  | total loss: \u001b[1m\u001b[32m0.34666\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1929 | loss: 0.34666 - acc: 0.9563 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1930  | total loss: \u001b[1m\u001b[32m0.33144\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1930 | loss: 0.33144 - acc: 0.9594 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1931  | total loss: \u001b[1m\u001b[32m0.31773\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1931 | loss: 0.31773 - acc: 0.9621 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1932  | total loss: \u001b[1m\u001b[32m0.30537\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1932 | loss: 0.30537 - acc: 0.9646 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1933  | total loss: \u001b[1m\u001b[32m0.29423\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1933 | loss: 0.29423 - acc: 0.9668 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1934  | total loss: \u001b[1m\u001b[32m0.28418\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1934 | loss: 0.28418 - acc: 0.9688 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1935  | total loss: \u001b[1m\u001b[32m0.27512\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1935 | loss: 0.27512 - acc: 0.9706 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1936  | total loss: \u001b[1m\u001b[32m0.26695\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1936 | loss: 0.26695 - acc: 0.9722 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1937  | total loss: \u001b[1m\u001b[32m0.25957\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1937 | loss: 0.25957 - acc: 0.9737 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1938  | total loss: \u001b[1m\u001b[32m0.25290\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1938 | loss: 0.25290 - acc: 0.9750 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1939  | total loss: \u001b[1m\u001b[32m0.24689\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1939 | loss: 0.24689 - acc: 0.9762 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1940  | total loss: \u001b[1m\u001b[32m0.24145\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1940 | loss: 0.24145 - acc: 0.9773 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1941  | total loss: \u001b[1m\u001b[32m0.23653\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1941 | loss: 0.23653 - acc: 0.9782 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1942  | total loss: \u001b[1m\u001b[32m0.23207\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1942 | loss: 0.23207 - acc: 0.9791 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1943  | total loss: \u001b[1m\u001b[32m0.22804\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1943 | loss: 0.22804 - acc: 0.9799 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1944  | total loss: \u001b[1m\u001b[32m0.22439\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1944 | loss: 0.22439 - acc: 0.9806 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1945  | total loss: \u001b[1m\u001b[32m0.22107\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1945 | loss: 0.22107 - acc: 0.9812 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1946  | total loss: \u001b[1m\u001b[32m0.21806\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1946 | loss: 0.21806 - acc: 0.9818 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1947  | total loss: \u001b[1m\u001b[32m0.21533\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1947 | loss: 0.21533 - acc: 0.9823 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1948  | total loss: \u001b[1m\u001b[32m0.21284\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1948 | loss: 0.21284 - acc: 0.9827 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1949  | total loss: \u001b[1m\u001b[32m0.21057\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1949 | loss: 0.21057 - acc: 0.9831 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1950  | total loss: \u001b[1m\u001b[32m0.20850\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1950 | loss: 0.20850 - acc: 0.9835 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1951  | total loss: \u001b[1m\u001b[32m0.20662\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1951 | loss: 0.20662 - acc: 0.9838 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1952  | total loss: \u001b[1m\u001b[32m0.20489\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1952 | loss: 0.20489 - acc: 0.9841 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1953  | total loss: \u001b[1m\u001b[32m0.20331\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1953 | loss: 0.20331 - acc: 0.9844 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1954  | total loss: \u001b[1m\u001b[32m0.20186\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1954 | loss: 0.20186 - acc: 0.9847 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1955  | total loss: \u001b[1m\u001b[32m0.20053\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1955 | loss: 0.20053 - acc: 0.9849 -- iter: 76/76\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1956  | total loss: \u001b[1m\u001b[32m0.19931\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1956 | loss: 0.19931 - acc: 0.9851 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1957  | total loss: \u001b[1m\u001b[32m0.19818\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1957 | loss: 0.19818 - acc: 0.9852 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1958  | total loss: \u001b[1m\u001b[32m0.19714\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1958 | loss: 0.19714 - acc: 0.9854 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1959  | total loss: \u001b[1m\u001b[32m0.19617\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1959 | loss: 0.19617 - acc: 0.9855 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1960  | total loss: \u001b[1m\u001b[32m0.19528\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1960 | loss: 0.19528 - acc: 0.9857 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1961  | total loss: \u001b[1m\u001b[32m0.19445\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1961 | loss: 0.19445 - acc: 0.9858 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1962  | total loss: \u001b[1m\u001b[32m0.19368\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1962 | loss: 0.19368 - acc: 0.9859 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1963  | total loss: \u001b[1m\u001b[32m0.19295\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1963 | loss: 0.19295 - acc: 0.9860 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1964  | total loss: \u001b[1m\u001b[32m0.19228\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1964 | loss: 0.19228 - acc: 0.9861 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1965  | total loss: \u001b[1m\u001b[32m0.19165\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1965 | loss: 0.19165 - acc: 0.9862 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1966  | total loss: \u001b[1m\u001b[32m0.19105\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1966 | loss: 0.19105 - acc: 0.9862 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1967  | total loss: \u001b[1m\u001b[32m0.19049\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1967 | loss: 0.19049 - acc: 0.9863 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1968  | total loss: \u001b[1m\u001b[32m0.18996\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1968 | loss: 0.18996 - acc: 0.9863 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1969  | total loss: \u001b[1m\u001b[32m0.18946\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1969 | loss: 0.18946 - acc: 0.9864 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1970  | total loss: \u001b[1m\u001b[32m0.18898\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1970 | loss: 0.18898 - acc: 0.9864 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1971  | total loss: \u001b[1m\u001b[32m0.18853\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1971 | loss: 0.18853 - acc: 0.9865 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1972  | total loss: \u001b[1m\u001b[32m0.18810\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1972 | loss: 0.18810 - acc: 0.9865 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1973  | total loss: \u001b[1m\u001b[32m0.18769\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1973 | loss: 0.18769 - acc: 0.9865 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1974  | total loss: \u001b[1m\u001b[32m0.18729\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1974 | loss: 0.18729 - acc: 0.9866 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1975  | total loss: \u001b[1m\u001b[32m0.18691\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1975 | loss: 0.18691 - acc: 0.9866 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1976  | total loss: \u001b[1m\u001b[32m0.51232\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1976 | loss: 0.51232 - acc: 0.9222 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1977  | total loss: \u001b[1m\u001b[32m0.47943\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1977 | loss: 0.47943 - acc: 0.9286 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1978  | total loss: \u001b[1m\u001b[32m0.44984\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 1978 | loss: 0.44984 - acc: 0.9344 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1979  | total loss: \u001b[1m\u001b[32m0.42322\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1979 | loss: 0.42322 - acc: 0.9397 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.71003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1980 | loss: 0.71003 - acc: 0.8878 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1981  | total loss: \u001b[1m\u001b[32m0.65746\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1981 | loss: 0.65746 - acc: 0.8977 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1982  | total loss: \u001b[1m\u001b[32m0.61019\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1982 | loss: 0.61019 - acc: 0.9053 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1983  | total loss: \u001b[1m\u001b[32m0.56769\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1983 | loss: 0.56769 - acc: 0.9122 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1984  | total loss: \u001b[1m\u001b[32m0.52947\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1984 | loss: 0.52947 - acc: 0.9183 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1985  | total loss: \u001b[1m\u001b[32m0.49508\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1985 | loss: 0.49508 - acc: 0.9238 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1986  | total loss: \u001b[1m\u001b[32m0.46415\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1986 | loss: 0.46415 - acc: 0.9288 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1987  | total loss: \u001b[1m\u001b[32m0.43631\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1987 | loss: 0.43631 - acc: 0.9333 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1988  | total loss: \u001b[1m\u001b[32m0.41125\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1988 | loss: 0.41125 - acc: 0.9374 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1989  | total loss: \u001b[1m\u001b[32m0.38868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1989 | loss: 0.38868 - acc: 0.9423 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1990  | total loss: \u001b[1m\u001b[32m0.36836\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1990 | loss: 0.36836 - acc: 0.9468 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1991  | total loss: \u001b[1m\u001b[32m0.35005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1991 | loss: 0.35005 - acc: 0.9508 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1992  | total loss: \u001b[1m\u001b[32m0.33356\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1992 | loss: 0.33356 - acc: 0.9544 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1993  | total loss: \u001b[1m\u001b[32m0.31871\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1993 | loss: 0.31871 - acc: 0.9576 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1994  | total loss: \u001b[1m\u001b[32m0.30532\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 1994 | loss: 0.30532 - acc: 0.9605 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1995  | total loss: \u001b[1m\u001b[32m0.29327\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1995 | loss: 0.29327 - acc: 0.9632 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1996  | total loss: \u001b[1m\u001b[32m0.28241\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1996 | loss: 0.28241 - acc: 0.9655 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1997  | total loss: \u001b[1m\u001b[32m0.27263\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1997 | loss: 0.27263 - acc: 0.9677 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1998  | total loss: \u001b[1m\u001b[32m0.26381\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1998 | loss: 0.26381 - acc: 0.9696 -- iter: 76/76\n",
      "--\n",
      "Training Step: 1999  | total loss: \u001b[1m\u001b[32m0.25586\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 1999 | loss: 0.25586 - acc: 0.9713 -- iter: 76/76\n",
      "--\n",
      "Training Step: 2000  | total loss: \u001b[1m\u001b[32m0.24870\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 2000 | loss: 0.24870 - acc: 0.9729 -- iter: 76/76\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_data, train_labels, n_epoch=2000, batch_size=150, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training test score [0.97297298908233643]\n"
     ]
    }
   ],
   "source": [
    "#pred = model.predict(test)\n",
    "#pred = pd.DataFrame(pred)\n",
    "score = model.evaluate(test_data, test_labels)\n",
    "print('Training test score', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred\n",
    "#result = []\n",
    "#for i in range(len(pred)):\n",
    "   # if 0[i] > 1[i] and 0[i] > 2[i]:\n",
    "   #     result.append('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
