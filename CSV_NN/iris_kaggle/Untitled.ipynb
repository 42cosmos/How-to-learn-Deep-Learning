{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('iris.csv')\n",
    "df = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.reindex(np.random.permutation(data.index))\n",
    "msk = np.random.rand(len(data)) < 0.8\n",
    "train = data[msk]\n",
    "test = data[~msk]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "labels = df['Species'].copy()\n",
    "test_labels = test['Species'].copy()\n",
    "df = df.drop(df.columns[[0, 5]], axis=1)\n",
    "test = test.drop(test.columns[[0, 5]], axis=1)\n",
    "labels = pd.get_dummies(labels, columns=['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels, dtype=np.float32)\n",
    "test_labels = np.array(labels, dtype=np.float32)\n",
    "df = np.array(df, dtype=np.float32)\n",
    "test = np.array(test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tf.input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tf.input_data(shape=[None, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = tf.fully_connected(net, 3, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = tf.regression(net)\n",
    "model = tf.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: 3NHIC5\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 150\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 0.053s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 150/150\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.99027\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 002 | loss: 0.99027 - acc: 0.3000 -- iter: 150/150\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m1.06396\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 003 | loss: 1.06396 - acc: 0.3436 -- iter: 150/150\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m1.09024\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 004 | loss: 1.09024 - acc: 0.3309 -- iter: 150/150\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m1.08117\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 005 | loss: 1.08117 - acc: 0.3834 -- iter: 150/150\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m1.07739\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 006 | loss: 1.07739 - acc: 0.3983 -- iter: 150/150\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m1.07488\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 007 | loss: 1.07488 - acc: 0.4113 -- iter: 150/150\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m1.08965\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 008 | loss: 1.08965 - acc: 0.3562 -- iter: 150/150\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m1.07883\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 009 | loss: 1.07883 - acc: 0.4006 -- iter: 150/150\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m1.07306\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 010 | loss: 1.07306 - acc: 0.4236 -- iter: 150/150\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m1.06936\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 011 | loss: 1.06936 - acc: 0.4345 -- iter: 150/150\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m1.06655\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 012 | loss: 1.06655 - acc: 0.4400 -- iter: 150/150\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m1.06414\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 013 | loss: 1.06414 - acc: 0.4429 -- iter: 150/150\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m1.06192\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 014 | loss: 1.06192 - acc: 0.4417 -- iter: 150/150\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.05978\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 015 | loss: 1.05978 - acc: 0.4332 -- iter: 150/150\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m1.05769\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 016 | loss: 1.05769 - acc: 0.4283 -- iter: 150/150\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m1.05561\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 017 | loss: 1.05561 - acc: 0.4253 -- iter: 150/150\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.05354\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 018 | loss: 1.05354 - acc: 0.4235 -- iter: 150/150\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m1.05149\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 019 | loss: 1.05149 - acc: 0.4201 -- iter: 150/150\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m1.04944\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 020 | loss: 1.04944 - acc: 0.4158 -- iter: 150/150\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m1.04739\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 021 | loss: 1.04739 - acc: 0.4129 -- iter: 150/150\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m1.04536\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 022 | loss: 1.04536 - acc: 0.4111 -- iter: 150/150\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m1.06205\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 023 | loss: 1.06205 - acc: 0.3866 -- iter: 150/150\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m1.05484\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 024 | loss: 1.05484 - acc: 0.3941 -- iter: 150/150\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m1.06836\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 025 | loss: 1.06836 - acc: 0.3739 -- iter: 150/150\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m1.05889\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 026 | loss: 1.05889 - acc: 0.3914 -- iter: 150/150\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.05166\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 027 | loss: 1.05166 - acc: 0.4159 -- iter: 150/150\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.04596\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 028 | loss: 1.04596 - acc: 0.4369 -- iter: 150/150\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.06039\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 029 | loss: 1.06039 - acc: 0.4133 -- iter: 150/150\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m1.07328\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 030 | loss: 1.07328 - acc: 0.3912 -- iter: 150/150\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m1.06186\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 031 | loss: 1.06186 - acc: 0.4456 -- iter: 150/150\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m1.07247\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 032 | loss: 1.07247 - acc: 0.4203 -- iter: 150/150\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m1.06116\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 033 | loss: 1.06116 - acc: 0.4715 -- iter: 150/150\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m1.06960\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 034 | loss: 1.06960 - acc: 0.4476 -- iter: 150/150\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m1.05885\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 035 | loss: 1.05885 - acc: 0.4920 -- iter: 150/150\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m1.05027\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 036 | loss: 1.05027 - acc: 0.5278 -- iter: 150/150\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m1.04331\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 037 | loss: 1.04331 - acc: 0.5555 -- iter: 150/150\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m1.03756\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 038 | loss: 1.03756 - acc: 0.5773 -- iter: 150/150\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m1.03272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 039 | loss: 1.03272 - acc: 0.5944 -- iter: 150/150\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m1.04788\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 040 | loss: 1.04788 - acc: 0.5429 -- iter: 150/150\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m1.04078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 041 | loss: 1.04078 - acc: 0.5657 -- iter: 150/150\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m1.03482\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 042 | loss: 1.03482 - acc: 0.5839 -- iter: 150/150\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m1.02975\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 043 | loss: 1.02975 - acc: 0.5985 -- iter: 150/150\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m1.02537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 044 | loss: 1.02537 - acc: 0.6103 -- iter: 150/150\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m1.02152\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 045 | loss: 1.02152 - acc: 0.6198 -- iter: 150/150\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.01808\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 046 | loss: 1.01808 - acc: 0.6276 -- iter: 150/150\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.01497\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 047 | loss: 1.01497 - acc: 0.6340 -- iter: 150/150\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.01212\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 048 | loss: 1.01212 - acc: 0.6393 -- iter: 150/150\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.00947\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 049 | loss: 1.00947 - acc: 0.6436 -- iter: 150/150\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.02542\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 050 | loss: 1.02542 - acc: 0.5955 -- iter: 150/150\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.02028\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 051 | loss: 1.02028 - acc: 0.6063 -- iter: 150/150\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.03237\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 052 | loss: 1.03237 - acc: 0.5674 -- iter: 150/150\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.02590\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 053 | loss: 1.02590 - acc: 0.5820 -- iter: 150/150\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.02024\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 054 | loss: 1.02024 - acc: 0.5943 -- iter: 150/150\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.01525\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 055 | loss: 1.01525 - acc: 0.6046 -- iter: 150/150\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.01080\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 056 | loss: 1.01080 - acc: 0.6134 -- iter: 150/150\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.00679\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 057 | loss: 1.00679 - acc: 0.6207 -- iter: 150/150\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.00315\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 058 | loss: 1.00315 - acc: 0.6270 -- iter: 150/150\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m0.99981\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 059 | loss: 0.99981 - acc: 0.6323 -- iter: 150/150\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.99673\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 060 | loss: 0.99673 - acc: 0.6369 -- iter: 150/150\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m0.99385\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 061 | loss: 0.99385 - acc: 0.6408 -- iter: 150/150\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m0.99114\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 062 | loss: 0.99114 - acc: 0.6441 -- iter: 150/150\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.98858\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 063 | loss: 0.98858 - acc: 0.6470 -- iter: 150/150\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.98614\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 064 | loss: 0.98614 - acc: 0.6494 -- iter: 150/150\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.98380\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 065 | loss: 0.98380 - acc: 0.6515 -- iter: 150/150\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.98155\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 066 | loss: 0.98155 - acc: 0.6534 -- iter: 150/150\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.97937\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 067 | loss: 0.97937 - acc: 0.6550 -- iter: 150/150\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.97725\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 068 | loss: 0.97725 - acc: 0.6564 -- iter: 150/150\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.97519\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 069 | loss: 0.97519 - acc: 0.6576 -- iter: 150/150\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.97318\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 070 | loss: 0.97318 - acc: 0.6586 -- iter: 150/150\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.97120\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 071 | loss: 0.97120 - acc: 0.6595 -- iter: 150/150\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.99127\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 072 | loss: 0.99127 - acc: 0.6161 -- iter: 150/150\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.98695\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 073 | loss: 0.98695 - acc: 0.6217 -- iter: 150/150\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.98297\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 074 | loss: 0.98297 - acc: 0.6266 -- iter: 150/150\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.97929\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 075 | loss: 0.97929 - acc: 0.6310 -- iter: 150/150\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.97587\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 076 | loss: 0.97587 - acc: 0.6348 -- iter: 150/150\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.97267\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 077 | loss: 0.97267 - acc: 0.6382 -- iter: 150/150\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.96967\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 078 | loss: 0.96967 - acc: 0.6412 -- iter: 150/150\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.96683\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 079 | loss: 0.96683 - acc: 0.6438 -- iter: 150/150\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.96414\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 080 | loss: 0.96414 - acc: 0.6461 -- iter: 150/150\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.96158\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 081 | loss: 0.96158 - acc: 0.6482 -- iter: 150/150\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.98001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 082 | loss: 0.98001 - acc: 0.6107 -- iter: 150/150\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.97556\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 083 | loss: 0.97556 - acc: 0.6163 -- iter: 150/150\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.97141\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 084 | loss: 0.97141 - acc: 0.6214 -- iter: 150/150\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.96752\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 085 | loss: 0.96752 - acc: 0.6259 -- iter: 150/150\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.96386\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 086 | loss: 0.96386 - acc: 0.6300 -- iter: 150/150\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.96040\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 087 | loss: 0.96040 - acc: 0.6336 -- iter: 150/150\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.95714\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 088 | loss: 0.95714 - acc: 0.6369 -- iter: 150/150\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.95403\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 089 | loss: 0.95403 - acc: 0.6399 -- iter: 150/150\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.95108\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 090 | loss: 0.95108 - acc: 0.6426 -- iter: 150/150\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.94825\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 091 | loss: 0.94825 - acc: 0.6450 -- iter: 150/150\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.96679\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 092 | loss: 0.96679 - acc: 0.6105 -- iter: 150/150\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.96209\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 093 | loss: 0.96209 - acc: 0.6161 -- iter: 150/150\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.95771\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 094 | loss: 0.95771 - acc: 0.6212 -- iter: 150/150\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.95363\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 095 | loss: 0.95363 - acc: 0.6257 -- iter: 150/150\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.97346\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 096 | loss: 0.97346 - acc: 0.5891 -- iter: 150/150\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.96752\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 097 | loss: 0.96752 - acc: 0.5969 -- iter: 150/150\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.98379\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 098 | loss: 0.98379 - acc: 0.5685 -- iter: 150/150\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.97658\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 099 | loss: 0.97658 - acc: 0.5784 -- iter: 150/150\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.96998\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 100 | loss: 0.96998 - acc: 0.5872 -- iter: 150/150\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.96392\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 101 | loss: 0.96392 - acc: 0.5951 -- iter: 150/150\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.95834\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 102 | loss: 0.95834 - acc: 0.6023 -- iter: 150/150\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.95318\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 103 | loss: 0.95318 - acc: 0.6087 -- iter: 150/150\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.97135\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 104 | loss: 0.97135 - acc: 0.5819 -- iter: 150/150\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.96465\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 105 | loss: 0.96465 - acc: 0.5903 -- iter: 150/150\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.98092\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 106 | loss: 0.98092 - acc: 0.5646 -- iter: 150/150\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.97304\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 107 | loss: 0.97304 - acc: 0.5748 -- iter: 150/150\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.98600\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 108 | loss: 0.98600 - acc: 0.5547 -- iter: 150/150\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.97741\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 109 | loss: 0.97741 - acc: 0.5659 -- iter: 150/150\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.99487\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 110 | loss: 0.99487 - acc: 0.5400 -- iter: 150/150\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.98523\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 111 | loss: 0.98523 - acc: 0.5526 -- iter: 150/150\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.97646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 112 | loss: 0.97646 - acc: 0.5640 -- iter: 150/150\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.96847\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 113 | loss: 0.96847 - acc: 0.5743 -- iter: 150/150\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.98205\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 114 | loss: 0.98205 - acc: 0.5535 -- iter: 150/150\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.97331\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 115 | loss: 0.97331 - acc: 0.5648 -- iter: 150/150\n",
      "--\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.96534\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 116 | loss: 0.96534 - acc: 0.5750 -- iter: 150/150\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.95806\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 117 | loss: 0.95806 - acc: 0.5842 -- iter: 150/150\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.95139\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 118 | loss: 0.95139 - acc: 0.5924 -- iter: 150/150\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.94526\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 119 | loss: 0.94526 - acc: 0.5999 -- iter: 150/150\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.93962\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 120 | loss: 0.93962 - acc: 0.6065 -- iter: 150/150\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.93441\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 121 | loss: 0.93441 - acc: 0.6126 -- iter: 150/150\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.95613\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 122 | loss: 0.95613 - acc: 0.5813 -- iter: 150/150\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.94901\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 123 | loss: 0.94901 - acc: 0.5898 -- iter: 150/150\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.94249\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 124 | loss: 0.94249 - acc: 0.5975 -- iter: 150/150\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.93650\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 125 | loss: 0.93650 - acc: 0.6044 -- iter: 150/150\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.95898\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 126 | loss: 0.95898 - acc: 0.5707 -- iter: 150/150\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.95110\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 127 | loss: 0.95110 - acc: 0.5803 -- iter: 150/150\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.94391\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 128 | loss: 0.94391 - acc: 0.5889 -- iter: 150/150\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.93731\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 129 | loss: 0.93731 - acc: 0.5967 -- iter: 150/150\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.95966\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 130 | loss: 0.95966 - acc: 0.5690 -- iter: 150/150\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.95127\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 131 | loss: 0.95127 - acc: 0.5788 -- iter: 150/150\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.94361\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 132 | loss: 0.94361 - acc: 0.5876 -- iter: 150/150\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.93661\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 133 | loss: 0.93661 - acc: 0.5955 -- iter: 150/150\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.95570\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 134 | loss: 0.95570 - acc: 0.5706 -- iter: 150/150\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.94728\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 135 | loss: 0.94728 - acc: 0.5802 -- iter: 150/150\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.93959\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 136 | loss: 0.93959 - acc: 0.5888 -- iter: 150/150\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.93257\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 137 | loss: 0.93257 - acc: 0.5966 -- iter: 150/150\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.92613\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 138 | loss: 0.92613 - acc: 0.6036 -- iter: 150/150\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.92021\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 139 | loss: 0.92021 - acc: 0.6099 -- iter: 150/150\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.94069\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 140 | loss: 0.94069 - acc: 0.5849 -- iter: 150/150\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.93309\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 141 | loss: 0.93309 - acc: 0.5931 -- iter: 150/150\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.92614\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 142 | loss: 0.92614 - acc: 0.6005 -- iter: 150/150\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.91978\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 143 | loss: 0.91978 - acc: 0.6071 -- iter: 150/150\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.91393\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 144 | loss: 0.91393 - acc: 0.6130 -- iter: 150/150\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.90854\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 145 | loss: 0.90854 - acc: 0.6184 -- iter: 150/150\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.90357\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 146 | loss: 0.90357 - acc: 0.6232 -- iter: 150/150\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.89896\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 147 | loss: 0.89896 - acc: 0.6276 -- iter: 150/150\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.92388\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 148 | loss: 0.92388 - acc: 0.5975 -- iter: 150/150\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.91700\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 149 | loss: 0.91700 - acc: 0.6044 -- iter: 150/150\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.91070\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 150 | loss: 0.91070 - acc: 0.6106 -- iter: 150/150\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.90492\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 151 | loss: 0.90492 - acc: 0.6162 -- iter: 150/150\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.92780\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 152 | loss: 0.92780 - acc: 0.5886 -- iter: 150/150\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.92008\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 153 | loss: 0.92008 - acc: 0.5964 -- iter: 150/150\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.91303\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 154 | loss: 0.91303 - acc: 0.6034 -- iter: 150/150\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.90657\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 155 | loss: 0.90657 - acc: 0.6098 -- iter: 150/150\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.92918\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 156 | loss: 0.92918 - acc: 0.5868 -- iter: 150/150\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.92090\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 157 | loss: 0.92090 - acc: 0.5948 -- iter: 150/150\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.91335\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 158 | loss: 0.91335 - acc: 0.6020 -- iter: 150/150\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.90645\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 159 | loss: 0.90645 - acc: 0.6084 -- iter: 150/150\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.93206\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 160 | loss: 0.93206 - acc: 0.5783 -- iter: 150/150\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.92309\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 161 | loss: 0.92309 - acc: 0.5871 -- iter: 150/150\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.94409\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 162 | loss: 0.94409 - acc: 0.5637 -- iter: 150/150\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.93374\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 163 | loss: 0.93374 - acc: 0.5740 -- iter: 150/150\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.92435\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 164 | loss: 0.92435 - acc: 0.5833 -- iter: 150/150\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.91580\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 165 | loss: 0.91580 - acc: 0.5916 -- iter: 150/150\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.90802\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 166 | loss: 0.90802 - acc: 0.5991 -- iter: 150/150\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.90091\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 167 | loss: 0.90091 - acc: 0.6059 -- iter: 150/150\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.92305\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 168 | loss: 0.92305 - acc: 0.5813 -- iter: 150/150\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.91424\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 169 | loss: 0.91424 - acc: 0.5898 -- iter: 150/150\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.90621\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 170 | loss: 0.90621 - acc: 0.5975 -- iter: 150/150\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.89889\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 171 | loss: 0.89889 - acc: 0.6044 -- iter: 150/150\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.92481\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 172 | loss: 0.92481 - acc: 0.5747 -- iter: 150/150\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.91544\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 173 | loss: 0.91544 - acc: 0.5839 -- iter: 150/150\n",
      "--\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.93987\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 174 | loss: 0.93987 - acc: 0.5581 -- iter: 150/150\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.92883\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 175 | loss: 0.92883 - acc: 0.5690 -- iter: 150/150\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.91881\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 176 | loss: 0.91881 - acc: 0.5788 -- iter: 150/150\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.90971\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 177 | loss: 0.90971 - acc: 0.5875 -- iter: 150/150\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.90142\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 178 | loss: 0.90142 - acc: 0.5955 -- iter: 150/150\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.89386\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 179 | loss: 0.89386 - acc: 0.6026 -- iter: 150/150\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.91596\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 180 | loss: 0.91596 - acc: 0.5777 -- iter: 150/150\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.90676\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 181 | loss: 0.90676 - acc: 0.5866 -- iter: 150/150\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.89838\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 182 | loss: 0.89838 - acc: 0.5946 -- iter: 150/150\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.89075\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 183 | loss: 0.89075 - acc: 0.6018 -- iter: 150/150\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.92140\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 184 | loss: 0.92140 - acc: 0.5696 -- iter: 150/150\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.91128\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 185 | loss: 0.91128 - acc: 0.5793 -- iter: 150/150\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.90209\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 186 | loss: 0.90209 - acc: 0.5880 -- iter: 150/150\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.89372\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 187 | loss: 0.89372 - acc: 0.5959 -- iter: 150/150\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.92339\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 188 | loss: 0.92339 - acc: 0.5670 -- iter: 150/150\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.91272\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 189 | loss: 0.91272 - acc: 0.5769 -- iter: 150/150\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.90304\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 190 | loss: 0.90304 - acc: 0.5866 -- iter: 150/150\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.89424\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 191 | loss: 0.89424 - acc: 0.5953 -- iter: 150/150\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.92093\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 192 | loss: 0.92093 - acc: 0.5711 -- iter: 150/150\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.91017\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 193 | loss: 0.91017 - acc: 0.5813 -- iter: 150/150\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.90040\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 194 | loss: 0.90040 - acc: 0.5905 -- iter: 150/150\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.89153\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 195 | loss: 0.89153 - acc: 0.5988 -- iter: 150/150\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.88345\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 196 | loss: 0.88345 - acc: 0.6062 -- iter: 150/150\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.87608\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 197 | loss: 0.87608 - acc: 0.6129 -- iter: 150/150\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.90852\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 198 | loss: 0.90852 - acc: 0.5830 -- iter: 150/150\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.89846\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 199 | loss: 0.89846 - acc: 0.5920 -- iter: 150/150\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.88933\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 200 | loss: 0.88933 - acc: 0.6002 -- iter: 150/150\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.88101\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 201 | loss: 0.88101 - acc: 0.6075 -- iter: 150/150\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.87344\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 202 | loss: 0.87344 - acc: 0.6141 -- iter: 150/150\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.86652\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 203 | loss: 0.86652 - acc: 0.6200 -- iter: 150/150\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.86019\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 204 | loss: 0.86019 - acc: 0.6253 -- iter: 150/150\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.85438\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 205 | loss: 0.85438 - acc: 0.6301 -- iter: 150/150\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.84905\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 206 | loss: 0.84905 - acc: 0.6344 -- iter: 150/150\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.84414\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 207 | loss: 0.84414 - acc: 0.6383 -- iter: 150/150\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.87652\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 208 | loss: 0.87652 - acc: 0.6105 -- iter: 150/150\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.86866\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 209 | loss: 0.86866 - acc: 0.6161 -- iter: 150/150\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.86149\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 210 | loss: 0.86149 - acc: 0.6212 -- iter: 150/150\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.85494\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 211 | loss: 0.85494 - acc: 0.6257 -- iter: 150/150\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.84895\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 212 | loss: 0.84895 - acc: 0.6298 -- iter: 150/150\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.84344\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 213 | loss: 0.84344 - acc: 0.6335 -- iter: 150/150\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.83839\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 214 | loss: 0.83839 - acc: 0.6368 -- iter: 150/150\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.83373\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 215 | loss: 0.83373 - acc: 0.6398 -- iter: 150/150\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.87222\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 216 | loss: 0.87222 - acc: 0.6032 -- iter: 150/150\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.86398\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 217 | loss: 0.86398 - acc: 0.6102 -- iter: 150/150\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.89452\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 218 | loss: 0.89452 - acc: 0.5852 -- iter: 150/150\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.88389\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 219 | loss: 0.88389 - acc: 0.5940 -- iter: 150/150\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.90633\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 220 | loss: 0.90633 - acc: 0.5746 -- iter: 150/150\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.89437\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 221 | loss: 0.89437 - acc: 0.5845 -- iter: 150/150\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.88354\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 222 | loss: 0.88354 - acc: 0.5933 -- iter: 150/150\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.87372\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 223 | loss: 0.87372 - acc: 0.6013 -- iter: 150/150\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.86480\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 224 | loss: 0.86480 - acc: 0.6085 -- iter: 150/150\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.85668\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 225 | loss: 0.85668 - acc: 0.6150 -- iter: 150/150\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.84929\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 226 | loss: 0.84929 - acc: 0.6208 -- iter: 150/150\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.84255\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 227 | loss: 0.84255 - acc: 0.6261 -- iter: 150/150\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.83638\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 228 | loss: 0.83638 - acc: 0.6308 -- iter: 150/150\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.83073\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 229 | loss: 0.83073 - acc: 0.6357 -- iter: 150/150\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.82555\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 230 | loss: 0.82555 - acc: 0.6408 -- iter: 150/150\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.82078\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 231 | loss: 0.82078 - acc: 0.6454 -- iter: 150/150\n",
      "--\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.81639\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 232 | loss: 0.81639 - acc: 0.6495 -- iter: 150/150\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.81232\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 233 | loss: 0.81232 - acc: 0.6533 -- iter: 150/150\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.80856\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 234 | loss: 0.80856 - acc: 0.6566 -- iter: 150/150\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.80506\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 235 | loss: 0.80506 - acc: 0.6596 -- iter: 150/150\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.84375\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 236 | loss: 0.84375 - acc: 0.6243 -- iter: 150/150\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.83654\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 237 | loss: 0.83654 - acc: 0.6305 -- iter: 150/150\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.82996\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 238 | loss: 0.82996 - acc: 0.6362 -- iter: 150/150\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.82394\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 239 | loss: 0.82394 - acc: 0.6412 -- iter: 150/150\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.81843\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 240 | loss: 0.81843 - acc: 0.6458 -- iter: 150/150\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.81337\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 241 | loss: 0.81337 - acc: 0.6498 -- iter: 150/150\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.80872\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 242 | loss: 0.80872 - acc: 0.6535 -- iter: 150/150\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.80443\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 243 | loss: 0.80443 - acc: 0.6568 -- iter: 150/150\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.80047\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 244 | loss: 0.80047 - acc: 0.6598 -- iter: 150/150\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.79681\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 245 | loss: 0.79681 - acc: 0.6625 -- iter: 150/150\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.79340\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 246 | loss: 0.79340 - acc: 0.6649 -- iter: 150/150\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.79023\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 247 | loss: 0.79023 - acc: 0.6671 -- iter: 150/150\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.78728\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 248 | loss: 0.78728 - acc: 0.6691 -- iter: 150/150\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.78451\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 249 | loss: 0.78451 - acc: 0.6708 -- iter: 150/150\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.83171\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 250 | loss: 0.83171 - acc: 0.6331 -- iter: 150/150\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.82431\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 251 | loss: 0.82431 - acc: 0.6384 -- iter: 150/150\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.81756\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 252 | loss: 0.81756 - acc: 0.6433 -- iter: 150/150\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.81141\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 253 | loss: 0.81141 - acc: 0.6476 -- iter: 150/150\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.86139\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 254 | loss: 0.86139 - acc: 0.6115 -- iter: 150/150\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.85070\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 255 | loss: 0.85070 - acc: 0.6190 -- iter: 150/150\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.84100\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 256 | loss: 0.84100 - acc: 0.6258 -- iter: 150/150\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.83220\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 257 | loss: 0.83220 - acc: 0.6319 -- iter: 150/150\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.82420\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 258 | loss: 0.82420 - acc: 0.6374 -- iter: 150/150\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.81692\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 259 | loss: 0.81692 - acc: 0.6423 -- iter: 150/150\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.81028\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 260 | loss: 0.81028 - acc: 0.6474 -- iter: 150/150\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.80422\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 261 | loss: 0.80422 - acc: 0.6520 -- iter: 150/150\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.79868\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 262 | loss: 0.79868 - acc: 0.6561 -- iter: 150/150\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.79360\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 263 | loss: 0.79360 - acc: 0.6605 -- iter: 150/150\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.83926\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 264 | loss: 0.83926 - acc: 0.6225 -- iter: 150/150\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.82996\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 265 | loss: 0.82996 - acc: 0.6309 -- iter: 150/150\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.82152\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 266 | loss: 0.82152 - acc: 0.6385 -- iter: 150/150\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.81384\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 267 | loss: 0.81384 - acc: 0.6453 -- iter: 150/150\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.85066\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 268 | loss: 0.85066 - acc: 0.6154 -- iter: 150/150\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.83993\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 269 | loss: 0.83993 - acc: 0.6245 -- iter: 150/150\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.87625\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 270 | loss: 0.87625 - acc: 0.5941 -- iter: 150/150\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.86284\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 271 | loss: 0.86284 - acc: 0.6053 -- iter: 150/150\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.85071\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 272 | loss: 0.85071 - acc: 0.6161 -- iter: 150/150\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.83973\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 273 | loss: 0.83973 - acc: 0.6272 -- iter: 150/150\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.82979\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 274 | loss: 0.82979 - acc: 0.6371 -- iter: 150/150\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.82077\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 275 | loss: 0.82077 - acc: 0.6468 -- iter: 150/150\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.81258\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 276 | loss: 0.81258 - acc: 0.6548 -- iter: 150/150\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.80513\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 277 | loss: 0.80513 - acc: 0.6619 -- iter: 150/150\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.79834\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 278 | loss: 0.79834 - acc: 0.6684 -- iter: 150/150\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.79215\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 279 | loss: 0.79215 - acc: 0.6729 -- iter: 150/150\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.83387\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 280 | loss: 0.83387 - acc: 0.6396 -- iter: 150/150\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.82398\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 281 | loss: 0.82398 - acc: 0.6463 -- iter: 150/150\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.81500\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 282 | loss: 0.81500 - acc: 0.6524 -- iter: 150/150\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.80684\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 283 | loss: 0.80684 - acc: 0.6578 -- iter: 150/150\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.79942\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 284 | loss: 0.79942 - acc: 0.6627 -- iter: 150/150\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.79267\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 285 | loss: 0.79267 - acc: 0.6671 -- iter: 150/150\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.84057\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 286 | loss: 0.84057 - acc: 0.6284 -- iter: 150/150\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.82956\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 287 | loss: 0.82956 - acc: 0.6362 -- iter: 150/150\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.81958\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 288 | loss: 0.81958 - acc: 0.6432 -- iter: 150/150\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.81053\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 289 | loss: 0.81053 - acc: 0.6496 -- iter: 150/150\n",
      "--\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.80231\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 290 | loss: 0.80231 - acc: 0.6553 -- iter: 150/150\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.79484\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 291 | loss: 0.79484 - acc: 0.6604 -- iter: 150/150\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.83739\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 292 | loss: 0.83739 - acc: 0.6291 -- iter: 150/150\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.82627\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 293 | loss: 0.82627 - acc: 0.6388 -- iter: 150/150\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.81621\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 294 | loss: 0.81621 - acc: 0.6476 -- iter: 150/150\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.80708\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 295 | loss: 0.80708 - acc: 0.6562 -- iter: 150/150\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.85008\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 296 | loss: 0.85008 - acc: 0.6232 -- iter: 150/150\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.83744\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 297 | loss: 0.83744 - acc: 0.6349 -- iter: 150/150\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.87598\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 298 | loss: 0.87598 - acc: 0.6041 -- iter: 150/150\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.86065\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 299 | loss: 0.86065 - acc: 0.6177 -- iter: 150/150\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.89214\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 300 | loss: 0.89214 - acc: 0.5966 -- iter: 150/150\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.87512\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 301 | loss: 0.87512 - acc: 0.6109 -- iter: 150/150\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.91393\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 302 | loss: 0.91393 - acc: 0.5778 -- iter: 150/150\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.89467\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 303 | loss: 0.89467 - acc: 0.5940 -- iter: 150/150\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.92939\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 304 | loss: 0.92939 - acc: 0.5700 -- iter: 150/150\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.90854\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 305 | loss: 0.90854 - acc: 0.5876 -- iter: 150/150\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.93788\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 306 | loss: 0.93788 - acc: 0.5629 -- iter: 150/150\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.91615\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 307 | loss: 0.91615 - acc: 0.5813 -- iter: 150/150\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.89658\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 308 | loss: 0.89658 - acc: 0.5985 -- iter: 150/150\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.87893\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 309 | loss: 0.87893 - acc: 0.6139 -- iter: 150/150\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.91708\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 310 | loss: 0.91708 - acc: 0.5832 -- iter: 150/150\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.89733\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 311 | loss: 0.89733 - acc: 0.6016 -- iter: 150/150\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.87952\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 312 | loss: 0.87952 - acc: 0.6181 -- iter: 150/150\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.86346\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 313 | loss: 0.86346 - acc: 0.6329 -- iter: 150/150\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.84895\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 314 | loss: 0.84895 - acc: 0.6463 -- iter: 150/150\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.83585\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 315 | loss: 0.83585 - acc: 0.6583 -- iter: 150/150\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.82399\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 316 | loss: 0.82399 - acc: 0.6692 -- iter: 150/150\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.81325\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 317 | loss: 0.81325 - acc: 0.6789 -- iter: 150/150\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.80351\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 318 | loss: 0.80351 - acc: 0.6877 -- iter: 150/150\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.79467\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 319 | loss: 0.79467 - acc: 0.6949 -- iter: 150/150\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.83761\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 320 | loss: 0.83761 - acc: 0.6694 -- iter: 150/150\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.82522\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 321 | loss: 0.82522 - acc: 0.6694 -- iter: 150/150\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.87258\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 322 | loss: 0.87258 - acc: 0.6312 -- iter: 150/150\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.85659\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 323 | loss: 0.85659 - acc: 0.6427 -- iter: 150/150\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.89621\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 324 | loss: 0.89621 - acc: 0.6111 -- iter: 150/150\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.87778\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 325 | loss: 0.87778 - acc: 0.6247 -- iter: 150/150\n",
      "--\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.91437\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 326 | loss: 0.91437 - acc: 0.5949 -- iter: 150/150\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.89407\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 327 | loss: 0.89407 - acc: 0.6094 -- iter: 150/150\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.93213\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 328 | loss: 0.93213 - acc: 0.5791 -- iter: 150/150\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.91001\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 329 | loss: 0.91001 - acc: 0.6109 -- iter: 150/150\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.89008\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 330 | loss: 0.89008 - acc: 0.6109 -- iter: 150/150\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.87210\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 331 | loss: 0.87210 - acc: 0.6245 -- iter: 150/150\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.85589\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 332 | loss: 0.85589 - acc: 0.6367 -- iter: 150/150\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.84125\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 333 | loss: 0.84125 - acc: 0.6484 -- iter: 150/150\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.82802\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 334 | loss: 0.82802 - acc: 0.6589 -- iter: 150/150\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.81606\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 335 | loss: 0.81606 - acc: 0.6683 -- iter: 150/150\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.80523\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 336 | loss: 0.80523 - acc: 0.6768 -- iter: 150/150\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.79542\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 337 | loss: 0.79542 - acc: 0.6845 -- iter: 150/150\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.78652\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 338 | loss: 0.78652 - acc: 0.6914 -- iter: 150/150\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.77844\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 339 | loss: 0.77844 - acc: 0.6976 -- iter: 150/150\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.82183\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 340 | loss: 0.82183 - acc: 0.6638 -- iter: 150/150\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.81008\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 341 | loss: 0.81008 - acc: 0.6728 -- iter: 150/150\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.79945\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 342 | loss: 0.79945 - acc: 0.6808 -- iter: 150/150\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.78981\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 343 | loss: 0.78981 - acc: 0.6881 -- iter: 150/150\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.82444\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 344 | loss: 0.82444 - acc: 0.6573 -- iter: 150/150\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.81219\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 345 | loss: 0.81219 - acc: 0.6669 -- iter: 150/150\n",
      "--\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.84630\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 346 | loss: 0.84630 - acc: 0.6395 -- iter: 150/150\n",
      "--\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.83176\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 347 | loss: 0.83176 - acc: 0.6509 -- iter: 150/150\n",
      "--\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.87211\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 348 | loss: 0.87211 - acc: 0.6211 -- iter: 150/150\n",
      "--\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.85491\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 349 | loss: 0.85491 - acc: 0.6344 -- iter: 150/150\n",
      "--\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.83939\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 350 | loss: 0.83939 - acc: 0.6463 -- iter: 150/150\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.82537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 351 | loss: 0.82537 - acc: 0.6570 -- iter: 150/150\n",
      "--\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.86094\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 352 | loss: 0.86094 - acc: 0.6279 -- iter: 150/150\n",
      "--\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.84469\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 353 | loss: 0.84469 - acc: 0.6418 -- iter: 150/150\n",
      "--\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.88522\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 354 | loss: 0.88522 - acc: 0.6050 -- iter: 150/150\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.86647\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 355 | loss: 0.86647 - acc: 0.6211 -- iter: 150/150\n",
      "--\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.84956\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 356 | loss: 0.84956 - acc: 0.6357 -- iter: 150/150\n",
      "--\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.83430\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 357 | loss: 0.83430 - acc: 0.6488 -- iter: 150/150\n",
      "--\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.87806\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 358 | loss: 0.87806 - acc: 0.6219 -- iter: 150/150\n",
      "--\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.85988\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 359 | loss: 0.85988 - acc: 0.6364 -- iter: 150/150\n",
      "--\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.84349\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 360 | loss: 0.84349 - acc: 0.6494 -- iter: 150/150\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.82869\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 361 | loss: 0.82869 - acc: 0.6611 -- iter: 150/150\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.86665\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 362 | loss: 0.86665 - acc: 0.6304 -- iter: 150/150\n",
      "--\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.84946\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 363 | loss: 0.84946 - acc: 0.6440 -- iter: 150/150\n",
      "--\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.83395\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 364 | loss: 0.83395 - acc: 0.6563 -- iter: 150/150\n",
      "--\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.81995\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 365 | loss: 0.81995 - acc: 0.6666 -- iter: 150/150\n",
      "--\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.86575\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 366 | loss: 0.86575 - acc: 0.6306 -- iter: 150/150\n",
      "--\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.84849\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 367 | loss: 0.84849 - acc: 0.6429 -- iter: 150/150\n",
      "--\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.88728\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 368 | loss: 0.88728 - acc: 0.6093 -- iter: 150/150\n",
      "--\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.86780\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 369 | loss: 0.86780 - acc: 0.6244 -- iter: 150/150\n",
      "--\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.85024\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 370 | loss: 0.85024 - acc: 0.6386 -- iter: 150/150\n",
      "--\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.83439\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 371 | loss: 0.83439 - acc: 0.6514 -- iter: 150/150\n",
      "--\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.88045\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 372 | loss: 0.88045 - acc: 0.6149 -- iter: 150/150\n",
      "--\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.86152\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 373 | loss: 0.86152 - acc: 0.6301 -- iter: 150/150\n",
      "--\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.84444\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 374 | loss: 0.84444 - acc: 0.6438 -- iter: 150/150\n",
      "--\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.82903\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 375 | loss: 0.82903 - acc: 0.6560 -- iter: 150/150\n",
      "--\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.81511\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 376 | loss: 0.81511 - acc: 0.6691 -- iter: 150/150\n",
      "--\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.80254\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 377 | loss: 0.80254 - acc: 0.6809 -- iter: 150/150\n",
      "--\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.79116\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 378 | loss: 0.79116 - acc: 0.6921 -- iter: 150/150\n",
      "--\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.78086\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 379 | loss: 0.78086 - acc: 0.7022 -- iter: 150/150\n",
      "--\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.77153\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 380 | loss: 0.77153 - acc: 0.7113 -- iter: 150/150\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.76306\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 381 | loss: 0.76306 - acc: 0.7189 -- iter: 150/150\n",
      "--\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.75537\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 382 | loss: 0.75537 - acc: 0.7243 -- iter: 150/150\n",
      "--\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.74838\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 383 | loss: 0.74838 - acc: 0.7286 -- iter: 150/150\n",
      "--\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.79360\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 384 | loss: 0.79360 - acc: 0.6930 -- iter: 150/150\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.78266\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 385 | loss: 0.78266 - acc: 0.7004 -- iter: 150/150\n",
      "--\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.82803\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 386 | loss: 0.82803 - acc: 0.6664 -- iter: 150/150\n",
      "--\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.81354\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 387 | loss: 0.81354 - acc: 0.6764 -- iter: 150/150\n",
      "--\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.85501\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 388 | loss: 0.85501 - acc: 0.6441 -- iter: 150/150\n",
      "--\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.83774\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 389 | loss: 0.83774 - acc: 0.6563 -- iter: 150/150\n",
      "--\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.87763\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 390 | loss: 0.87763 - acc: 0.6220 -- iter: 150/150\n",
      "--\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.85803\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 391 | loss: 0.85803 - acc: 0.6365 -- iter: 150/150\n",
      "--\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.84037\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 392 | loss: 0.84037 - acc: 0.6495 -- iter: 150/150\n",
      "--\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.82443\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 393 | loss: 0.82443 - acc: 0.6612 -- iter: 150/150\n",
      "--\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.81005\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 394 | loss: 0.81005 - acc: 0.6718 -- iter: 150/150\n",
      "--\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.79706\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 395 | loss: 0.79706 - acc: 0.6813 -- iter: 150/150\n",
      "--\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.78531\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 396 | loss: 0.78531 - acc: 0.6898 -- iter: 150/150\n",
      "--\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.77469\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 397 | loss: 0.77469 - acc: 0.6975 -- iter: 150/150\n",
      "--\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.76507\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 398 | loss: 0.76507 - acc: 0.7044 -- iter: 150/150\n",
      "--\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.75635\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 399 | loss: 0.75635 - acc: 0.7106 -- iter: 150/150\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.80254\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 400 | loss: 0.80254 - acc: 0.6729 -- iter: 150/150\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.78996\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 401 | loss: 0.78996 - acc: 0.6829 -- iter: 150/150\n",
      "--\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.77859\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 402 | loss: 0.77859 - acc: 0.6940 -- iter: 150/150\n",
      "--\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.76830\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 403 | loss: 0.76830 - acc: 0.7039 -- iter: 150/150\n",
      "--\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.82617\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 404 | loss: 0.82617 - acc: 0.6635 -- iter: 150/150\n",
      "--\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.81103\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 405 | loss: 0.81103 - acc: 0.6765 -- iter: 150/150\n",
      "--\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.79736\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 406 | loss: 0.79736 - acc: 0.6882 -- iter: 150/150\n",
      "--\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.78500\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 407 | loss: 0.78500 - acc: 0.6967 -- iter: 150/150\n",
      "--\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.83286\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 408 | loss: 0.83286 - acc: 0.6557 -- iter: 150/150\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.81687\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 409 | loss: 0.81687 - acc: 0.6675 -- iter: 150/150\n",
      "--\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.86088\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 410 | loss: 0.86088 - acc: 0.6361 -- iter: 150/150\n",
      "--\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.84202\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 411 | loss: 0.84202 - acc: 0.6518 -- iter: 150/150\n",
      "--\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.88777\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 412 | loss: 0.88777 - acc: 0.6153 -- iter: 150/150\n",
      "--\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.86618\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 413 | loss: 0.86618 - acc: 0.6337 -- iter: 150/150\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.84673\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 414 | loss: 0.84673 - acc: 0.6510 -- iter: 150/150\n",
      "--\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.82919\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 415 | loss: 0.82919 - acc: 0.6679 -- iter: 150/150\n",
      "--\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.81338\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 416 | loss: 0.81338 - acc: 0.6831 -- iter: 150/150\n",
      "--\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.79910\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 417 | loss: 0.79910 - acc: 0.6968 -- iter: 150/150\n",
      "--\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.78621\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 418 | loss: 0.78621 - acc: 0.7091 -- iter: 150/150\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.77455\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 419 | loss: 0.77455 - acc: 0.7202 -- iter: 150/150\n",
      "--\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.76401\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 420 | loss: 0.76401 - acc: 0.7302 -- iter: 150/150\n",
      "--\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.75446\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 421 | loss: 0.75446 - acc: 0.7392 -- iter: 150/150\n",
      "--\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.74581\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 422 | loss: 0.74581 - acc: 0.7473 -- iter: 150/150\n",
      "--\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.73796\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 423 | loss: 0.73796 - acc: 0.7539 -- iter: 150/150\n",
      "--\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.73083\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 424 | loss: 0.73083 - acc: 0.7592 -- iter: 150/150\n",
      "--\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.72434\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 425 | loss: 0.72434 - acc: 0.7632 -- iter: 150/150\n",
      "--\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.78172\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 426 | loss: 0.78172 - acc: 0.7189 -- iter: 150/150\n",
      "--\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.77003\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 427 | loss: 0.77003 - acc: 0.7270 -- iter: 150/150\n",
      "--\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.75945\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 428 | loss: 0.75945 - acc: 0.7343 -- iter: 150/150\n",
      "--\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.74988\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 429 | loss: 0.74988 - acc: 0.7409 -- iter: 150/150\n",
      "--\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.80031\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 430 | loss: 0.80031 - acc: 0.7021 -- iter: 150/150\n",
      "--\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.78655\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 431 | loss: 0.78655 - acc: 0.7119 -- iter: 150/150\n",
      "--\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.77412\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 432 | loss: 0.77412 - acc: 0.7214 -- iter: 150/150\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.76288\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 433 | loss: 0.76288 - acc: 0.7306 -- iter: 150/150\n",
      "--\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.75272\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 434 | loss: 0.75272 - acc: 0.7389 -- iter: 150/150\n",
      "--\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.74351\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 435 | loss: 0.74351 - acc: 0.7463 -- iter: 150/150\n",
      "--\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.73517\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 436 | loss: 0.73517 - acc: 0.7530 -- iter: 150/150\n",
      "--\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.72761\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 437 | loss: 0.72761 - acc: 0.7590 -- iter: 150/150\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.78199\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 438 | loss: 0.78199 - acc: 0.7158 -- iter: 150/150\n",
      "--\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.76963\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 439 | loss: 0.76963 - acc: 0.7249 -- iter: 150/150\n",
      "--\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.75846\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 440 | loss: 0.75846 - acc: 0.7331 -- iter: 150/150\n",
      "--\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.74836\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 441 | loss: 0.74836 - acc: 0.7398 -- iter: 150/150\n",
      "--\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.73922\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 442 | loss: 0.73922 - acc: 0.7458 -- iter: 150/150\n",
      "--\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.73093\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 443 | loss: 0.73093 - acc: 0.7512 -- iter: 150/150\n",
      "--\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.79338\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 444 | loss: 0.79338 - acc: 0.7094 -- iter: 150/150\n",
      "--\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.77957\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 445 | loss: 0.77957 - acc: 0.7178 -- iter: 150/150\n",
      "--\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.83219\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 446 | loss: 0.83219 - acc: 0.6767 -- iter: 150/150\n",
      "--\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.81444\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 447 | loss: 0.81444 - acc: 0.6884 -- iter: 150/150\n",
      "--\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.87147\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 448 | loss: 0.87147 - acc: 0.6522 -- iter: 150/150\n",
      "--\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.84974\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 449 | loss: 0.84974 - acc: 0.6670 -- iter: 150/150\n",
      "--\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.88662\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 450 | loss: 0.88662 - acc: 0.6383 -- iter: 150/150\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.86335\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 451 | loss: 0.86335 - acc: 0.6558 -- iter: 150/150\n",
      "--\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.84239\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 452 | loss: 0.84239 - acc: 0.6722 -- iter: 150/150\n",
      "--\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.82351\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 453 | loss: 0.82351 - acc: 0.6870 -- iter: 150/150\n",
      "--\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.80649\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 454 | loss: 0.80649 - acc: 0.7016 -- iter: 150/150\n",
      "--\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.79114\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 455 | loss: 0.79114 - acc: 0.7148 -- iter: 150/150\n",
      "--\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.77728\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 456 | loss: 0.77728 - acc: 0.7273 -- iter: 150/150\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.76476\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 457 | loss: 0.76476 - acc: 0.7386 -- iter: 150/150\n",
      "--\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.75345\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 458 | loss: 0.75345 - acc: 0.7487 -- iter: 150/150\n",
      "--\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.74322\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 459 | loss: 0.74322 - acc: 0.7585 -- iter: 150/150\n",
      "--\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.73396\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 460 | loss: 0.73396 - acc: 0.7673 -- iter: 150/150\n",
      "--\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.72556\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 461 | loss: 0.72556 - acc: 0.7753 -- iter: 150/150\n",
      "--\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.71795\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 462 | loss: 0.71795 - acc: 0.7824 -- iter: 150/150\n",
      "--\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.71103\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 463 | loss: 0.71103 - acc: 0.7888 -- iter: 150/150\n",
      "--\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.70475\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 464 | loss: 0.70475 - acc: 0.7946 -- iter: 150/150\n",
      "--\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.69902\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 465 | loss: 0.69902 - acc: 0.7998 -- iter: 150/150\n",
      "--\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.69381\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 466 | loss: 0.69381 - acc: 0.8045 -- iter: 150/150\n",
      "--\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.68905\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 467 | loss: 0.68905 - acc: 0.8081 -- iter: 150/150\n",
      "--\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.74572\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 468 | loss: 0.74572 - acc: 0.7626 -- iter: 150/150\n",
      "--\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.73565\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 469 | loss: 0.73565 - acc: 0.7703 -- iter: 150/150\n",
      "--\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.72653\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 470 | loss: 0.72653 - acc: 0.7773 -- iter: 150/150\n",
      "--\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.71827\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 471 | loss: 0.71827 - acc: 0.7829 -- iter: 150/150\n",
      "--\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.71079\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 472 | loss: 0.71079 - acc: 0.7879 -- iter: 150/150\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.70399\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 473 | loss: 0.70399 - acc: 0.7925 -- iter: 150/150\n",
      "--\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.69781\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 474 | loss: 0.69781 - acc: 0.7990 -- iter: 150/150\n",
      "--\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.69219\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 475 | loss: 0.69219 - acc: 0.7990 -- iter: 150/150\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.68707\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 476 | loss: 0.68707 - acc: 0.8011 -- iter: 150/150\n",
      "--\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.68240\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 477 | loss: 0.68240 - acc: 0.8023 -- iter: 150/150\n",
      "--\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.74561\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 478 | loss: 0.74561 - acc: 0.7514 -- iter: 150/150\n",
      "--\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.73498\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 479 | loss: 0.73498 - acc: 0.7576 -- iter: 150/150\n",
      "--\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.72536\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 480 | loss: 0.72536 - acc: 0.7632 -- iter: 150/150\n",
      "--\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.71666\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 481 | loss: 0.71666 - acc: 0.7682 -- iter: 150/150\n",
      "--\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.76406\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 482 | loss: 0.76406 - acc: 0.7327 -- iter: 150/150\n",
      "--\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.75140\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 483 | loss: 0.75140 - acc: 0.7408 -- iter: 150/150\n",
      "--\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.73995\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 484 | loss: 0.73995 - acc: 0.7494 -- iter: 150/150\n",
      "--\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.72960\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 485 | loss: 0.72960 - acc: 0.7578 -- iter: 150/150\n",
      "--\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.72024\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 486 | loss: 0.72024 - acc: 0.7728 -- iter: 150/150\n",
      "--\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.71177\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 487 | loss: 0.71177 - acc: 0.7728 -- iter: 150/150\n",
      "--\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.76960\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 488 | loss: 0.76960 - acc: 0.7322 -- iter: 150/150\n",
      "--\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.75610\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 489 | loss: 0.75610 - acc: 0.7436 -- iter: 150/150\n",
      "--\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.74391\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 490 | loss: 0.74391 - acc: 0.7539 -- iter: 150/150\n",
      "--\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.73290\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 491 | loss: 0.73290 - acc: 0.7632 -- iter: 150/150\n",
      "--\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.78350\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 492 | loss: 0.78350 - acc: 0.7255 -- iter: 150/150\n",
      "--\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.76845\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 493 | loss: 0.76845 - acc: 0.7377 -- iter: 150/150\n",
      "--\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.75487\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 494 | loss: 0.75487 - acc: 0.7584 -- iter: 150/150\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.74260\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 495 | loss: 0.74260 - acc: 0.7584 -- iter: 150/150\n",
      "--\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.80206\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 496 | loss: 0.80206 - acc: 0.7132 -- iter: 150/150\n",
      "--\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.78501\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 497 | loss: 0.78501 - acc: 0.7279 -- iter: 150/150\n",
      "--\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.76963\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 498 | loss: 0.76963 - acc: 0.7418 -- iter: 150/150\n",
      "--\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.75576\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 499 | loss: 0.75576 - acc: 0.7542 -- iter: 150/150\n",
      "--\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.74324\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 500 | loss: 0.74324 - acc: 0.7655 -- iter: 150/150\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit(df, labels, n_epoch=500, batch_size=150, show_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training test score [0.29411765933036804]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(test)\n",
    "pred = pd.DataFrame(pred)\n",
    "score = model.evaluate(test, test_labels)\n",
    "print('Training test score', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.666469</td>\n",
       "      <td>0.221619</td>\n",
       "      <td>0.111912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060794</td>\n",
       "      <td>0.402876</td>\n",
       "      <td>0.536330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.084424</td>\n",
       "      <td>0.392068</td>\n",
       "      <td>0.523509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.193943</td>\n",
       "      <td>0.420825</td>\n",
       "      <td>0.385232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.690923</td>\n",
       "      <td>0.206210</td>\n",
       "      <td>0.102867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.218055</td>\n",
       "      <td>0.371331</td>\n",
       "      <td>0.410614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.127170</td>\n",
       "      <td>0.374001</td>\n",
       "      <td>0.498829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.127567</td>\n",
       "      <td>0.442508</td>\n",
       "      <td>0.429925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.188786</td>\n",
       "      <td>0.442645</td>\n",
       "      <td>0.368569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.645486</td>\n",
       "      <td>0.236235</td>\n",
       "      <td>0.118279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.734593</td>\n",
       "      <td>0.177061</td>\n",
       "      <td>0.088346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.106449</td>\n",
       "      <td>0.387858</td>\n",
       "      <td>0.505693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.109706</td>\n",
       "      <td>0.408274</td>\n",
       "      <td>0.482020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.203916</td>\n",
       "      <td>0.405609</td>\n",
       "      <td>0.390474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.722209</td>\n",
       "      <td>0.187202</td>\n",
       "      <td>0.090589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.731976</td>\n",
       "      <td>0.179078</td>\n",
       "      <td>0.088946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.142444</td>\n",
       "      <td>0.385231</td>\n",
       "      <td>0.472325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.097072</td>\n",
       "      <td>0.366362</td>\n",
       "      <td>0.536565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.089819</td>\n",
       "      <td>0.391779</td>\n",
       "      <td>0.518401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.185846</td>\n",
       "      <td>0.384555</td>\n",
       "      <td>0.429599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.656857</td>\n",
       "      <td>0.230497</td>\n",
       "      <td>0.112646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.229384</td>\n",
       "      <td>0.391851</td>\n",
       "      <td>0.378765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.166037</td>\n",
       "      <td>0.404539</td>\n",
       "      <td>0.429425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.028511</td>\n",
       "      <td>0.356662</td>\n",
       "      <td>0.614827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.675035</td>\n",
       "      <td>0.211996</td>\n",
       "      <td>0.112969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.175106</td>\n",
       "      <td>0.404861</td>\n",
       "      <td>0.420033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.146884</td>\n",
       "      <td>0.414557</td>\n",
       "      <td>0.438559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.084053</td>\n",
       "      <td>0.340938</td>\n",
       "      <td>0.575009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.691979</td>\n",
       "      <td>0.205461</td>\n",
       "      <td>0.102559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.658516</td>\n",
       "      <td>0.225266</td>\n",
       "      <td>0.116219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.195617</td>\n",
       "      <td>0.392241</td>\n",
       "      <td>0.412142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.080405</td>\n",
       "      <td>0.352332</td>\n",
       "      <td>0.567263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.087588</td>\n",
       "      <td>0.393141</td>\n",
       "      <td>0.519271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.164632</td>\n",
       "      <td>0.421466</td>\n",
       "      <td>0.413902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2\n",
       "0   0.666469  0.221619  0.111912\n",
       "1   0.060794  0.402876  0.536330\n",
       "2   0.084424  0.392068  0.523509\n",
       "3   0.193943  0.420825  0.385232\n",
       "4   0.690923  0.206210  0.102867\n",
       "5   0.218055  0.371331  0.410614\n",
       "6   0.127170  0.374001  0.498829\n",
       "7   0.127567  0.442508  0.429925\n",
       "8   0.188786  0.442645  0.368569\n",
       "9   0.645486  0.236235  0.118279\n",
       "10  0.734593  0.177061  0.088346\n",
       "11  0.106449  0.387858  0.505693\n",
       "12  0.109706  0.408274  0.482020\n",
       "13  0.203916  0.405609  0.390474\n",
       "14  0.722209  0.187202  0.090589\n",
       "15  0.731976  0.179078  0.088946\n",
       "16  0.142444  0.385231  0.472325\n",
       "17  0.097072  0.366362  0.536565\n",
       "18  0.089819  0.391779  0.518401\n",
       "19  0.185846  0.384555  0.429599\n",
       "20  0.656857  0.230497  0.112646\n",
       "21  0.229384  0.391851  0.378765\n",
       "22  0.166037  0.404539  0.429425\n",
       "23  0.028511  0.356662  0.614827\n",
       "24  0.675035  0.211996  0.112969\n",
       "25  0.175106  0.404861  0.420033\n",
       "26  0.146884  0.414557  0.438559\n",
       "27  0.084053  0.340938  0.575009\n",
       "28  0.691979  0.205461  0.102559\n",
       "29  0.658516  0.225266  0.116219\n",
       "30  0.195617  0.392241  0.412142\n",
       "31  0.080405  0.352332  0.567263\n",
       "32  0.087588  0.393141  0.519271\n",
       "33  0.164632  0.421466  0.413902"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
