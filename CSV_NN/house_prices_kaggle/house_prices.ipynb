{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdf5 is not supported on this machine (please install/reinstall h5py for optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import tflearn as tf\n",
    "import tensorflow as tff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1459, 80) (1460, 81)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "print(test_data.shape, train_data.shape)\n",
    "test_data['SalePrice'] = 0\n",
    "pd.options.display.max_rows = 3000\n",
    "pd.options.display.max_columns = 999\n",
    "all_data = pd.concat([train_data, test_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2919, 247)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1459, 244)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_data(data):\n",
    "        data[\"LotFrontage\"].fillna(data[\"LotFrontage\"].mean(), inplace=True)\n",
    "        data[\"MasVnrArea\"].fillna(data[\"MasVnrArea\"].mean(), inplace=True)\n",
    "        data[\"GarageYrBlt\"].fillna(data[\"GarageYrBlt\"].mean(), inplace=True)\n",
    "        data = pd.get_dummies(data, columns=['MSZoning', 'Street',\n",
    "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
    "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
    "       'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
    "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n",
    "       'BsmtFinType2', 'Heating',\n",
    "       'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC',\n",
    "       'Fence', 'MiscFeature', 'SaleType', 'SaleCondition'], drop_first=True)\n",
    "        print(data.shape)\n",
    "        return data\n",
    "\n",
    "all_data = clean_data(all_data)\n",
    "train_data = all_data.iloc[:1459]\n",
    "test_data = all_data.iloc[1459:]\n",
    "ids = all_data.Id.iloc[1460:]\n",
    "test_data = test_data.drop(test_data.columns[0], axis=1)\n",
    "train_data = train_data.drop(train_data.columns[0], axis=1)\n",
    "train_labels = pd.DataFrame()\n",
    "train_labels = train_data['SalePrice'].copy()\n",
    "train_data = train_data.drop(train_data.columns[-1], axis=1)\n",
    "test_data = train_data.drop(train_data.columns[-1], axis=1)\n",
    "train_data.shape\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ewallner/Library/Python/3.6/lib/python/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  return getattr(obj, method)(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.reshape(train_labels, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1459, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.array(train_data, dtype=np.int32)\n",
    "test_data = np.array(train_data, dtype=np.int32)\n",
    "train_labels = np.array(train_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[208500],\n",
       "       [181500],\n",
       "       [223500],\n",
       "       ..., \n",
       "       [210000],\n",
       "       [266500],\n",
       "       [142125]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tff.reset_default_graph()\n",
    "#r2 = tf.R2()\n",
    "#net = tf.input_data(shape=[None, train_data.shape[1]])\n",
    "#net = tf.fully_connected(net, 30)\n",
    "#net = tf.fully_connected(net, 10)\n",
    "#net = tf.fully_connected(net, 1, activation='linear')\n",
    "#sgd = tf.SGD()\n",
    "#net = tf.regression(net)\n",
    "#model = tf.DNN(net)\n",
    "r2 = tf.R2()\n",
    "net = tf.input_data(shape=[None, train_data.shape[1]])\n",
    "net = tf.fully_connected(net, 30, activation='linear')\n",
    "net = tf.fully_connected(net, 10, activation = 'linear')\n",
    "net = tf.fully_connected(net, 1, activation = 'linear')\n",
    "sgd = tf.SGD(learning_rate=0.1, lr_decay = 0.01, decay_step=100)\n",
    "net = tf.regression(net, optimizer=sgd, loss='mean_square', metric=r2)\n",
    "model = tf.DNN(net)\n",
    "model.fit(train_data, train_labels, show_metric=True, validation_set=0.2, shuffle = True, n_epoch=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tff.reset_default_graph()\n",
    "# Network building\n",
    "net = tf.input_data([None, train_data.shape[1]])\n",
    "net = tf.fully_connected(net, 64, activation='linear',\n",
    "                                 regularizer='L2', weight_decay=0.0005)\n",
    "#net = tf.embedding(net, input_dim=100, output_dim=128)\n",
    "#net = tf.lstm(net, 128, dropout=0.1)\n",
    "net = tf.fully_connected(net, 1, activation='linear')\n",
    "net = tf.regression(net, optimizer=tf.optimizers.AdaGrad(learning_rate=0.01, initial_accumulator_value=0.01), loss='mean_square', learning_rate=0.05)\n",
    "\n",
    "# Training\n",
    "model = tf.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(train_data, train_labels, show_metric=True, n_epoch=2000, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preds_DNN = model.predict(test_data)\n",
    "preds_DNN = np.exp(preds_DNN)\n",
    "preds_DNN = preds_DNN.reshape(-1,)\n",
    "\n",
    "print(ids)\n",
    "print(preds_DNN)\n",
    "\n",
    "\n",
    "output = pd.DataFrame({\"id\":ids, \"SalePrice\": preds_DNN})\n",
    "output.to_csv('output2.csv', index=False)\n",
    "preds_DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: TC3G76\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name StandardError/ (raw) is illegal; using StandardError/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1167\n",
      "Validation samples: 292\n",
      "--\n",
      "Training Step: 1  | time: 1.068s\n",
      "| SGD | epoch: 001 | loss: 0.00000 - R2: 0.0000 | val_loss: 39477420032.00000 - val_acc: 0.0000 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m34938777600.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 002 | loss: 34938777600.00000 - R2: 0.0000 | val_loss: 32734269440.00000 - val_acc: 0.0090 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m37729742848.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 003 | loss: 37729742848.00000 - R2: 0.0000 | val_loss: 15640097792.00000 - val_acc: 0.1402 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m34157819904.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 004 | loss: 34157819904.00000 - R2: 0.0068 | val_loss: 258702016.00000 - val_acc: 0.8456 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m21024911360.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 005 | loss: 21024911360.00000 - R2: 0.0992 | val_loss: 24531474432.00000 - val_acc: 3.1808 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m8114358272.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 006 | loss: 8114358272.00000 - R2: 0.5850 | val_loss: 97714680.00000 - val_acc: 0.9036 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m17562628096.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 007 | loss: 17562628096.00000 - R2: 2.1435 | val_loss: 12255255552.00000 - val_acc: 0.1991 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m14346890240.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 008 | loss: 14346890240.00000 - R2: 1.4464 | val_loss: 240448832.00000 - val_acc: 0.8510 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m13050859520.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 009 | loss: 13050859520.00000 - R2: 0.7861 | val_loss: 15125179392.00000 - val_acc: 2.6089 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m6641595392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 010 | loss: 6641595392.00000 - R2: 0.8188 | val_loss: 107136616.00000 - val_acc: 0.8992 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m14564474880.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 011 | loss: 14564474880.00000 - R2: 1.6541 | val_loss: 13443085312.00000 - val_acc: 2.4963 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m8056994816.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 012 | loss: 8056994816.00000 - R2: 1.3147 | val_loss: 31047884.00000 - val_acc: 0.9453 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m10210438144.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 013 | loss: 10210438144.00000 - R2: 1.8218 | val_loss: 12009568256.00000 - val_acc: 2.3969 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m6496807936.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 014 | loss: 6496807936.00000 - R2: 1.4658 | val_loss: 1550880.87500 - val_acc: 0.9887 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m8528172032.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 015 | loss: 8528172032.00000 - R2: 1.8307 | val_loss: 6184642560.00000 - val_acc: 0.3680 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m5818187776.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 016 | loss: 5818187776.00000 - R2: 1.5219 | val_loss: 27065454.00000 - val_acc: 0.9489 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m5884288512.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 017 | loss: 5884288512.00000 - R2: 1.1066 | val_loss: 7775728128.00000 - val_acc: 2.0767 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m4297079808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 018 | loss: 4297079808.00000 - R2: 1.0464 | val_loss: 2502329.75000 - val_acc: 0.9851 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m5388040192.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 019 | loss: 5388040192.00000 - R2: 1.3903 | val_loss: 4572797952.00000 - val_acc: 0.4379 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m4034383360.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 020 | loss: 4034383360.00000 - R2: 1.2686 | val_loss: 23851002.00000 - val_acc: 0.9520 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m4159174656.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 021 | loss: 4159174656.00000 - R2: 1.0109 | val_loss: 4240148224.00000 - val_acc: 0.4547 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m4227847168.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 022 | loss: 4227847168.00000 - R2: 0.9911 | val_loss: 62150024.00000 - val_acc: 0.9229 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m4194548480.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 023 | loss: 4194548480.00000 - R2: 0.8354 | val_loss: 3613390592.00000 - val_acc: 1.6918 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m3282436608.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 024 | loss: 3282436608.00000 - R2: 0.8556 | val_loss: 26283804.00000 - val_acc: 0.9497 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m3347673088.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 025 | loss: 3347673088.00000 - R2: 1.0840 | val_loss: 3236234496.00000 - val_acc: 1.6501 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m2803999488.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 026 | loss: 2803999488.00000 - R2: 1.0474 | val_loss: 6889520.00000 - val_acc: 0.9746 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m2894194944.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 027 | loss: 2894194944.00000 - R2: 1.2027 | val_loss: 2884265472.00000 - val_acc: 1.6094 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m2483720192.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 028 | loss: 2483720192.00000 - R2: 1.1426 | val_loss: 628634.37500 - val_acc: 0.9954 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m2563919360.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 029 | loss: 2563919360.00000 - R2: 1.2565 | val_loss: 1971657728.00000 - val_acc: 0.6051 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m2936081920.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 030 | loss: 2936081920.00000 - R2: 1.2028 | val_loss: 7128603.00000 - val_acc: 0.9743 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m2699359744.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 031 | loss: 2699359744.00000 - R2: 1.0650 | val_loss: 1819655936.00000 - val_acc: 0.6188 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m2378310400.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 032 | loss: 2378310400.00000 - R2: 1.0476 | val_loss: 20840242.00000 - val_acc: 0.9553 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m2243191808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 033 | loss: 2243191808.00000 - R2: 0.9536 | val_loss: 665327488.00000 - val_acc: 0.7588 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m2332280320.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 034 | loss: 2332280320.00000 - R2: 0.9490 | val_loss: 125439128.00000 - val_acc: 1.1145 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m1978514688.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 035 | loss: 1978514688.00000 - R2: 0.9095 | val_loss: 587334208.00000 - val_acc: 0.7725 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m2025554048.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 036 | loss: 2025554048.00000 - R2: 0.9581 | val_loss: 90754288.00000 - val_acc: 1.0968 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m1733779840.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 037 | loss: 1733779840.00000 - R2: 0.9212 | val_loss: 520073280.00000 - val_acc: 0.7851 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m1540694016.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 038 | loss: 1540694016.00000 - R2: 0.9529 | val_loss: 64186228.00000 - val_acc: 1.0809 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m1341737984.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 039 | loss: 1341737984.00000 - R2: 0.9210 | val_loss: 461920096.00000 - val_acc: 0.7968 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m1233602944.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 040 | loss: 1233602944.00000 - R2: 0.9501 | val_loss: 44130392.00000 - val_acc: 1.0665 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m1088857344.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 041 | loss: 1088857344.00000 - R2: 0.9222 | val_loss: 411736224.00000 - val_acc: 0.8076 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m1130955776.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 042 | loss: 1130955776.00000 - R2: 0.9494 | val_loss: 29233030.00000 - val_acc: 1.0535 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m1001451392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 043 | loss: 1001451392.00000 - R2: 0.9246 | val_loss: 368313344.00000 - val_acc: 0.8175 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m1303673216.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 044 | loss: 1303673216.00000 - R2: 0.9473 | val_loss: 18452940.00000 - val_acc: 1.0418 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m1142610816.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 045 | loss: 1142610816.00000 - R2: 0.9255 | val_loss: 330686176.00000 - val_acc: 0.8267 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m1289398784.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 046 | loss: 1289398784.00000 - R2: 0.9448 | val_loss: 10931672.00000 - val_acc: 1.0312 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m1130590080.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 047 | loss: 1130590080.00000 - R2: 0.9257 | val_loss: 298024608.00000 - val_acc: 0.8351 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m1080328960.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 048 | loss: 1080328960.00000 - R2: 0.9437 | val_loss: 5962841.50000 - val_acc: 1.0216 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m955131392.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 049 | loss: 955131392.00000 - R2: 0.9267 | val_loss: 269628416.00000 - val_acc: 0.8428 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m957306432.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 050 | loss: 957306432.00000 - R2: 0.9403 | val_loss: 2995858.50000 - val_acc: 1.0130 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m850944384.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 051 | loss: 850944384.00000 - R2: 0.9256 | val_loss: 244858480.00000 - val_acc: 0.8499 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m1215249024.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 052 | loss: 1215249024.00000 - R2: 0.9378 | val_loss: 1577775.12500 - val_acc: 1.0051 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m1070796224.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 053 | loss: 1070796224.00000 - R2: 0.9251 | val_loss: 223202640.00000 - val_acc: 0.8565 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m1035082432.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 054 | loss: 1035082432.00000 - R2: 0.9362 | val_loss: 1339113.37500 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m917973888.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 055 | loss: 917973888.00000 - R2: 0.9250 | val_loss: 204351552.00000 - val_acc: 0.8625 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m1341322752.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 056 | loss: 1341322752.00000 - R2: 0.9396 | val_loss: 2012980.62500 - val_acc: 0.9916 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m1182903296.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 057 | loss: 1182903296.00000 - R2: 0.9291 | val_loss: 187669904.00000 - val_acc: 0.8681 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m1221883392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 058 | loss: 1221883392.00000 - R2: 0.9398 | val_loss: 3312559.75000 - val_acc: 0.9858 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m1082079872.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 059 | loss: 1082079872.00000 - R2: 0.9303 | val_loss: 173194368.00000 - val_acc: 0.8731 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m1056884544.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 060 | loss: 1056884544.00000 - R2: 0.9395 | val_loss: 5115932.50000 - val_acc: 0.9805 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m940838720.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 061 | loss: 940838720.00000 - R2: 0.9310 | val_loss: 21996828.00000 - val_acc: 0.9548 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m895665088.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 062 | loss: 895665088.00000 - R2: 0.9379 | val_loss: 35714708.00000 - val_acc: 1.0597 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m784946560.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 063 | loss: 784946560.00000 - R2: 0.9402 | val_loss: 17975944.00000 - val_acc: 0.9593 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m859172800.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 064 | loss: 859172800.00000 - R2: 0.9543 | val_loss: 30532542.00000 - val_acc: 1.0550 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m755518784.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 065 | loss: 755518784.00000 - R2: 0.9551 | val_loss: 14678261.00000 - val_acc: 0.9634 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m818253696.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 066 | loss: 818253696.00000 - R2: 0.9681 | val_loss: 26175682.00000 - val_acc: 1.0507 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m721901824.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 067 | loss: 721901824.00000 - R2: 0.9677 | val_loss: 11977644.00000 - val_acc: 0.9672 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m2120607488.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 068 | loss: 2120607488.00000 - R2: 0.9777 | val_loss: 22516018.00000 - val_acc: 1.0469 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m1874239744.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 069 | loss: 1874239744.00000 - R2: 0.9766 | val_loss: 9772761.00000 - val_acc: 0.9707 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m2028326144.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 070 | loss: 2028326144.00000 - R2: 0.9815 | val_loss: 19425126.00000 - val_acc: 1.0433 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m1798473984.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 071 | loss: 1798473984.00000 - R2: 0.9804 | val_loss: 7971934.50000 - val_acc: 0.9739 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m1655499648.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 072 | loss: 1655499648.00000 - R2: 0.9861 | val_loss: 16814058.00000 - val_acc: 1.0401 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m1472561408.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 073 | loss: 1472561408.00000 - R2: 0.9849 | val_loss: 6514318.50000 - val_acc: 0.9767 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m1722970752.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 074 | loss: 1722970752.00000 - R2: 0.9957 | val_loss: 14608786.00000 - val_acc: 1.0372 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m1536978560.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 075 | loss: 1536978560.00000 - R2: 0.9938 | val_loss: 5333784.00000 - val_acc: 0.9794 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m1454489600.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 076 | loss: 1454489600.00000 - R2: 0.9998 | val_loss: 12735890.00000 - val_acc: 1.0345 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m1301185920.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 077 | loss: 1301185920.00000 - R2: 0.9978 | val_loss: 4368631.00000 - val_acc: 0.9818 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m1242369664.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 078 | loss: 1242369664.00000 - R2: 1.0001 | val_loss: 11155166.00000 - val_acc: 1.0321 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m1114441216.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 079 | loss: 1114441216.00000 - R2: 0.9983 | val_loss: 3602595.50000 - val_acc: 0.9840 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m1094381312.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 080 | loss: 1094381312.00000 - R2: 1.0041 | val_loss: 9800710.00000 - val_acc: 1.0298 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m984221888.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 081 | loss: 984221888.00000 - R2: 1.0022 | val_loss: 2989498.75000 - val_acc: 0.9860 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m944039552.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 082 | loss: 944039552.00000 - R2: 1.0032 | val_loss: 8644911.00000 - val_acc: 1.0278 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m850082368.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 083 | loss: 850082368.00000 - R2: 1.0016 | val_loss: 2504938.00000 - val_acc: 0.9878 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m839028480.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 084 | loss: 839028480.00000 - R2: 1.0052 | val_loss: 7654705.00000 - val_acc: 1.0259 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m755528832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 085 | loss: 755528832.00000 - R2: 1.0036 | val_loss: 2123632.75000 - val_acc: 0.9895 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m1041863168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 086 | loss: 1041863168.00000 - R2: 1.0065 | val_loss: 6806317.50000 - val_acc: 1.0242 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m938046336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 087 | loss: 938046336.00000 - R2: 1.0049 | val_loss: 1824062.62500 - val_acc: 0.9910 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m1202511872.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 088 | loss: 1202511872.00000 - R2: 1.0073 | val_loss: 6069472.50000 - val_acc: 1.0226 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m1082603392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 089 | loss: 1082603392.00000 - R2: 1.0058 | val_loss: 1592252.00000 - val_acc: 0.9923 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m1067405824.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 090 | loss: 1067405824.00000 - R2: 1.0087 | val_loss: 5426182.50000 - val_acc: 1.0212 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m960987392.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 091 | loss: 960987392.00000 - R2: 1.0072 | val_loss: 1414969.00000 - val_acc: 0.9935 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m949384896.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 092 | loss: 949384896.00000 - R2: 1.0094 | val_loss: 4858002.00000 - val_acc: 1.0198 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m854752704.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 093 | loss: 854752704.00000 - R2: 1.0080 | val_loss: 1280497.50000 - val_acc: 0.9946 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m1099539712.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 094 | loss: 1099539712.00000 - R2: 1.0086 | val_loss: 4347022.00000 - val_acc: 1.0184 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m989879616.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 095 | loss: 989879616.00000 - R2: 1.0074 | val_loss: 1182303.62500 - val_acc: 0.9955 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m1044475840.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 096 | loss: 1044475840.00000 - R2: 1.0090 | val_loss: 3874703.75000 - val_acc: 1.0171 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m940312832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 097 | loss: 940312832.00000 - R2: 1.0078 | val_loss: 1112101.50000 - val_acc: 0.9962 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m1153830400.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 098 | loss: 1153830400.00000 - R2: 1.0132 | val_loss: 3424711.50000 - val_acc: 1.0158 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m1038724608.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 099 | loss: 1038724608.00000 - R2: 1.0116 | val_loss: 1063096.75000 - val_acc: 0.9967 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m1017087488.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 100 | loss: 1017087488.00000 - R2: 1.0121 | val_loss: 2990728.00000 - val_acc: 1.0144 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m915649664.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 101 | loss: 915649664.00000 - R2: 1.0107 | val_loss: 1031450.06250 - val_acc: 0.9970 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m1029801472.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 102 | loss: 1029801472.00000 - R2: 1.0106 | val_loss: 2590995.25000 - val_acc: 1.0129 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m927086976.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 103 | loss: 927086976.00000 - R2: 1.0093 | val_loss: 1010588.37500 - val_acc: 0.9971 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m1182656256.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 104 | loss: 1182656256.00000 - R2: 1.0071 | val_loss: 2247499.75000 - val_acc: 1.0116 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m1064651520.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 105 | loss: 1064651520.00000 - R2: 1.0062 | val_loss: 990711.43750 - val_acc: 0.9971 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m1034234240.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 106 | loss: 1034234240.00000 - R2: 1.0070 | val_loss: 1962226.87500 - val_acc: 1.0103 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m931066560.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 107 | loss: 931066560.00000 - R2: 1.0061 | val_loss: 976532.81250 - val_acc: 0.9972 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m944687744.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 108 | loss: 944687744.00000 - R2: 1.0073 | val_loss: 1727946.37500 - val_acc: 1.0092 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m850470592.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 109 | loss: 850470592.00000 - R2: 1.0064 | val_loss: 964007.00000 - val_acc: 0.9972 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m846736064.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 110 | loss: 846736064.00000 - R2: 1.0069 | val_loss: 1537504.75000 - val_acc: 1.0082 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m762310464.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 111 | loss: 762310464.00000 - R2: 1.0060 | val_loss: 952651.81250 - val_acc: 0.9972 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m761713152.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 112 | loss: 761713152.00000 - R2: 1.0058 | val_loss: 1382853.87500 - val_acc: 1.0072 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m685786624.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 113 | loss: 685786624.00000 - R2: 1.0051 | val_loss: 942098.62500 - val_acc: 0.9972 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m1049448576.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 114 | loss: 1049448576.00000 - R2: 1.0020 | val_loss: 1257458.00000 - val_acc: 1.0063 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m944745472.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 115 | loss: 944745472.00000 - R2: 1.0016 | val_loss: 932771.06250 - val_acc: 0.9972 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m1213910656.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 116 | loss: 1213910656.00000 - R2: 1.0026 | val_loss: 1156260.87500 - val_acc: 1.0055 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m1092758656.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 117 | loss: 1092758656.00000 - R2: 1.0022 | val_loss: 923805.81250 - val_acc: 0.9972 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m1042386304.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 118 | loss: 1042386304.00000 - R2: 1.0023 | val_loss: 1074511.12500 - val_acc: 1.0048 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m938384256.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 119 | loss: 938384256.00000 - R2: 1.0019 | val_loss: 916210.31250 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m1043747648.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 120 | loss: 1043747648.00000 - R2: 1.0008 | val_loss: 1009330.87500 - val_acc: 1.0042 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m939607296.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 121 | loss: 939607296.00000 - R2: 1.0006 | val_loss: 909380.81250 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m943162624.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 122 | loss: 943162624.00000 - R2: 1.0025 | val_loss: 957433.31250 - val_acc: 1.0036 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m849078848.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 123 | loss: 849078848.00000 - R2: 1.0021 | val_loss: 903176.25000 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m1046482624.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 124 | loss: 1046482624.00000 - R2: 1.0019 | val_loss: 916352.68750 - val_acc: 1.0030 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m942065088.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 125 | loss: 942065088.00000 - R2: 1.0015 | val_loss: 895693.37500 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m943467520.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 126 | loss: 943467520.00000 - R2: 1.0010 | val_loss: 883341.37500 - val_acc: 1.0025 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m849349504.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 127 | loss: 849349504.00000 - R2: 1.0008 | val_loss: 890541.25000 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m908835968.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 128 | loss: 908835968.00000 - R2: 1.0006 | val_loss: 857875.31250 - val_acc: 1.0021 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m818179584.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 129 | loss: 818179584.00000 - R2: 1.0004 | val_loss: 886012.43750 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m981761920.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 130 | loss: 981761920.00000 - R2: 1.0021 | val_loss: 838480.12500 - val_acc: 1.0017 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m883811584.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 131 | loss: 883811584.00000 - R2: 1.0017 | val_loss: 881797.06250 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m895179648.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 132 | loss: 895179648.00000 - R2: 1.0030 | val_loss: 823682.87500 - val_acc: 1.0013 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m805886336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 133 | loss: 805886336.00000 - R2: 1.0025 | val_loss: 877687.81250 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m833214272.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 134 | loss: 833214272.00000 - R2: 1.0016 | val_loss: 812507.62500 - val_acc: 1.0009 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m750116352.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 135 | loss: 750116352.00000 - R2: 1.0013 | val_loss: 874010.50000 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m930482688.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 136 | loss: 930482688.00000 - R2: 1.0048 | val_loss: 804405.31250 - val_acc: 1.0006 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m837656832.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 137 | loss: 837656832.00000 - R2: 1.0042 | val_loss: 870444.37500 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m822631424.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 138 | loss: 822631424.00000 - R2: 1.0028 | val_loss: 798619.87500 - val_acc: 1.0003 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m740589696.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 139 | loss: 740589696.00000 - R2: 1.0024 | val_loss: 867632.25000 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m1015041024.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 140 | loss: 1015041024.00000 - R2: 1.0025 | val_loss: 795108.00000 - val_acc: 1.0001 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m913757568.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 141 | loss: 913757568.00000 - R2: 1.0021 | val_loss: 864937.18750 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m1048485120.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 142 | loss: 1048485120.00000 - R2: 1.0033 | val_loss: 792934.00000 - val_acc: 0.9999 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m943856448.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 143 | loss: 943856448.00000 - R2: 1.0028 | val_loss: 862582.43750 - val_acc: 0.9973 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m1143126784.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 144 | loss: 1143126784.00000 - R2: 1.0031 | val_loss: 792078.62500 - val_acc: 0.9996 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m1029033280.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 145 | loss: 1029033280.00000 - R2: 1.0026 | val_loss: 860442.81250 - val_acc: 0.9974 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m1241666560.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 146 | loss: 1241666560.00000 - R2: 1.0017 | val_loss: 792089.25000 - val_acc: 0.9994 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m1117718528.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 147 | loss: 1117718528.00000 - R2: 1.0014 | val_loss: 858329.56250 - val_acc: 0.9974 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m1077402880.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 148 | loss: 1077402880.00000 - R2: 1.0005 | val_loss: 792646.31250 - val_acc: 0.9993 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m969880576.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 149 | loss: 969880576.00000 - R2: 1.0003 | val_loss: 856502.00000 - val_acc: 0.9974 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m977112704.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 150 | loss: 977112704.00000 - R2: 1.0003 | val_loss: 793820.37500 - val_acc: 0.9991 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m879618880.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 151 | loss: 879618880.00000 - R2: 1.0001 | val_loss: 854630.18750 - val_acc: 0.9974 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m916491328.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 152 | loss: 916491328.00000 - R2: 1.0005 | val_loss: 795153.25000 - val_acc: 0.9989 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m825059136.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 153 | loss: 825059136.00000 - R2: 1.0003 | val_loss: 797611.12500 - val_acc: 0.9987 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m742767552.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 154 | loss: 742767552.00000 - R2: 1.0003 | val_loss: 780808.25000 - val_acc: 0.9993 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m668703808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 155 | loss: 668703808.00000 - R2: 1.0002 | val_loss: 822004.81250 - val_acc: 0.9978 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m913355072.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 156 | loss: 913355072.00000 - R2: 1.0006 | val_loss: 782145.56250 - val_acc: 0.9991 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m822232320.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 157 | loss: 822232320.00000 - R2: 1.0004 | val_loss: 822893.81250 - val_acc: 0.9978 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m1104371072.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 158 | loss: 1104371072.00000 - R2: 0.9984 | val_loss: 783854.06250 - val_acc: 0.9989 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m994146368.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 159 | loss: 994146368.00000 - R2: 0.9984 | val_loss: 823334.37500 - val_acc: 0.9977 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m1869145088.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 160 | loss: 1869145088.00000 - R2: 0.9993 | val_loss: 785699.50000 - val_acc: 0.9988 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m1682442624.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 161 | loss: 1682442624.00000 - R2: 0.9993 | val_loss: 823526.00000 - val_acc: 0.9977 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m2218525440.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 162 | loss: 2218525440.00000 - R2: 0.9968 | val_loss: 787618.62500 - val_acc: 0.9987 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m1996884608.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 163 | loss: 1996884608.00000 - R2: 0.9970 | val_loss: 823575.93750 - val_acc: 0.9977 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m2099261184.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 164 | loss: 2099261184.00000 - R2: 0.9974 | val_loss: 789575.43750 - val_acc: 0.9986 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m1889546496.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 165 | loss: 1889546496.00000 - R2: 0.9976 | val_loss: 823481.31250 - val_acc: 0.9977 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m2034677376.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 166 | loss: 2034677376.00000 - R2: 0.9978 | val_loss: 791481.87500 - val_acc: 0.9985 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m1831420800.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 167 | loss: 1831420800.00000 - R2: 0.9979 | val_loss: 823388.87500 - val_acc: 0.9977 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m1778986880.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 168 | loss: 1778986880.00000 - R2: 0.9983 | val_loss: 793410.93750 - val_acc: 0.9984 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m1601299200.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 169 | loss: 1601299200.00000 - R2: 0.9984 | val_loss: 823285.56250 - val_acc: 0.9976 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m1772237312.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 170 | loss: 1772237312.00000 - R2: 0.9961 | val_loss: 795291.81250 - val_acc: 0.9983 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m1595224320.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 171 | loss: 1595224320.00000 - R2: 0.9964 | val_loss: 823108.06250 - val_acc: 0.9976 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m1634268544.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 172 | loss: 1634268544.00000 - R2: 0.9987 | val_loss: 797061.93750 - val_acc: 0.9982 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m1471052288.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 173 | loss: 1471052288.00000 - R2: 0.9987 | val_loss: 779051.56250 - val_acc: 0.9989 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m1423154432.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 174 | loss: 1423154432.00000 - R2: 0.9960 | val_loss: 777279.43750 - val_acc: 0.9989 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m1281047808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 175 | loss: 1281047808.00000 - R2: 0.9964 | val_loss: 793376.75000 - val_acc: 0.9983 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m1273648384.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 176 | loss: 1273648384.00000 - R2: 0.9979 | val_loss: 779706.50000 - val_acc: 0.9987 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m1146492160.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 177 | loss: 1146492160.00000 - R2: 0.9981 | val_loss: 795618.87500 - val_acc: 0.9982 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m1144173696.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 178 | loss: 1144173696.00000 - R2: 0.9994 | val_loss: 781778.93750 - val_acc: 0.9986 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m1029964800.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 179 | loss: 1029964800.00000 - R2: 0.9994 | val_loss: 797101.06250 - val_acc: 0.9981 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m1245631744.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 180 | loss: 1245631744.00000 - R2: 0.9980 | val_loss: 783623.68750 - val_acc: 0.9985 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m1121276928.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 181 | loss: 1121276928.00000 - R2: 0.9981 | val_loss: 798197.75000 - val_acc: 0.9981 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m1275805184.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 182 | loss: 1275805184.00000 - R2: 0.9968 | val_loss: 785289.12500 - val_acc: 0.9985 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m1148432896.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 183 | loss: 1148432896.00000 - R2: 0.9970 | val_loss: 798993.00000 - val_acc: 0.9981 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m1095639424.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 184 | loss: 1095639424.00000 - R2: 0.9979 | val_loss: 786771.50000 - val_acc: 0.9984 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m986283520.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 185 | loss: 986283520.00000 - R2: 0.9981 | val_loss: 799606.81250 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m957775424.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 186 | loss: 957775424.00000 - R2: 0.9973 | val_loss: 788119.43750 - val_acc: 0.9983 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m862205824.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 187 | loss: 862205824.00000 - R2: 0.9975 | val_loss: 800089.87500 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m837626112.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 188 | loss: 837626112.00000 - R2: 0.9967 | val_loss: 789356.43750 - val_acc: 0.9983 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m754071296.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 189 | loss: 754071296.00000 - R2: 0.9970 | val_loss: 800524.50000 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m897479040.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 190 | loss: 897479040.00000 - R2: 0.9987 | val_loss: 790524.68750 - val_acc: 0.9982 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m807938880.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 191 | loss: 807938880.00000 - R2: 0.9988 | val_loss: 800855.18750 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m795864640.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 192 | loss: 795864640.00000 - R2: 0.9993 | val_loss: 791574.00000 - val_acc: 0.9982 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m716485824.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 193 | loss: 716485824.00000 - R2: 0.9993 | val_loss: 801154.62500 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m1879243264.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 194 | loss: 1879243264.00000 - R2: 0.9993 | val_loss: 792553.18750 - val_acc: 0.9982 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m1691526528.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 195 | loss: 1691526528.00000 - R2: 0.9993 | val_loss: 801390.87500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m1631520768.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 196 | loss: 1631520768.00000 - R2: 0.9984 | val_loss: 793442.75000 - val_acc: 0.9981 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m1468576256.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 197 | loss: 1468576256.00000 - R2: 0.9985 | val_loss: 801595.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m1399998080.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 198 | loss: 1399998080.00000 - R2: 1.0005 | val_loss: 794264.56250 - val_acc: 0.9981 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m1260205696.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 199 | loss: 1260205696.00000 - R2: 1.0003 | val_loss: 801736.87500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m1190666112.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 200 | loss: 1190666112.00000 - R2: 0.9998 | val_loss: 794986.00000 - val_acc: 0.9981 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m1071806848.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 201 | loss: 1071806848.00000 - R2: 0.9997 | val_loss: 801898.06250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m1265308416.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 202 | loss: 1265308416.00000 - R2: 1.0007 | val_loss: 795672.81250 - val_acc: 0.9981 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m1138984960.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 203 | loss: 1138984960.00000 - R2: 1.0005 | val_loss: 802018.87500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m1137515008.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 204 | loss: 1137515008.00000 - R2: 1.0010 | val_loss: 796312.87500 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m1023970816.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 205 | loss: 1023970816.00000 - R2: 1.0008 | val_loss: 802132.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m1238898048.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 206 | loss: 1238898048.00000 - R2: 0.9994 | val_loss: 796880.87500 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m1115215488.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 207 | loss: 1115215488.00000 - R2: 0.9994 | val_loss: 802227.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m1066963520.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 208 | loss: 1066963520.00000 - R2: 1.0016 | val_loss: 797410.93750 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m960474368.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 209 | loss: 960474368.00000 - R2: 1.0014 | val_loss: 802210.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m925316672.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 210 | loss: 925316672.00000 - R2: 1.0004 | val_loss: 797798.50000 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m832992192.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 211 | loss: 832992192.00000 - R2: 1.0003 | val_loss: 802279.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m809940096.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 212 | loss: 809940096.00000 - R2: 0.9999 | val_loss: 798231.81250 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m729153216.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 213 | loss: 729153216.00000 - R2: 0.9998 | val_loss: 802335.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m730749376.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 214 | loss: 730749376.00000 - R2: 0.9996 | val_loss: 798620.68750 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m657881536.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 215 | loss: 657881536.00000 - R2: 0.9995 | val_loss: 802380.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m722939712.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 216 | loss: 722939712.00000 - R2: 0.9979 | val_loss: 798984.81250 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m650852800.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 217 | loss: 650852800.00000 - R2: 0.9980 | val_loss: 802428.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m935640960.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 218 | loss: 935640960.00000 - R2: 0.9977 | val_loss: 799321.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m842283904.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 219 | loss: 842283904.00000 - R2: 0.9978 | val_loss: 802480.50000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m844832512.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 220 | loss: 844832512.00000 - R2: 0.9989 | val_loss: 799631.43750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m760556288.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 221 | loss: 760556288.00000 - R2: 0.9989 | val_loss: 802508.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m928897408.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 222 | loss: 928897408.00000 - R2: 0.9991 | val_loss: 799918.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m836214656.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 223 | loss: 836214656.00000 - R2: 0.9991 | val_loss: 802547.31250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m1065354752.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 224 | loss: 1065354752.00000 - R2: 0.9977 | val_loss: 800169.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m959026240.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 225 | loss: 959026240.00000 - R2: 0.9978 | val_loss: 802580.25000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m1236370176.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 226 | loss: 1236370176.00000 - R2: 0.9984 | val_loss: 800404.93750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m1112940160.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 227 | loss: 1112940160.00000 - R2: 0.9985 | val_loss: 802617.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m1243947648.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 228 | loss: 1243947648.00000 - R2: 0.9992 | val_loss: 800624.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m1119759872.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 229 | loss: 1119759872.00000 - R2: 0.9991 | val_loss: 802639.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m1308069888.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 230 | loss: 1308069888.00000 - R2: 0.9977 | val_loss: 800815.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m1177469824.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 231 | loss: 1177469824.00000 - R2: 0.9978 | val_loss: 802659.50000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m1275552896.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 232 | loss: 1275552896.00000 - R2: 0.9970 | val_loss: 800998.43750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m1148204544.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 233 | loss: 1148204544.00000 - R2: 0.9972 | val_loss: 802683.56250 - val_acc: 0.9978 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m1078210944.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 234 | loss: 1078210944.00000 - R2: 0.9979 | val_loss: 801163.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m970596800.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 235 | loss: 970596800.00000 - R2: 0.9980 | val_loss: 799656.93750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m945591744.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 236 | loss: 945591744.00000 - R2: 0.9962 | val_loss: 798338.75000 - val_acc: 0.9980 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m851239360.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 237 | loss: 851239360.00000 - R2: 0.9965 | val_loss: 799695.43750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m1019943168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 238 | loss: 1019943168.00000 - R2: 0.9965 | val_loss: 798475.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m918155584.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 239 | loss: 918155584.00000 - R2: 0.9968 | val_loss: 799715.50000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m1146855808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 240 | loss: 1146855808.00000 - R2: 0.9960 | val_loss: 798602.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m1032376960.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 241 | loss: 1032376960.00000 - R2: 0.9963 | val_loss: 799735.93750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m1053678656.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 242 | loss: 1053678656.00000 - R2: 0.9951 | val_loss: 798716.25000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m948517504.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 243 | loss: 948517504.00000 - R2: 0.9955 | val_loss: 799757.62500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m1143561728.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 244 | loss: 1143561728.00000 - R2: 0.9984 | val_loss: 798837.43750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m1029412288.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 245 | loss: 1029412288.00000 - R2: 0.9985 | val_loss: 799771.93750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m1300800256.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 246 | loss: 1300800256.00000 - R2: 0.9977 | val_loss: 798930.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m1170926976.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 247 | loss: 1170926976.00000 - R2: 0.9978 | val_loss: 799787.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m1428487168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 248 | loss: 1428487168.00000 - R2: 0.9967 | val_loss: 799016.31250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m1285845120.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 249 | loss: 1285845120.00000 - R2: 0.9969 | val_loss: 799797.43750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m1206458112.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 250 | loss: 1206458112.00000 - R2: 0.9969 | val_loss: 799097.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m1086019072.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 251 | loss: 1086019072.00000 - R2: 0.9971 | val_loss: 799823.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m1060607104.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 252 | loss: 1060607104.00000 - R2: 0.9972 | val_loss: 799175.31250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m954753088.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 253 | loss: 954753088.00000 - R2: 0.9974 | val_loss: 799830.62500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m944238016.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 254 | loss: 944238016.00000 - R2: 0.9973 | val_loss: 799239.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m850020928.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 255 | loss: 850020928.00000 - R2: 0.9975 | val_loss: 798653.50000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m841196608.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 256 | loss: 841196608.00000 - R2: 0.9953 | val_loss: 798128.25000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m757283584.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 257 | loss: 757283584.00000 - R2: 0.9957 | val_loss: 798669.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m821887936.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 258 | loss: 821887936.00000 - R2: 0.9963 | val_loss: 798185.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m739905792.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 259 | loss: 739905792.00000 - R2: 0.9965 | val_loss: 798675.06250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m1114660864.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 260 | loss: 1114660864.00000 - R2: 0.9981 | val_loss: 798237.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m1003401408.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 261 | loss: 1003401408.00000 - R2: 0.9982 | val_loss: 798684.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m974782848.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 262 | loss: 974782848.00000 - R2: 0.9977 | val_loss: 798281.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m877511168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 263 | loss: 877511168.00000 - R2: 0.9978 | val_loss: 798691.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m942335104.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 264 | loss: 942335104.00000 - R2: 1.0013 | val_loss: 798333.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m848308224.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 265 | loss: 848308224.00000 - R2: 1.0010 | val_loss: 798707.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m1049522816.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 266 | loss: 1049522816.00000 - R2: 0.9984 | val_loss: 798370.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m944777152.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 267 | loss: 944777152.00000 - R2: 0.9985 | val_loss: 798707.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m1102560000.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 268 | loss: 1102560000.00000 - R2: 0.9966 | val_loss: 798409.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m992510656.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 269 | loss: 992510656.00000 - R2: 0.9969 | val_loss: 798710.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m1002747328.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 270 | loss: 1002747328.00000 - R2: 0.9981 | val_loss: 798438.87500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m902679232.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 271 | loss: 902679232.00000 - R2: 0.9982 | val_loss: 798726.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m933648832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 272 | loss: 933648832.00000 - R2: 0.9971 | val_loss: 798483.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m840490560.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 273 | loss: 840490560.00000 - R2: 0.9973 | val_loss: 798735.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m860853376.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 274 | loss: 860853376.00000 - R2: 0.9989 | val_loss: 798504.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m774974656.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 275 | loss: 774974656.00000 - R2: 0.9989 | val_loss: 798737.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m1024038528.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 276 | loss: 1024038528.00000 - R2: 0.9971 | val_loss: 798530.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m921841280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 277 | loss: 921841280.00000 - R2: 0.9973 | val_loss: 798742.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m1190092800.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 278 | loss: 1190092800.00000 - R2: 0.9975 | val_loss: 798561.62500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m1071290112.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 279 | loss: 1071290112.00000 - R2: 0.9977 | val_loss: 798746.43750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m1245674752.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 280 | loss: 1245674752.00000 - R2: 0.9988 | val_loss: 798583.62500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m1121313920.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 281 | loss: 1121313920.00000 - R2: 0.9988 | val_loss: 798753.87500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m1086669056.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 282 | loss: 1086669056.00000 - R2: 0.9989 | val_loss: 798600.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m978208768.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 283 | loss: 978208768.00000 - R2: 0.9989 | val_loss: 798759.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m965942208.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 284 | loss: 965942208.00000 - R2: 0.9971 | val_loss: 798615.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m869554624.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 285 | loss: 869554624.00000 - R2: 0.9973 | val_loss: 798766.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m884690240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 286 | loss: 884690240.00000 - R2: 0.9993 | val_loss: 798637.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m796427840.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 287 | loss: 796427840.00000 - R2: 0.9993 | val_loss: 798767.31250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m876892160.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 288 | loss: 876892160.00000 - R2: 1.0016 | val_loss: 798654.06250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m789409536.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 289 | loss: 789409536.00000 - R2: 1.0013 | val_loss: 798774.43750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m773650752.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 290 | loss: 773650752.00000 - R2: 1.0011 | val_loss: 798662.87500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m696492288.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 291 | loss: 696492288.00000 - R2: 1.0009 | val_loss: 798779.06250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m717986496.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 292 | loss: 717986496.00000 - R2: 1.0010 | val_loss: 798674.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m646394496.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 293 | loss: 646394496.00000 - R2: 1.0008 | val_loss: 798777.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m645407232.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 294 | loss: 645407232.00000 - R2: 1.0011 | val_loss: 798689.25000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m581073152.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 295 | loss: 581073152.00000 - R2: 1.0009 | val_loss: 798779.31250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m720798848.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 296 | loss: 720798848.00000 - R2: 0.9983 | val_loss: 798700.25000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m648925568.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 297 | loss: 648925568.00000 - R2: 0.9983 | val_loss: 798785.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m818733888.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 298 | loss: 818733888.00000 - R2: 0.9997 | val_loss: 798711.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m737067136.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 299 | loss: 737067136.00000 - R2: 0.9996 | val_loss: 798787.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m746747776.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 300 | loss: 746747776.00000 - R2: 1.0006 | val_loss: 798720.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m672279616.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 301 | loss: 672279616.00000 - R2: 1.0005 | val_loss: 798787.75000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m728474240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 302 | loss: 728474240.00000 - R2: 1.0001 | val_loss: 798724.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m655833408.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 303 | loss: 655833408.00000 - R2: 1.0000 | val_loss: 798787.43750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m883058816.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 304 | loss: 883058816.00000 - R2: 1.0010 | val_loss: 798733.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m794959552.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 305 | loss: 794959552.00000 - R2: 1.0008 | val_loss: 798785.50000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m993694592.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 306 | loss: 993694592.00000 - R2: 1.0007 | val_loss: 798742.50000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m894531712.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 307 | loss: 894531712.00000 - R2: 1.0006 | val_loss: 798789.25000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m1176382336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 308 | loss: 1176382336.00000 - R2: 0.9984 | val_loss: 798744.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m1058950720.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 309 | loss: 1058950720.00000 - R2: 0.9984 | val_loss: 798788.50000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m1308419968.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 310 | loss: 1308419968.00000 - R2: 0.9990 | val_loss: 798744.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m1177784576.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 311 | loss: 1177784576.00000 - R2: 0.9990 | val_loss: 798792.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m1404002688.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 312 | loss: 1404002688.00000 - R2: 1.0021 | val_loss: 798753.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m1263809024.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 313 | loss: 1263809024.00000 - R2: 1.0018 | val_loss: 798792.93750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m1184482816.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 314 | loss: 1184482816.00000 - R2: 1.0016 | val_loss: 798755.75000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m1066241152.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 315 | loss: 1066241152.00000 - R2: 1.0013 | val_loss: 798792.87500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m1048190336.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 316 | loss: 1048190336.00000 - R2: 1.0007 | val_loss: 798761.25000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m943577920.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 317 | loss: 943577920.00000 - R2: 1.0005 | val_loss: 798793.25000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m933177344.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 318 | loss: 933177344.00000 - R2: 1.0011 | val_loss: 798764.31250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m840066240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 319 | loss: 840066240.00000 - R2: 1.0009 | val_loss: 798790.93750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m1126380672.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 320 | loss: 1126380672.00000 - R2: 1.0012 | val_loss: 798763.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m1013949184.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 321 | loss: 1013949184.00000 - R2: 1.0010 | val_loss: 798790.43750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m1208843008.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 322 | loss: 1208843008.00000 - R2: 1.0039 | val_loss: 798770.43750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m1088165376.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 323 | loss: 1088165376.00000 - R2: 1.0034 | val_loss: 798796.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m1080067840.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 324 | loss: 1080067840.00000 - R2: 1.0026 | val_loss: 798770.62500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m972267648.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 325 | loss: 972267648.00000 - R2: 1.0022 | val_loss: 798795.93750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m983877952.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 326 | loss: 983877952.00000 - R2: 1.0038 | val_loss: 798771.06250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m885696768.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 327 | loss: 885696768.00000 - R2: 1.0034 | val_loss: 798795.50000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m860723712.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 328 | loss: 860723712.00000 - R2: 1.0036 | val_loss: 798772.06250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m774857984.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 329 | loss: 774857984.00000 - R2: 1.0031 | val_loss: 798795.87500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m775474432.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 330 | loss: 775474432.00000 - R2: 1.0015 | val_loss: 798773.37500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m698133632.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 331 | loss: 698133632.00000 - R2: 1.0013 | val_loss: 798796.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m732582336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 332 | loss: 732582336.00000 - R2: 0.9992 | val_loss: 798772.25000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m659530688.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 333 | loss: 659530688.00000 - R2: 0.9992 | val_loss: 798795.50000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m726819264.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 334 | loss: 726819264.00000 - R2: 0.9994 | val_loss: 798772.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m654343936.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 335 | loss: 654343936.00000 - R2: 0.9994 | val_loss: 798796.87500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m679378240.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 336 | loss: 679378240.00000 - R2: 1.0005 | val_loss: 798777.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m611647040.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 337 | loss: 611647040.00000 - R2: 1.0004 | val_loss: 798797.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m754078464.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 338 | loss: 754078464.00000 - R2: 0.9982 | val_loss: 798779.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m678877248.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 339 | loss: 678877248.00000 - R2: 0.9983 | val_loss: 798798.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m913443648.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 340 | loss: 913443648.00000 - R2: 0.9964 | val_loss: 798789.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m822305920.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 341 | loss: 822305920.00000 - R2: 0.9967 | val_loss: 798798.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m1133921536.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 342 | loss: 1133921536.00000 - R2: 0.9957 | val_loss: 798792.87500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m1020736000.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 343 | loss: 1020736000.00000 - R2: 0.9960 | val_loss: 798798.87500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m1023742592.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 344 | loss: 1023742592.00000 - R2: 0.9953 | val_loss: 798792.62500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m921574912.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 345 | loss: 921574912.00000 - R2: 0.9956 | val_loss: 798800.43750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m1154740992.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 346 | loss: 1154740992.00000 - R2: 0.9926 | val_loss: 798799.31250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m1039473472.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 347 | loss: 1039473472.00000 - R2: 0.9932 | val_loss: 798801.75000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m1250207360.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 348 | loss: 1250207360.00000 - R2: 0.9946 | val_loss: 798800.31250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m1125393280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 349 | loss: 1125393280.00000 - R2: 0.9951 | val_loss: 798801.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m1078293248.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 350 | loss: 1078293248.00000 - R2: 0.9950 | val_loss: 798802.56250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m970670528.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 351 | loss: 970670528.00000 - R2: 0.9954 | val_loss: 798803.68750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m953767936.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 352 | loss: 953767936.00000 - R2: 0.9952 | val_loss: 798802.06250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m858597760.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 353 | loss: 858597760.00000 - R2: 0.9956 | val_loss: 798802.06250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m1100791552.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 354 | loss: 1100791552.00000 - R2: 0.9967 | val_loss: 798802.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m990918976.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 355 | loss: 990918976.00000 - R2: 0.9970 | val_loss: 798802.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m1134707712.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 356 | loss: 1134707712.00000 - R2: 0.9982 | val_loss: 798801.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m1021443520.00000\u001b[0m\u001b[0m | time: 1.019s\n",
      "| SGD | epoch: 357 | loss: 1021443520.00000 - R2: 0.9982 | val_loss: 798802.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m1014456960.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 358 | loss: 1014456960.00000 - R2: 0.9977 | val_loss: 798801.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m913217856.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 359 | loss: 913217856.00000 - R2: 0.9979 | val_loss: 798801.31250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m936598784.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 360 | loss: 936598784.00000 - R2: 0.9992 | val_loss: 798801.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m843145536.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 361 | loss: 843145536.00000 - R2: 0.9992 | val_loss: 798801.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m840420544.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 362 | loss: 840420544.00000 - R2: 0.9989 | val_loss: 798801.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m756585088.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 363 | loss: 756585088.00000 - R2: 0.9989 | val_loss: 798801.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m954694976.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 364 | loss: 954694976.00000 - R2: 0.9993 | val_loss: 798801.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m859432064.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 365 | loss: 859432064.00000 - R2: 0.9993 | val_loss: 798801.12500 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m881752448.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 366 | loss: 881752448.00000 - R2: 0.9982 | val_loss: 798801.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m793783808.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 367 | loss: 793783808.00000 - R2: 0.9983 | val_loss: 798801.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m830678592.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 368 | loss: 830678592.00000 - R2: 1.0007 | val_loss: 798801.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m747817344.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 369 | loss: 747817344.00000 - R2: 1.0005 | val_loss: 798801.00000 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m767499648.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 370 | loss: 767499648.00000 - R2: 0.9991 | val_loss: 798800.93750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m690956288.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 371 | loss: 690956288.00000 - R2: 0.9991 | val_loss: 798800.93750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m978979200.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 372 | loss: 978979200.00000 - R2: 1.0019 | val_loss: 798800.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m881287872.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 373 | loss: 881287872.00000 - R2: 1.0016 | val_loss: 798800.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m853187648.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 374 | loss: 853187648.00000 - R2: 0.9997 | val_loss: 798800.31250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m768075520.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 375 | loss: 768075520.00000 - R2: 0.9996 | val_loss: 798800.31250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m1007638592.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 376 | loss: 1007638592.00000 - R2: 1.0016 | val_loss: 798800.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m907081344.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 377 | loss: 907081344.00000 - R2: 1.0013 | val_loss: 798800.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m940510144.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 378 | loss: 940510144.00000 - R2: 1.0002 | val_loss: 798800.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m846665728.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 379 | loss: 846665728.00000 - R2: 1.0000 | val_loss: 798800.18750 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m852090944.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 380 | loss: 852090944.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m767088448.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 381 | loss: 767088448.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m867762432.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 382 | loss: 867762432.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m781192832.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 383 | loss: 781192832.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m847156672.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 384 | loss: 847156672.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m762647616.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 385 | loss: 762647616.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m1084169472.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 386 | loss: 1084169472.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m975959104.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 387 | loss: 975959104.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m963853248.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 388 | loss: 963853248.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m867674496.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 389 | loss: 867674496.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m781113664.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 390 | loss: 781113664.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m703208896.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 391 | loss: 703208896.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m700382528.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 392 | loss: 700382528.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m630550912.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 393 | loss: 630550912.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m899548416.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 394 | loss: 899548416.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m809800192.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 395 | loss: 809800192.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m1086342144.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 396 | loss: 1086342144.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m977914560.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 397 | loss: 977914560.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m1220995584.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 398 | loss: 1220995584.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m1099102592.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 399 | loss: 1099102592.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m1357464960.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 400 | loss: 1357464960.00000 - R2: 1.0013 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m1221925120.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 401 | loss: 1221925120.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m1158031232.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 402 | loss: 1158031232.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m1042434688.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 403 | loss: 1042434688.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m2100652544.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 404 | loss: 2100652544.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m1890793856.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 405 | loss: 1890793856.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m1823066112.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 406 | loss: 1823066112.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m1640966144.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 407 | loss: 1640966144.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m1826944256.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 408 | loss: 1826944256.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m1644456448.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 409 | loss: 1644456448.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m1602264960.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 410 | loss: 1602264960.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m1442245120.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 411 | loss: 1442245120.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m1359985280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 412 | loss: 1359985280.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m1224193280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 413 | loss: 1224193280.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m1101980544.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 414 | loss: 1101980544.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m991989120.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 415 | loss: 991989120.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m1199852416.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 416 | loss: 1199852416.00000 - R2: 1.0025 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m1080073728.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 417 | loss: 1080073728.00000 - R2: 1.0022 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m1024569024.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 418 | loss: 1024569024.00000 - R2: 1.0024 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m922318720.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 419 | loss: 922318720.00000 - R2: 1.0021 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m1090946304.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 420 | loss: 1090946304.00000 - R2: 1.0019 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m982058304.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 421 | loss: 982058304.00000 - R2: 1.0016 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m1161485568.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 422 | loss: 1161485568.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m1045543616.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 423 | loss: 1045543616.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m1019899968.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 424 | loss: 1019899968.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m918116608.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 425 | loss: 918116608.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m871135168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 426 | loss: 871135168.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m784228288.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 427 | loss: 784228288.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m1084442880.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 428 | loss: 1084442880.00000 - R2: 0.9961 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m976205184.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 429 | loss: 976205184.00000 - R2: 0.9964 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m935026944.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 430 | loss: 935026944.00000 - R2: 0.9963 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m841730880.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 431 | loss: 841730880.00000 - R2: 0.9966 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m853677504.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 432 | loss: 853677504.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m768516352.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 433 | loss: 768516352.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m775906624.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 434 | loss: 775906624.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m698522560.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 435 | loss: 698522560.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m753067264.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 436 | loss: 753067264.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m677967168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 437 | loss: 677967168.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m765273344.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 438 | loss: 765273344.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m688952640.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 439 | loss: 688952640.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m724583040.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 440 | loss: 724583040.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m652331328.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 441 | loss: 652331328.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m888824448.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 442 | loss: 888824448.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m800148608.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 443 | loss: 800148608.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m788120768.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 444 | loss: 788120768.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m709515328.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 445 | loss: 709515328.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m977343296.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 446 | loss: 977343296.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m879815552.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 447 | loss: 879815552.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m944566848.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 448 | loss: 944566848.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m850316800.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 449 | loss: 850316800.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m877087872.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 450 | loss: 877087872.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m789585664.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 451 | loss: 789585664.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m818580224.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 452 | loss: 818580224.00000 - R2: 1.0016 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m736928832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 453 | loss: 736928832.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m841165376.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 454 | loss: 841165376.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m757255424.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 455 | loss: 757255424.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m786653376.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 456 | loss: 786653376.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m708194624.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 457 | loss: 708194624.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m733582272.00000\u001b[0m\u001b[0m | time: 1.029s\n",
      "| SGD | epoch: 458 | loss: 733582272.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m660430656.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 459 | loss: 660430656.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m697347072.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 460 | loss: 697347072.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m627819008.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 461 | loss: 627819008.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m781053568.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 462 | loss: 781053568.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m703154816.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 463 | loss: 703154816.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m872560064.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 464 | loss: 872560064.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m785510656.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 465 | loss: 785510656.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m774128256.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 466 | loss: 774128256.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m696922048.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 467 | loss: 696922048.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m695394944.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 468 | loss: 695394944.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m626062080.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 469 | loss: 626062080.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m638172672.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 470 | loss: 638172672.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m574562048.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 471 | loss: 574562048.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m563699136.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 472 | loss: 563699136.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m507535840.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 473 | loss: 507535840.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m533588640.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 474 | loss: 533588640.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m480436384.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 475 | loss: 480436384.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m492187296.00000\u001b[0m\u001b[0m | time: 1.089s\n",
      "| SGD | epoch: 476 | loss: 492187296.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m443175168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 477 | loss: 443175168.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m505519264.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 478 | loss: 505519264.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m455173952.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 479 | loss: 455173952.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m526074048.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 480 | loss: 526074048.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m473673248.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 481 | loss: 473673248.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m514058816.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 482 | loss: 514058816.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m462859552.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 483 | loss: 462859552.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m732557568.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 484 | loss: 732557568.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m659508416.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 485 | loss: 659508416.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m880442624.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 486 | loss: 880442624.00000 - R2: 1.0018 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m792604992.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 487 | loss: 792604992.00000 - R2: 1.0015 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m851540288.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 488 | loss: 851540288.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m766592896.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 489 | loss: 766592896.00000 - R2: 1.0015 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m1047250304.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 490 | loss: 1047250304.00000 - R2: 1.0037 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m942731904.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 491 | loss: 942731904.00000 - R2: 1.0032 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m1529690112.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 492 | loss: 1529690112.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m1376927744.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 493 | loss: 1376927744.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m1366508416.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 494 | loss: 1366508416.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m1230064128.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 495 | loss: 1230064128.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m1107264256.00000\u001b[0m\u001b[0m | time: 1.020s\n",
      "| SGD | epoch: 496 | loss: 1107264256.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m996744448.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 497 | loss: 996744448.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m1011862976.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 498 | loss: 1011862976.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m910883264.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 499 | loss: 910883264.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m1164830080.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 500 | loss: 1164830080.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 501  | total loss: \u001b[1m\u001b[32m1048553664.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 501 | loss: 1048553664.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 502  | total loss: \u001b[1m\u001b[32m1679875072.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 502 | loss: 1679875072.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 503  | total loss: \u001b[1m\u001b[32m1512094208.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 503 | loss: 1512094208.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 504  | total loss: \u001b[1m\u001b[32m1491815808.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 504 | loss: 1491815808.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 505  | total loss: \u001b[1m\u001b[32m1342840832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 505 | loss: 1342840832.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 506  | total loss: \u001b[1m\u001b[32m1449003776.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 506 | loss: 1449003776.00000 - R2: 1.0038 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 507  | total loss: \u001b[1m\u001b[32m1304310016.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 507 | loss: 1304310016.00000 - R2: 1.0033 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 508  | total loss: \u001b[1m\u001b[32m1278315264.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 508 | loss: 1278315264.00000 - R2: 1.0013 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 509  | total loss: \u001b[1m\u001b[32m1150690304.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 509 | loss: 1150690304.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 510  | total loss: \u001b[1m\u001b[32m1176275968.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 510 | loss: 1176275968.00000 - R2: 1.0012 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 511  | total loss: \u001b[1m\u001b[32m1058854976.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 511 | loss: 1058854976.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 512  | total loss: \u001b[1m\u001b[32m1025931136.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 512 | loss: 1025931136.00000 - R2: 1.0013 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 513  | total loss: \u001b[1m\u001b[32m923544640.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 513 | loss: 923544640.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 514  | total loss: \u001b[1m\u001b[32m893729408.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 514 | loss: 893729408.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 515  | total loss: \u001b[1m\u001b[32m804563072.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 515 | loss: 804563072.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 516  | total loss: \u001b[1m\u001b[32m724313344.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 516 | loss: 724313344.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 517  | total loss: \u001b[1m\u001b[32m652088640.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 517 | loss: 652088640.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 518  | total loss: \u001b[1m\u001b[32m682329088.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 518 | loss: 682329088.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 519  | total loss: \u001b[1m\u001b[32m614302784.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 519 | loss: 614302784.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 520  | total loss: \u001b[1m\u001b[32m642896512.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 520 | loss: 642896512.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 521  | total loss: \u001b[1m\u001b[32m578813504.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 521 | loss: 578813504.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 522  | total loss: \u001b[1m\u001b[32m608946432.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 522 | loss: 608946432.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 523  | total loss: \u001b[1m\u001b[32m548258432.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 523 | loss: 548258432.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 524  | total loss: \u001b[1m\u001b[32m629334656.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 524 | loss: 629334656.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 525  | total loss: \u001b[1m\u001b[32m566607808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 525 | loss: 566607808.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 526  | total loss: \u001b[1m\u001b[32m798921216.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 526 | loss: 798921216.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 527  | total loss: \u001b[1m\u001b[32m719235712.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 527 | loss: 719235712.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 528  | total loss: \u001b[1m\u001b[32m980890304.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 528 | loss: 980890304.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 529  | total loss: \u001b[1m\u001b[32m883007872.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 529 | loss: 883007872.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 530  | total loss: \u001b[1m\u001b[32m1012617600.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 530 | loss: 1012617600.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 531  | total loss: \u001b[1m\u001b[32m911562432.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 531 | loss: 911562432.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 532  | total loss: \u001b[1m\u001b[32m1125643904.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 532 | loss: 1125643904.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 533  | total loss: \u001b[1m\u001b[32m1013286144.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 533 | loss: 1013286144.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 534  | total loss: \u001b[1m\u001b[32m989854272.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 534 | loss: 989854272.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 535  | total loss: \u001b[1m\u001b[32m891075456.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 535 | loss: 891075456.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 536  | total loss: \u001b[1m\u001b[32m1026147776.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 536 | loss: 1026147776.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 537  | total loss: \u001b[1m\u001b[32m923739584.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 537 | loss: 923739584.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 538  | total loss: \u001b[1m\u001b[32m923065984.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 538 | loss: 923065984.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 539  | total loss: \u001b[1m\u001b[32m830966016.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 539 | loss: 830966016.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 540  | total loss: \u001b[1m\u001b[32m853674432.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 540 | loss: 853674432.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 541  | total loss: \u001b[1m\u001b[32m768513600.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 541 | loss: 768513600.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 542  | total loss: \u001b[1m\u001b[32m783455808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 542 | loss: 783455808.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 543  | total loss: \u001b[1m\u001b[32m705316864.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 543 | loss: 705316864.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 544  | total loss: \u001b[1m\u001b[32m905995712.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 544 | loss: 905995712.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 545  | total loss: \u001b[1m\u001b[32m815602752.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 545 | loss: 815602752.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 546  | total loss: \u001b[1m\u001b[32m1112320512.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 546 | loss: 1112320512.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 547  | total loss: \u001b[1m\u001b[32m1001295040.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 547 | loss: 1001295040.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 548  | total loss: \u001b[1m\u001b[32m993679360.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 548 | loss: 993679360.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 549  | total loss: \u001b[1m\u001b[32m894518016.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 549 | loss: 894518016.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 550  | total loss: \u001b[1m\u001b[32m899015232.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 550 | loss: 899015232.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 551  | total loss: \u001b[1m\u001b[32m809320320.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 551 | loss: 809320320.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 552  | total loss: \u001b[1m\u001b[32m827660736.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 552 | loss: 827660736.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 553  | total loss: \u001b[1m\u001b[32m745101248.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 553 | loss: 745101248.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 554  | total loss: \u001b[1m\u001b[32m1038358784.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 554 | loss: 1038358784.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 555  | total loss: \u001b[1m\u001b[32m934729536.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 555 | loss: 934729536.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 556  | total loss: \u001b[1m\u001b[32m907750400.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 556 | loss: 907750400.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 557  | total loss: \u001b[1m\u001b[32m817181952.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 557 | loss: 817181952.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 558  | total loss: \u001b[1m\u001b[32m1073704448.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 558 | loss: 1073704448.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 559  | total loss: \u001b[1m\u001b[32m966540608.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 559 | loss: 966540608.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 560  | total loss: \u001b[1m\u001b[32m961684224.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 560 | loss: 961684224.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 561  | total loss: \u001b[1m\u001b[32m865722432.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 561 | loss: 865722432.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 562  | total loss: \u001b[1m\u001b[32m1159202816.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 562 | loss: 1159202816.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 563  | total loss: \u001b[1m\u001b[32m1043489152.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 563 | loss: 1043489152.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 564  | total loss: \u001b[1m\u001b[32m1354034688.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 564 | loss: 1354034688.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 565  | total loss: \u001b[1m\u001b[32m1218837760.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 565 | loss: 1218837760.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 566  | total loss: \u001b[1m\u001b[32m1383206784.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 566 | loss: 1383206784.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 567  | total loss: \u001b[1m\u001b[32m1245092736.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 567 | loss: 1245092736.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 568  | total loss: \u001b[1m\u001b[32m1417839104.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 568 | loss: 1417839104.00000 - R2: 1.0020 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 569  | total loss: \u001b[1m\u001b[32m1276261760.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 569 | loss: 1276261760.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 570  | total loss: \u001b[1m\u001b[32m1457768320.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 570 | loss: 1457768320.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 571  | total loss: \u001b[1m\u001b[32m1312198144.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 571 | loss: 1312198144.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 572  | total loss: \u001b[1m\u001b[32m1271406848.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 572 | loss: 1271406848.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 573  | total loss: \u001b[1m\u001b[32m1144472704.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 573 | loss: 1144472704.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 574  | total loss: \u001b[1m\u001b[32m1124368256.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 574 | loss: 1124368256.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 575  | total loss: \u001b[1m\u001b[32m1012138048.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 575 | loss: 1012138048.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 576  | total loss: \u001b[1m\u001b[32m1034467072.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 576 | loss: 1034467072.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 577  | total loss: \u001b[1m\u001b[32m931226944.00000\u001b[0m\u001b[0m | time: 1.102s\n",
      "| SGD | epoch: 577 | loss: 931226944.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 578  | total loss: \u001b[1m\u001b[32m925951040.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 578 | loss: 925951040.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 579  | total loss: \u001b[1m\u001b[32m833562560.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 579 | loss: 833562560.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 580  | total loss: \u001b[1m\u001b[32m1398671616.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 580 | loss: 1398671616.00000 - R2: 1.0019 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 581  | total loss: \u001b[1m\u001b[32m1259011072.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 581 | loss: 1259011072.00000 - R2: 1.0016 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 582  | total loss: \u001b[1m\u001b[32m1225716352.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 582 | loss: 1225716352.00000 - R2: 1.0020 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 583  | total loss: \u001b[1m\u001b[32m1103351296.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 583 | loss: 1103351296.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 584  | total loss: \u001b[1m\u001b[32m1323719040.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 584 | loss: 1323719040.00000 - R2: 1.0021 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 585  | total loss: \u001b[1m\u001b[32m1191553792.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 585 | loss: 1191553792.00000 - R2: 1.0018 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 586  | total loss: \u001b[1m\u001b[32m1442348160.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 586 | loss: 1442348160.00000 - R2: 1.0022 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 587  | total loss: \u001b[1m\u001b[32m1298319872.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 587 | loss: 1298319872.00000 - R2: 1.0018 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 588  | total loss: \u001b[1m\u001b[32m1226959744.00000\u001b[0m\u001b[0m | time: 1.021s\n",
      "| SGD | epoch: 588 | loss: 1226959744.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 589  | total loss: \u001b[1m\u001b[32m1104470400.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 589 | loss: 1104470400.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 590  | total loss: \u001b[1m\u001b[32m1314548480.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 590 | loss: 1314548480.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 591  | total loss: \u001b[1m\u001b[32m1183300224.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 591 | loss: 1183300224.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 592  | total loss: \u001b[1m\u001b[32m1325954944.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 592 | loss: 1325954944.00000 - R2: 1.0023 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 593  | total loss: \u001b[1m\u001b[32m1193566080.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 593 | loss: 1193566080.00000 - R2: 1.0019 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 594  | total loss: \u001b[1m\u001b[32m1363687296.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 594 | loss: 1363687296.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 595  | total loss: \u001b[1m\u001b[32m1227525120.00000\u001b[0m\u001b[0m | time: 1.037s\n",
      "| SGD | epoch: 595 | loss: 1227525120.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 596  | total loss: \u001b[1m\u001b[32m1172209792.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 596 | loss: 1172209792.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 597  | total loss: \u001b[1m\u001b[32m1055195392.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 597 | loss: 1055195392.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 598  | total loss: \u001b[1m\u001b[32m1212872448.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 598 | loss: 1212872448.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 599  | total loss: \u001b[1m\u001b[32m1091791872.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 599 | loss: 1091791872.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 600  | total loss: \u001b[1m\u001b[32m1359976576.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 600 | loss: 1359976576.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 601  | total loss: \u001b[1m\u001b[32m1224185472.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 601 | loss: 1224185472.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 602  | total loss: \u001b[1m\u001b[32m1451401856.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 602 | loss: 1451401856.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 603  | total loss: \u001b[1m\u001b[32m1306468224.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 603 | loss: 1306468224.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 604  | total loss: \u001b[1m\u001b[32m1280288768.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 604 | loss: 1280288768.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 605  | total loss: \u001b[1m\u001b[32m1152466432.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 605 | loss: 1152466432.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 606  | total loss: \u001b[1m\u001b[32m1399515264.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 606 | loss: 1399515264.00000 - R2: 0.9959 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 607  | total loss: \u001b[1m\u001b[32m1259770368.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 607 | loss: 1259770368.00000 - R2: 0.9962 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 608  | total loss: \u001b[1m\u001b[32m1278383360.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 608 | loss: 1278383360.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 609  | total loss: \u001b[1m\u001b[32m1150751616.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 609 | loss: 1150751616.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 610  | total loss: \u001b[1m\u001b[32m1357146112.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 610 | loss: 1357146112.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 611  | total loss: \u001b[1m\u001b[32m1221638144.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 611 | loss: 1221638144.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 612  | total loss: \u001b[1m\u001b[32m2140983808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 612 | loss: 2140983808.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 613  | total loss: \u001b[1m\u001b[32m1927091968.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 613 | loss: 1927091968.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 614  | total loss: \u001b[1m\u001b[32m1986548992.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 614 | loss: 1986548992.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 615  | total loss: \u001b[1m\u001b[32m1788100608.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 615 | loss: 1788100608.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 616  | total loss: \u001b[1m\u001b[32m1908793728.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 616 | loss: 1908793728.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 617  | total loss: \u001b[1m\u001b[32m1718120960.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 617 | loss: 1718120960.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 618  | total loss: \u001b[1m\u001b[32m1631537280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 618 | loss: 1631537280.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 619  | total loss: \u001b[1m\u001b[32m1468590080.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 619 | loss: 1468590080.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 620  | total loss: \u001b[1m\u001b[32m1562080256.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 620 | loss: 1562080256.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 621  | total loss: \u001b[1m\u001b[32m1406078848.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 621 | loss: 1406078848.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 622  | total loss: \u001b[1m\u001b[32m1265677568.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 622 | loss: 1265677568.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 623  | total loss: \u001b[1m\u001b[32m1139316352.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 623 | loss: 1139316352.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 624  | total loss: \u001b[1m\u001b[32m2239944960.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 624 | loss: 2239944960.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 625  | total loss: \u001b[1m\u001b[32m2016157056.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 625 | loss: 2016157056.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 626  | total loss: \u001b[1m\u001b[32m1894370432.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 626 | loss: 1894370432.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 627  | total loss: \u001b[1m\u001b[32m1705139968.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 627 | loss: 1705139968.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 628  | total loss: \u001b[1m\u001b[32m1649203072.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 628 | loss: 1649203072.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 629  | total loss: \u001b[1m\u001b[32m1484489344.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 629 | loss: 1484489344.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 630  | total loss: \u001b[1m\u001b[32m1445920384.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 630 | loss: 1445920384.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 631  | total loss: \u001b[1m\u001b[32m1301534976.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 631 | loss: 1301534976.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 632  | total loss: \u001b[1m\u001b[32m1585109760.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 632 | loss: 1585109760.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 633  | total loss: \u001b[1m\u001b[32m1426805376.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 633 | loss: 1426805376.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 634  | total loss: \u001b[1m\u001b[32m1690918400.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 634 | loss: 1690918400.00000 - R2: 1.0015 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 635  | total loss: \u001b[1m\u001b[32m1522033152.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 635 | loss: 1522033152.00000 - R2: 1.0012 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 636  | total loss: \u001b[1m\u001b[32m1467867520.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 636 | loss: 1467867520.00000 - R2: 1.0020 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 637  | total loss: \u001b[1m\u001b[32m1321287424.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 637 | loss: 1321287424.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 638  | total loss: \u001b[1m\u001b[32m1270324096.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 638 | loss: 1270324096.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 639  | total loss: \u001b[1m\u001b[32m1143498240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 639 | loss: 1143498240.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 640  | total loss: \u001b[1m\u001b[32m1282242304.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 640 | loss: 1282242304.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 641  | total loss: \u001b[1m\u001b[32m1154224640.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 641 | loss: 1154224640.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 642  | total loss: \u001b[1m\u001b[32m1124037376.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 642 | loss: 1124037376.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 643  | total loss: \u001b[1m\u001b[32m1011840256.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 643 | loss: 1011840256.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 644  | total loss: \u001b[1m\u001b[32m1230969984.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 644 | loss: 1230969984.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 645  | total loss: \u001b[1m\u001b[32m1108079616.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 645 | loss: 1108079616.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 646  | total loss: \u001b[1m\u001b[32m1085123200.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 646 | loss: 1085123200.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 647  | total loss: \u001b[1m\u001b[32m976817472.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 647 | loss: 976817472.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 648  | total loss: \u001b[1m\u001b[32m968260288.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 648 | loss: 968260288.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 649  | total loss: \u001b[1m\u001b[32m871640832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 649 | loss: 871640832.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 650  | total loss: \u001b[1m\u001b[32m859510400.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 650 | loss: 859510400.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 651  | total loss: \u001b[1m\u001b[32m773765952.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 651 | loss: 773765952.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 652  | total loss: \u001b[1m\u001b[32m830530176.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 652 | loss: 830530176.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 653  | total loss: \u001b[1m\u001b[32m747683776.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 653 | loss: 747683776.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 654  | total loss: \u001b[1m\u001b[32m828167616.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 654 | loss: 828167616.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 655  | total loss: \u001b[1m\u001b[32m745557440.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 655 | loss: 745557440.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 656  | total loss: \u001b[1m\u001b[32m778916288.00000\u001b[0m\u001b[0m | time: 1.023s\n",
      "| SGD | epoch: 656 | loss: 778916288.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 657  | total loss: \u001b[1m\u001b[32m701231296.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 657 | loss: 701231296.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 658  | total loss: \u001b[1m\u001b[32m955616896.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 658 | loss: 955616896.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 659  | total loss: \u001b[1m\u001b[32m860261824.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 659 | loss: 860261824.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 660  | total loss: \u001b[1m\u001b[32m860063488.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 660 | loss: 860063488.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 661  | total loss: \u001b[1m\u001b[32m774263744.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 661 | loss: 774263744.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 662  | total loss: \u001b[1m\u001b[32m801778624.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 662 | loss: 801778624.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 663  | total loss: \u001b[1m\u001b[32m721807360.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 663 | loss: 721807360.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 664  | total loss: \u001b[1m\u001b[32m740745344.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 664 | loss: 740745344.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 665  | total loss: \u001b[1m\u001b[32m666877440.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 665 | loss: 666877440.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 666  | total loss: \u001b[1m\u001b[32m700900352.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 666 | loss: 700900352.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 667  | total loss: \u001b[1m\u001b[32m631016960.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 667 | loss: 631016960.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 668  | total loss: \u001b[1m\u001b[32m937130112.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 668 | loss: 937130112.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 669  | total loss: \u001b[1m\u001b[32m843623680.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 669 | loss: 843623680.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 670  | total loss: \u001b[1m\u001b[32m821282944.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 670 | loss: 821282944.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 671  | total loss: \u001b[1m\u001b[32m739361280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 671 | loss: 739361280.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 672  | total loss: \u001b[1m\u001b[32m764628480.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 672 | loss: 764628480.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 673  | total loss: \u001b[1m\u001b[32m688372224.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 673 | loss: 688372224.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 674  | total loss: \u001b[1m\u001b[32m892446784.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 674 | loss: 892446784.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 675  | total loss: \u001b[1m\u001b[32m803408704.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 675 | loss: 803408704.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 676  | total loss: \u001b[1m\u001b[32m1038165760.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 676 | loss: 1038165760.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 677  | total loss: \u001b[1m\u001b[32m934555776.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 677 | loss: 934555776.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 678  | total loss: \u001b[1m\u001b[32m907642176.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 678 | loss: 907642176.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 679  | total loss: \u001b[1m\u001b[32m817084544.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 679 | loss: 817084544.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 680  | total loss: \u001b[1m\u001b[32m1104005632.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 680 | loss: 1104005632.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 681  | total loss: \u001b[1m\u001b[32m993811648.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 681 | loss: 993811648.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 682  | total loss: \u001b[1m\u001b[32m1141070720.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 682 | loss: 1141070720.00000 - R2: 1.0015 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 683  | total loss: \u001b[1m\u001b[32m1027170240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 683 | loss: 1027170240.00000 - R2: 1.0012 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 684  | total loss: \u001b[1m\u001b[32m978166592.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 684 | loss: 978166592.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 685  | total loss: \u001b[1m\u001b[32m880556544.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 685 | loss: 880556544.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 686  | total loss: \u001b[1m\u001b[32m1064072896.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 686 | loss: 1064072896.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 687  | total loss: \u001b[1m\u001b[32m957872192.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 687 | loss: 957872192.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 688  | total loss: \u001b[1m\u001b[32m973987648.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 688 | loss: 973987648.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 689  | total loss: \u001b[1m\u001b[32m876795520.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 689 | loss: 876795520.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 690  | total loss: \u001b[1m\u001b[32m850876288.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 690 | loss: 850876288.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 691  | total loss: \u001b[1m\u001b[32m765995264.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 691 | loss: 765995264.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 692  | total loss: \u001b[1m\u001b[32m998214592.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 692 | loss: 998214592.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 693  | total loss: \u001b[1m\u001b[32m898599744.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 693 | loss: 898599744.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 694  | total loss: \u001b[1m\u001b[32m891824320.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 694 | loss: 891824320.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 695  | total loss: \u001b[1m\u001b[32m802848512.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 695 | loss: 802848512.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 696  | total loss: \u001b[1m\u001b[32m904544000.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 696 | loss: 904544000.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 697  | total loss: \u001b[1m\u001b[32m814296192.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 697 | loss: 814296192.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 698  | total loss: \u001b[1m\u001b[32m816487104.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 698 | loss: 816487104.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 699  | total loss: \u001b[1m\u001b[32m735044992.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 699 | loss: 735044992.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 700  | total loss: \u001b[1m\u001b[32m752262528.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 700 | loss: 752262528.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 701  | total loss: \u001b[1m\u001b[32m677242880.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 701 | loss: 677242880.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 702  | total loss: \u001b[1m\u001b[32m731051712.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 702 | loss: 731051712.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 703  | total loss: \u001b[1m\u001b[32m658153152.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 703 | loss: 658153152.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 704  | total loss: \u001b[1m\u001b[32m648049152.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 704 | loss: 648049152.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 705  | total loss: \u001b[1m\u001b[32m583450880.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 705 | loss: 583450880.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 706  | total loss: \u001b[1m\u001b[32m587603520.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 706 | loss: 587603520.00000 - R2: 0.9962 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 707  | total loss: \u001b[1m\u001b[32m529049792.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 707 | loss: 529049792.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 708  | total loss: \u001b[1m\u001b[32m584133632.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 708 | loss: 584133632.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 709  | total loss: \u001b[1m\u001b[32m525926880.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 709 | loss: 525926880.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 710  | total loss: \u001b[1m\u001b[32m533553984.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 710 | loss: 533553984.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 711  | total loss: \u001b[1m\u001b[32m480405216.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 711 | loss: 480405216.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 712  | total loss: \u001b[1m\u001b[32m1252365056.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 712 | loss: 1252365056.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 713  | total loss: \u001b[1m\u001b[32m1127335168.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 713 | loss: 1127335168.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 714  | total loss: \u001b[1m\u001b[32m1092738176.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 714 | loss: 1092738176.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 715  | total loss: \u001b[1m\u001b[32m983670976.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 715 | loss: 983670976.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 716  | total loss: \u001b[1m\u001b[32m1181680512.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 716 | loss: 1181680512.00000 - R2: 1.0025 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 717  | total loss: \u001b[1m\u001b[32m1063719040.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 717 | loss: 1063719040.00000 - R2: 1.0022 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 718  | total loss: \u001b[1m\u001b[32m1070261248.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 718 | loss: 1070261248.00000 - R2: 1.0021 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 719  | total loss: \u001b[1m\u001b[32m963441728.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 719 | loss: 963441728.00000 - R2: 1.0018 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 720  | total loss: \u001b[1m\u001b[32m953882496.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 720 | loss: 953882496.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 721  | total loss: \u001b[1m\u001b[32m858700864.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 721 | loss: 858700864.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 722  | total loss: \u001b[1m\u001b[32m1114671360.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 722 | loss: 1114671360.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 723  | total loss: \u001b[1m\u001b[32m1003410816.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 723 | loss: 1003410816.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 724  | total loss: \u001b[1m\u001b[32m1211480960.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 724 | loss: 1211480960.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 725  | total loss: \u001b[1m\u001b[32m1090539520.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 725 | loss: 1090539520.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 726  | total loss: \u001b[1m\u001b[32m1295679872.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 726 | loss: 1295679872.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 727  | total loss: \u001b[1m\u001b[32m1166318464.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 727 | loss: 1166318464.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 728  | total loss: \u001b[1m\u001b[32m1142531712.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 728 | loss: 1142531712.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 729  | total loss: \u001b[1m\u001b[32m1028485120.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 729 | loss: 1028485120.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 730  | total loss: \u001b[1m\u001b[32m1051698496.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 730 | loss: 1051698496.00000 - R2: 1.0021 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 731  | total loss: \u001b[1m\u001b[32m946735232.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 731 | loss: 946735232.00000 - R2: 1.0018 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 732  | total loss: \u001b[1m\u001b[32m1110520192.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 732 | loss: 1110520192.00000 - R2: 1.0043 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 733  | total loss: \u001b[1m\u001b[32m999674752.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 733 | loss: 999674752.00000 - R2: 1.0038 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 734  | total loss: \u001b[1m\u001b[32m952981824.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 734 | loss: 952981824.00000 - R2: 1.0028 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 735  | total loss: \u001b[1m\u001b[32m857890240.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 735 | loss: 857890240.00000 - R2: 1.0024 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 736  | total loss: \u001b[1m\u001b[32m1134872832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 736 | loss: 1134872832.00000 - R2: 1.0026 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 737  | total loss: \u001b[1m\u001b[32m1021592128.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 737 | loss: 1021592128.00000 - R2: 1.0022 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 738  | total loss: \u001b[1m\u001b[32m1005535808.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 738 | loss: 1005535808.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 739  | total loss: \u001b[1m\u001b[32m905188864.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 739 | loss: 905188864.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 740  | total loss: \u001b[1m\u001b[32m883836544.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 740 | loss: 883836544.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 741  | total loss: \u001b[1m\u001b[32m795659520.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 741 | loss: 795659520.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 742  | total loss: \u001b[1m\u001b[32m787846976.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 742 | loss: 787846976.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 743  | total loss: \u001b[1m\u001b[32m709268864.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 743 | loss: 709268864.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 744  | total loss: \u001b[1m\u001b[32m972138688.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 744 | loss: 972138688.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 745  | total loss: \u001b[1m\u001b[32m875131392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 745 | loss: 875131392.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 746  | total loss: \u001b[1m\u001b[32m1074475136.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 746 | loss: 1074475136.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 747  | total loss: \u001b[1m\u001b[32m967234240.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 747 | loss: 967234240.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 748  | total loss: \u001b[1m\u001b[32m962281792.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 748 | loss: 962281792.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 749  | total loss: \u001b[1m\u001b[32m866260224.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 749 | loss: 866260224.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 750  | total loss: \u001b[1m\u001b[32m887415360.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 750 | loss: 887415360.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 751  | total loss: \u001b[1m\u001b[32m798880448.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 751 | loss: 798880448.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 752  | total loss: \u001b[1m\u001b[32m1103157760.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 752 | loss: 1103157760.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 753  | total loss: \u001b[1m\u001b[32m993048576.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 753 | loss: 993048576.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 754  | total loss: \u001b[1m\u001b[32m1202203264.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 754 | loss: 1202203264.00000 - R2: 1.0040 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 755  | total loss: \u001b[1m\u001b[32m1082189568.00000\u001b[0m\u001b[0m | time: 1.112s\n",
      "| SGD | epoch: 755 | loss: 1082189568.00000 - R2: 1.0035 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 756  | total loss: \u001b[1m\u001b[32m1076061312.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 756 | loss: 1076061312.00000 - R2: 1.0027 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 757  | total loss: \u001b[1m\u001b[32m968661760.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 757 | loss: 968661760.00000 - R2: 1.0023 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 758  | total loss: \u001b[1m\u001b[32m957041920.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 758 | loss: 957041920.00000 - R2: 1.0022 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 759  | total loss: \u001b[1m\u001b[32m861544320.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 759 | loss: 861544320.00000 - R2: 1.0019 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 760  | total loss: \u001b[1m\u001b[32m842188416.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 760 | loss: 842188416.00000 - R2: 1.0016 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 761  | total loss: \u001b[1m\u001b[32m758176192.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 761 | loss: 758176192.00000 - R2: 1.0013 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 762  | total loss: \u001b[1m\u001b[32m1079912320.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 762 | loss: 1079912320.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 763  | total loss: \u001b[1m\u001b[32m972127680.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 763 | loss: 972127680.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 764  | total loss: \u001b[1m\u001b[32m954461248.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 764 | loss: 954461248.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 765  | total loss: \u001b[1m\u001b[32m859221760.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 765 | loss: 859221760.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 766  | total loss: \u001b[1m\u001b[32m897077376.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 766 | loss: 897077376.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 767  | total loss: \u001b[1m\u001b[32m807576256.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 767 | loss: 807576256.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 768  | total loss: \u001b[1m\u001b[32m1061366464.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 768 | loss: 1061366464.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 769  | total loss: \u001b[1m\u001b[32m955436416.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 769 | loss: 955436416.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 770  | total loss: \u001b[1m\u001b[32m971100480.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 770 | loss: 971100480.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 771  | total loss: \u001b[1m\u001b[32m874197056.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 771 | loss: 874197056.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 772  | total loss: \u001b[1m\u001b[32m1075571456.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 772 | loss: 1075571456.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 773  | total loss: \u001b[1m\u001b[32m968220928.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 773 | loss: 968220928.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 774  | total loss: \u001b[1m\u001b[32m975329856.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 774 | loss: 975329856.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 775  | total loss: \u001b[1m\u001b[32m878003456.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 775 | loss: 878003456.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 776  | total loss: \u001b[1m\u001b[32m1032372992.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 776 | loss: 1032372992.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 777  | total loss: \u001b[1m\u001b[32m929342272.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 777 | loss: 929342272.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 778  | total loss: \u001b[1m\u001b[32m894254208.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 778 | loss: 894254208.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 779  | total loss: \u001b[1m\u001b[32m805035392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 779 | loss: 805035392.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 780  | total loss: \u001b[1m\u001b[32m1850285056.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 780 | loss: 1850285056.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 781  | total loss: \u001b[1m\u001b[32m1665463168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 781 | loss: 1665463168.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 782  | total loss: \u001b[1m\u001b[32m1596945280.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 782 | loss: 1596945280.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 783  | total loss: \u001b[1m\u001b[32m1437457408.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 783 | loss: 1437457408.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 784  | total loss: \u001b[1m\u001b[32m1523097600.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 784 | loss: 1523097600.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 785  | total loss: \u001b[1m\u001b[32m1370994432.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 785 | loss: 1370994432.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 786  | total loss: \u001b[1m\u001b[32m1332451072.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 786 | loss: 1332451072.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 787  | total loss: \u001b[1m\u001b[32m1199412608.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 787 | loss: 1199412608.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 788  | total loss: \u001b[1m\u001b[32m1382162560.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 788 | loss: 1382162560.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 789  | total loss: \u001b[1m\u001b[32m1244152832.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 789 | loss: 1244152832.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 790  | total loss: \u001b[1m\u001b[32m1220423680.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 790 | loss: 1220423680.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 791  | total loss: \u001b[1m\u001b[32m1098587904.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 791 | loss: 1098587904.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 792  | total loss: \u001b[1m\u001b[32m1073226496.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 792 | loss: 1073226496.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 793  | total loss: \u001b[1m\u001b[32m966110464.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 793 | loss: 966110464.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 794  | total loss: \u001b[1m\u001b[32m1120331264.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 794 | loss: 1120331264.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 795  | total loss: \u001b[1m\u001b[32m1008504768.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 795 | loss: 1008504768.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 796  | total loss: \u001b[1m\u001b[32m1258368128.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 796 | loss: 1258368128.00000 - R2: 1.0037 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 797  | total loss: \u001b[1m\u001b[32m1132737920.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 797 | loss: 1132737920.00000 - R2: 1.0032 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 798  | total loss: \u001b[1m\u001b[32m1155203328.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 798 | loss: 1155203328.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 799  | total loss: \u001b[1m\u001b[32m1039889600.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 799 | loss: 1039889600.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 800  | total loss: \u001b[1m\u001b[32m1013925440.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 800 | loss: 1013925440.00000 - R2: 1.0018 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 801  | total loss: \u001b[1m\u001b[32m912739520.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 801 | loss: 912739520.00000 - R2: 1.0015 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 802  | total loss: \u001b[1m\u001b[32m922230528.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 802 | loss: 922230528.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 803  | total loss: \u001b[1m\u001b[32m830214080.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 803 | loss: 830214080.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 804  | total loss: \u001b[1m\u001b[32m1008517568.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 804 | loss: 1008517568.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 805  | total loss: \u001b[1m\u001b[32m907872384.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 805 | loss: 907872384.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 806  | total loss: \u001b[1m\u001b[32m1075010304.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 806 | loss: 1075010304.00000 - R2: 1.0032 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 807  | total loss: \u001b[1m\u001b[32m967715904.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 807 | loss: 967715904.00000 - R2: 1.0028 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 808  | total loss: \u001b[1m\u001b[32m930199360.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 808 | loss: 930199360.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 809  | total loss: \u001b[1m\u001b[32m837386048.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 809 | loss: 837386048.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 810  | total loss: \u001b[1m\u001b[32m845252224.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 810 | loss: 845252224.00000 - R2: 1.0020 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 811  | total loss: \u001b[1m\u001b[32m760933632.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 811 | loss: 760933632.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 812  | total loss: \u001b[1m\u001b[32m1010683968.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 812 | loss: 1010683968.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 813  | total loss: \u001b[1m\u001b[32m909822208.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 813 | loss: 909822208.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 814  | total loss: \u001b[1m\u001b[32m900250240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 814 | loss: 900250240.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 815  | total loss: \u001b[1m\u001b[32m810431808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 815 | loss: 810431808.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 816  | total loss: \u001b[1m\u001b[32m793464896.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 816 | loss: 793464896.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 817  | total loss: \u001b[1m\u001b[32m714324992.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 817 | loss: 714324992.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 818  | total loss: \u001b[1m\u001b[32m1086259712.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 818 | loss: 1086259712.00000 - R2: 1.0029 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 819  | total loss: \u001b[1m\u001b[32m977840320.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 819 | loss: 977840320.00000 - R2: 1.0025 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 820  | total loss: \u001b[1m\u001b[32m984745024.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 820 | loss: 984745024.00000 - R2: 1.0019 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 821  | total loss: \u001b[1m\u001b[32m886477120.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 821 | loss: 886477120.00000 - R2: 1.0016 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 822  | total loss: \u001b[1m\u001b[32m946734080.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 822 | loss: 946734080.00000 - R2: 1.0037 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 823  | total loss: \u001b[1m\u001b[32m852267264.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 823 | loss: 852267264.00000 - R2: 1.0032 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 824  | total loss: \u001b[1m\u001b[32m1046496384.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 824 | loss: 1046496384.00000 - R2: 1.0020 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 825  | total loss: \u001b[1m\u001b[32m942053376.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 825 | loss: 942053376.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 826  | total loss: \u001b[1m\u001b[32m1162141440.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 826 | loss: 1162141440.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 827  | total loss: \u001b[1m\u001b[32m1046133888.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 827 | loss: 1046133888.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 828  | total loss: \u001b[1m\u001b[32m1072281728.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 828 | loss: 1072281728.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 829  | total loss: \u001b[1m\u001b[32m965260160.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 829 | loss: 965260160.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 830  | total loss: \u001b[1m\u001b[32m987237120.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 830 | loss: 987237120.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 831  | total loss: \u001b[1m\u001b[32m888720000.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 831 | loss: 888720000.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 832  | total loss: \u001b[1m\u001b[32m1185204480.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 832 | loss: 1185204480.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 833  | total loss: \u001b[1m\u001b[32m1066890624.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 833 | loss: 1066890624.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 834  | total loss: \u001b[1m\u001b[32m1054214400.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 834 | loss: 1054214400.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 835  | total loss: \u001b[1m\u001b[32m948999552.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 835 | loss: 948999552.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 836  | total loss: \u001b[1m\u001b[32m1148617728.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 836 | loss: 1148617728.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 837  | total loss: \u001b[1m\u001b[32m1033962560.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 837 | loss: 1033962560.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 838  | total loss: \u001b[1m\u001b[32m1039577280.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 838 | loss: 1039577280.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 839  | total loss: \u001b[1m\u001b[32m935826176.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 839 | loss: 935826176.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 840  | total loss: \u001b[1m\u001b[32m1148711040.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 840 | loss: 1148711040.00000 - R2: 1.0034 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 841  | total loss: \u001b[1m\u001b[32m1034046528.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 841 | loss: 1034046528.00000 - R2: 1.0029 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 842  | total loss: \u001b[1m\u001b[32m1010949376.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 842 | loss: 1010949376.00000 - R2: 1.0024 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 843  | total loss: \u001b[1m\u001b[32m910061056.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 843 | loss: 910061056.00000 - R2: 1.0020 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 844  | total loss: \u001b[1m\u001b[32m930750592.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 844 | loss: 930750592.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 845  | total loss: \u001b[1m\u001b[32m837882112.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 845 | loss: 837882112.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 846  | total loss: \u001b[1m\u001b[32m849091392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 846 | loss: 849091392.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 847  | total loss: \u001b[1m\u001b[32m764388864.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 847 | loss: 764388864.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 848  | total loss: \u001b[1m\u001b[32m756262464.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 848 | loss: 756262464.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 849  | total loss: \u001b[1m\u001b[32m680842816.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 849 | loss: 680842816.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 850  | total loss: \u001b[1m\u001b[32m700990656.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 850 | loss: 700990656.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 851  | total loss: \u001b[1m\u001b[32m631098240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 851 | loss: 631098240.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 852  | total loss: \u001b[1m\u001b[32m664067904.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 852 | loss: 664067904.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 853  | total loss: \u001b[1m\u001b[32m597867712.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 853 | loss: 597867712.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 854  | total loss: \u001b[1m\u001b[32m623402240.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 854 | loss: 623402240.00000 - R2: 1.0018 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 855  | total loss: \u001b[1m\u001b[32m561268608.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 855 | loss: 561268608.00000 - R2: 1.0015 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 856  | total loss: \u001b[1m\u001b[32m556918336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 856 | loss: 556918336.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 857  | total loss: \u001b[1m\u001b[32m501433120.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 857 | loss: 501433120.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 858  | total loss: \u001b[1m\u001b[32m478898176.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 858 | loss: 478898176.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 859  | total loss: \u001b[1m\u001b[32m431214976.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 859 | loss: 431214976.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 860  | total loss: \u001b[1m\u001b[32m631574144.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 860 | loss: 631574144.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 861  | total loss: \u001b[1m\u001b[32m568623360.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 861 | loss: 568623360.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 862  | total loss: \u001b[1m\u001b[32m645158656.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 862 | loss: 645158656.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 863  | total loss: \u001b[1m\u001b[32m580849408.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 863 | loss: 580849408.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 864  | total loss: \u001b[1m\u001b[32m522971072.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 864 | loss: 522971072.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 865  | total loss: \u001b[1m\u001b[32m470880576.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 865 | loss: 470880576.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 866  | total loss: \u001b[1m\u001b[32m522694464.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 866 | loss: 522694464.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 867  | total loss: \u001b[1m\u001b[32m470631648.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 867 | loss: 470631648.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 868  | total loss: \u001b[1m\u001b[32m710019392.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 868 | loss: 710019392.00000 - R2: 1.0024 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 869  | total loss: \u001b[1m\u001b[32m639224064.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 869 | loss: 639224064.00000 - R2: 1.0021 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 870  | total loss: \u001b[1m\u001b[32m666624832.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 870 | loss: 666624832.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 871  | total loss: \u001b[1m\u001b[32m600168960.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 871 | loss: 600168960.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 872  | total loss: \u001b[1m\u001b[32m642880704.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 872 | loss: 642880704.00000 - R2: 1.0012 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 873  | total loss: \u001b[1m\u001b[32m578799232.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 873 | loss: 578799232.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 874  | total loss: \u001b[1m\u001b[32m624483008.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 874 | loss: 624483008.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 875  | total loss: \u001b[1m\u001b[32m562241344.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 875 | loss: 562241344.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 876  | total loss: \u001b[1m\u001b[32m814885312.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 876 | loss: 814885312.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 877  | total loss: \u001b[1m\u001b[32m733603392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 877 | loss: 733603392.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 878  | total loss: \u001b[1m\u001b[32m769792896.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 878 | loss: 769792896.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 879  | total loss: \u001b[1m\u001b[32m693020224.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 879 | loss: 693020224.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 880  | total loss: \u001b[1m\u001b[32m699452544.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 880 | loss: 699452544.00000 - R2: 1.0012 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 881  | total loss: \u001b[1m\u001b[32m629713920.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 881 | loss: 629713920.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 882  | total loss: \u001b[1m\u001b[32m817764992.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 882 | loss: 817764992.00000 - R2: 0.9962 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 883  | total loss: \u001b[1m\u001b[32m736195072.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 883 | loss: 736195072.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 884  | total loss: \u001b[1m\u001b[32m890622976.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 884 | loss: 890622976.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 885  | total loss: \u001b[1m\u001b[32m801767296.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 885 | loss: 801767296.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 886  | total loss: \u001b[1m\u001b[32m1019685312.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 886 | loss: 1019685312.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 887  | total loss: \u001b[1m\u001b[32m917923392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 887 | loss: 917923392.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 888  | total loss: \u001b[1m\u001b[32m946422592.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 888 | loss: 946422592.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 889  | total loss: \u001b[1m\u001b[32m851986944.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 889 | loss: 851986944.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 890  | total loss: \u001b[1m\u001b[32m871925568.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 890 | loss: 871925568.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 891  | total loss: \u001b[1m\u001b[32m784939648.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 891 | loss: 784939648.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 892  | total loss: \u001b[1m\u001b[32m840359104.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 892 | loss: 840359104.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 893  | total loss: \u001b[1m\u001b[32m756529792.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 893 | loss: 756529792.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 894  | total loss: \u001b[1m\u001b[32m763261568.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 894 | loss: 763261568.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 895  | total loss: \u001b[1m\u001b[32m687142016.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 895 | loss: 687142016.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 896  | total loss: \u001b[1m\u001b[32m717425280.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 896 | loss: 717425280.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 897  | total loss: \u001b[1m\u001b[32m645889344.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 897 | loss: 645889344.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 898  | total loss: \u001b[1m\u001b[32m674779328.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 898 | loss: 674779328.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 899  | total loss: \u001b[1m\u001b[32m607508032.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 899 | loss: 607508032.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 900  | total loss: \u001b[1m\u001b[32m639666432.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 900 | loss: 639666432.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 901  | total loss: \u001b[1m\u001b[32m575906432.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 901 | loss: 575906432.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 902  | total loss: \u001b[1m\u001b[32m655904640.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 902 | loss: 655904640.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 903  | total loss: \u001b[1m\u001b[32m590520768.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 903 | loss: 590520768.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 904  | total loss: \u001b[1m\u001b[32m625202240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 904 | loss: 625202240.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 905  | total loss: \u001b[1m\u001b[32m562888640.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 905 | loss: 562888640.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 906  | total loss: \u001b[1m\u001b[32m728473600.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 906 | loss: 728473600.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 907  | total loss: \u001b[1m\u001b[32m655832832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 907 | loss: 655832832.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 908  | total loss: \u001b[1m\u001b[32m688242880.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 908 | loss: 688242880.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 909  | total loss: \u001b[1m\u001b[32m619625216.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 909 | loss: 619625216.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 910  | total loss: \u001b[1m\u001b[32m830502336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 910 | loss: 830502336.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 911  | total loss: \u001b[1m\u001b[32m747658688.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 911 | loss: 747658688.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 912  | total loss: \u001b[1m\u001b[32m798107584.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 912 | loss: 798107584.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 913  | total loss: \u001b[1m\u001b[32m718503424.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 913 | loss: 718503424.00000 - R2: 1.0012 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 914  | total loss: \u001b[1m\u001b[32m728869248.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 914 | loss: 728869248.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 915  | total loss: \u001b[1m\u001b[32m656188928.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 915 | loss: 656188928.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 916  | total loss: \u001b[1m\u001b[32m926975872.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 916 | loss: 926975872.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 917  | total loss: \u001b[1m\u001b[32m834484864.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 917 | loss: 834484864.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 918  | total loss: \u001b[1m\u001b[32m854329152.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 918 | loss: 854329152.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 919  | total loss: \u001b[1m\u001b[32m769102848.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 919 | loss: 769102848.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 920  | total loss: \u001b[1m\u001b[32m990832192.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 920 | loss: 990832192.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 921  | total loss: \u001b[1m\u001b[32m891955584.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 921 | loss: 891955584.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 922  | total loss: \u001b[1m\u001b[32m1112646784.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 922 | loss: 1112646784.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 923  | total loss: \u001b[1m\u001b[32m1001588736.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 923 | loss: 1001588736.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 924  | total loss: \u001b[1m\u001b[32m1271645184.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 924 | loss: 1271645184.00000 - R2: 1.0026 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 925  | total loss: \u001b[1m\u001b[32m1144687232.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 925 | loss: 1144687232.00000 - R2: 1.0023 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 926  | total loss: \u001b[1m\u001b[32m1435646976.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 926 | loss: 1435646976.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 927  | total loss: \u001b[1m\u001b[32m1292288896.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 927 | loss: 1292288896.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 928  | total loss: \u001b[1m\u001b[32m1301035136.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 928 | loss: 1301035136.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 929  | total loss: \u001b[1m\u001b[32m1171138176.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 929 | loss: 1171138176.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 930  | total loss: \u001b[1m\u001b[32m1170545280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 930 | loss: 1170545280.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 931  | total loss: \u001b[1m\u001b[32m1053697344.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 931 | loss: 1053697344.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 932  | total loss: \u001b[1m\u001b[32m1150224512.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 932 | loss: 1150224512.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 933  | total loss: \u001b[1m\u001b[32m1035408640.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 933 | loss: 1035408640.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 934  | total loss: \u001b[1m\u001b[32m1212214656.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 934 | loss: 1212214656.00000 - R2: 0.9964 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 935  | total loss: \u001b[1m\u001b[32m1091199744.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 935 | loss: 1091199744.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 936  | total loss: \u001b[1m\u001b[32m1113073792.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 936 | loss: 1113073792.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 937  | total loss: \u001b[1m\u001b[32m1001972992.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 937 | loss: 1001972992.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 938  | total loss: \u001b[1m\u001b[32m1174257536.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 938 | loss: 1174257536.00000 - R2: 0.9954 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 939  | total loss: \u001b[1m\u001b[32m1057038400.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 939 | loss: 1057038400.00000 - R2: 0.9958 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 940  | total loss: \u001b[1m\u001b[32m1023537536.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 940 | loss: 1023537536.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 941  | total loss: \u001b[1m\u001b[32m921390400.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 941 | loss: 921390400.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 942  | total loss: \u001b[1m\u001b[32m1083068288.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 942 | loss: 1083068288.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 943  | total loss: \u001b[1m\u001b[32m974968064.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 943 | loss: 974968064.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 944  | total loss: \u001b[1m\u001b[32m950218560.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 944 | loss: 950218560.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 945  | total loss: \u001b[1m\u001b[32m855403328.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 945 | loss: 855403328.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 946  | total loss: \u001b[1m\u001b[32m864468672.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 946 | loss: 864468672.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 947  | total loss: \u001b[1m\u001b[32m778228416.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 947 | loss: 778228416.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 948  | total loss: \u001b[1m\u001b[32m821771968.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 948 | loss: 821771968.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 949  | total loss: \u001b[1m\u001b[32m739801408.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 949 | loss: 739801408.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 950  | total loss: \u001b[1m\u001b[32m1748887808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 950 | loss: 1748887808.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 951  | total loss: \u001b[1m\u001b[32m1574205568.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 951 | loss: 1574205568.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 952  | total loss: \u001b[1m\u001b[32m1510379392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 952 | loss: 1510379392.00000 - R2: 1.0013 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 953  | total loss: \u001b[1m\u001b[32m1359548032.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 953 | loss: 1359548032.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 954  | total loss: \u001b[1m\u001b[32m1488312832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 954 | loss: 1488312832.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 955  | total loss: \u001b[1m\u001b[32m1339688192.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 955 | loss: 1339688192.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 956  | total loss: \u001b[1m\u001b[32m1289713280.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 956 | loss: 1289713280.00000 - R2: 1.0021 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 957  | total loss: \u001b[1m\u001b[32m1160948608.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 957 | loss: 1160948608.00000 - R2: 1.0018 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 958  | total loss: \u001b[1m\u001b[32m1323799040.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 958 | loss: 1323799040.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 959  | total loss: \u001b[1m\u001b[32m1191625728.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 959 | loss: 1191625728.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 960  | total loss: \u001b[1m\u001b[32m2219255808.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 960 | loss: 2219255808.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 961  | total loss: \u001b[1m\u001b[32m1997536768.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 961 | loss: 1997536768.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 962  | total loss: \u001b[1m\u001b[32m2091213184.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 962 | loss: 2091213184.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 963  | total loss: \u001b[1m\u001b[32m1882298496.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 963 | loss: 1882298496.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 964  | total loss: \u001b[1m\u001b[32m1776903552.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 964 | loss: 1776903552.00000 - R2: 0.9956 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 965  | total loss: \u001b[1m\u001b[32m1599419776.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 965 | loss: 1599419776.00000 - R2: 0.9959 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 966  | total loss: \u001b[1m\u001b[32m1473235712.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 966 | loss: 1473235712.00000 - R2: 0.9960 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 967  | total loss: \u001b[1m\u001b[32m1326118784.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 967 | loss: 1326118784.00000 - R2: 0.9963 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 968  | total loss: \u001b[1m\u001b[32m1268799744.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 968 | loss: 1268799744.00000 - R2: 0.9951 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 969  | total loss: \u001b[1m\u001b[32m1142126336.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 969 | loss: 1142126336.00000 - R2: 0.9955 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 970  | total loss: \u001b[1m\u001b[32m1424436096.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 970 | loss: 1424436096.00000 - R2: 0.9944 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 971  | total loss: \u001b[1m\u001b[32m1282199040.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 971 | loss: 1282199040.00000 - R2: 0.9949 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 972  | total loss: \u001b[1m\u001b[32m1498688384.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 972 | loss: 1498688384.00000 - R2: 0.9951 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 973  | total loss: \u001b[1m\u001b[32m1349026176.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 973 | loss: 1349026176.00000 - R2: 0.9955 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 974  | total loss: \u001b[1m\u001b[32m1283458688.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 974 | loss: 1283458688.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 975  | total loss: \u001b[1m\u001b[32m1155319424.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 975 | loss: 1155319424.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 976  | total loss: \u001b[1m\u001b[32m1103716864.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 976 | loss: 1103716864.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 977  | total loss: \u001b[1m\u001b[32m993551808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 977 | loss: 993551808.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 978  | total loss: \u001b[1m\u001b[32m980340288.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 978 | loss: 980340288.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 979  | total loss: \u001b[1m\u001b[32m882512896.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 979 | loss: 882512896.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 980  | total loss: \u001b[1m\u001b[32m879618816.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 980 | loss: 879618816.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 981  | total loss: \u001b[1m\u001b[32m791863552.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 981 | loss: 791863552.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 982  | total loss: \u001b[1m\u001b[32m795660224.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 982 | loss: 795660224.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 983  | total loss: \u001b[1m\u001b[32m716300800.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 983 | loss: 716300800.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 984  | total loss: \u001b[1m\u001b[32m732359744.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 984 | loss: 732359744.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 985  | total loss: \u001b[1m\u001b[32m659330368.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 985 | loss: 659330368.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 986  | total loss: \u001b[1m\u001b[32m890382464.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 986 | loss: 890382464.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 987  | total loss: \u001b[1m\u001b[32m801550848.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 987 | loss: 801550848.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 988  | total loss: \u001b[1m\u001b[32m808224960.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 988 | loss: 808224960.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 989  | total loss: \u001b[1m\u001b[32m727609088.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 989 | loss: 727609088.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 990  | total loss: \u001b[1m\u001b[32m751465792.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 990 | loss: 751465792.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 991  | total loss: \u001b[1m\u001b[32m676525824.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 991 | loss: 676525824.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 992  | total loss: \u001b[1m\u001b[32m862697216.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 992 | loss: 862697216.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 993  | total loss: \u001b[1m\u001b[32m776634112.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 993 | loss: 776634112.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 994  | total loss: \u001b[1m\u001b[32m951676480.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 994 | loss: 951676480.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 995  | total loss: \u001b[1m\u001b[32m856715456.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 995 | loss: 856715456.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 996  | total loss: \u001b[1m\u001b[32m837850816.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 996 | loss: 837850816.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 997  | total loss: \u001b[1m\u001b[32m754272320.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 997 | loss: 754272320.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 998  | total loss: \u001b[1m\u001b[32m757600832.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 998 | loss: 757600832.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 999  | total loss: \u001b[1m\u001b[32m682047360.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 999 | loss: 682047360.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1000  | total loss: \u001b[1m\u001b[32m706759168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1000 | loss: 706759168.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1001  | total loss: \u001b[1m\u001b[32m636289856.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1001 | loss: 636289856.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1002  | total loss: \u001b[1m\u001b[32m991499712.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1002 | loss: 991499712.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1003  | total loss: \u001b[1m\u001b[32m892556352.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1003 | loss: 892556352.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1004  | total loss: \u001b[1m\u001b[32m984725696.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1004 | loss: 984725696.00000 - R2: 1.0013 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1005  | total loss: \u001b[1m\u001b[32m886459712.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1005 | loss: 886459712.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1006  | total loss: \u001b[1m\u001b[32m908570944.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1006 | loss: 908570944.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1007  | total loss: \u001b[1m\u001b[32m817920448.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1007 | loss: 817920448.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1008  | total loss: \u001b[1m\u001b[32m814023872.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1008 | loss: 814023872.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1009  | total loss: \u001b[1m\u001b[32m732828096.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1009 | loss: 732828096.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1010  | total loss: \u001b[1m\u001b[32m972278656.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1010 | loss: 972278656.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1011  | total loss: \u001b[1m\u001b[32m875257408.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1011 | loss: 875257408.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1012  | total loss: \u001b[1m\u001b[32m905217472.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1012 | loss: 905217472.00000 - R2: 0.9962 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1013  | total loss: \u001b[1m\u001b[32m814902336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1013 | loss: 814902336.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1014  | total loss: \u001b[1m\u001b[32m792388096.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1014 | loss: 792388096.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1015  | total loss: \u001b[1m\u001b[32m713355904.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1015 | loss: 713355904.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1016  | total loss: \u001b[1m\u001b[32m924353216.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1016 | loss: 924353216.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1017  | total loss: \u001b[1m\u001b[32m832124480.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1017 | loss: 832124480.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1018  | total loss: \u001b[1m\u001b[32m1022009408.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1018 | loss: 1022009408.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1019  | total loss: \u001b[1m\u001b[32m920015104.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1019 | loss: 920015104.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1020  | total loss: \u001b[1m\u001b[32m969776832.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1020 | loss: 969776832.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1021  | total loss: \u001b[1m\u001b[32m873005760.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1021 | loss: 873005760.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1022  | total loss: \u001b[1m\u001b[32m910908544.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1022 | loss: 910908544.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1023  | total loss: \u001b[1m\u001b[32m820024320.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1023 | loss: 820024320.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1024  | total loss: \u001b[1m\u001b[32m1026514496.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1024 | loss: 1026514496.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1025  | total loss: \u001b[1m\u001b[32m924069632.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1025 | loss: 924069632.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1026  | total loss: \u001b[1m\u001b[32m906296256.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1026 | loss: 906296256.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1027  | total loss: \u001b[1m\u001b[32m815873216.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1027 | loss: 815873216.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1028  | total loss: \u001b[1m\u001b[32m1009581824.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1028 | loss: 1009581824.00000 - R2: 0.9958 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1029  | total loss: \u001b[1m\u001b[32m908830272.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1029 | loss: 908830272.00000 - R2: 0.9961 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1030  | total loss: \u001b[1m\u001b[32m1088328704.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1030 | loss: 1088328704.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1031  | total loss: \u001b[1m\u001b[32m979702464.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1031 | loss: 979702464.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1032  | total loss: \u001b[1m\u001b[32m1229670912.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1032 | loss: 1229670912.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1033  | total loss: \u001b[1m\u001b[32m1106910464.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1033 | loss: 1106910464.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1034  | total loss: \u001b[1m\u001b[32m1105568896.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1034 | loss: 1105568896.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1035  | total loss: \u001b[1m\u001b[32m995218624.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1035 | loss: 995218624.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1036  | total loss: \u001b[1m\u001b[32m953398592.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1036 | loss: 953398592.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1037  | total loss: \u001b[1m\u001b[32m858265344.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1037 | loss: 858265344.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1038  | total loss: \u001b[1m\u001b[32m847394752.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1038 | loss: 847394752.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1039  | total loss: \u001b[1m\u001b[32m762861888.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1039 | loss: 762861888.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1040  | total loss: \u001b[1m\u001b[32m781688000.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1040 | loss: 781688000.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1041  | total loss: \u001b[1m\u001b[32m703725824.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1041 | loss: 703725824.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1042  | total loss: \u001b[1m\u001b[32m982140288.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1042 | loss: 982140288.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1043  | total loss: \u001b[1m\u001b[32m884132864.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1043 | loss: 884132864.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1044  | total loss: \u001b[1m\u001b[32m1147469696.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1044 | loss: 1147469696.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1045  | total loss: \u001b[1m\u001b[32m1032929344.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1045 | loss: 1032929344.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1046  | total loss: \u001b[1m\u001b[32m1251450240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1046 | loss: 1251450240.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1047  | total loss: \u001b[1m\u001b[32m1126511872.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1047 | loss: 1126511872.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1048  | total loss: \u001b[1m\u001b[32m1110807296.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1048 | loss: 1110807296.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1049  | total loss: \u001b[1m\u001b[32m999933184.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1049 | loss: 999933184.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1050  | total loss: \u001b[1m\u001b[32m965330752.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1050 | loss: 965330752.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1051  | total loss: \u001b[1m\u001b[32m869004288.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 1051 | loss: 869004288.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1052  | total loss: \u001b[1m\u001b[32m865556032.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1052 | loss: 865556032.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1053  | total loss: \u001b[1m\u001b[32m779207040.00000\u001b[0m\u001b[0m | time: 1.015s\n",
      "| SGD | epoch: 1053 | loss: 779207040.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1054  | total loss: \u001b[1m\u001b[32m1961734656.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1054 | loss: 1961734656.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1055  | total loss: \u001b[1m\u001b[32m1765767808.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1055 | loss: 1765767808.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1056  | total loss: \u001b[1m\u001b[32m1688633216.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1056 | loss: 1688633216.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1057  | total loss: \u001b[1m\u001b[32m1519976448.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1057 | loss: 1519976448.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1058  | total loss: \u001b[1m\u001b[32m1454112768.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1058 | loss: 1454112768.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1059  | total loss: \u001b[1m\u001b[32m1308908032.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1059 | loss: 1308908032.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1060  | total loss: \u001b[1m\u001b[32m1420182528.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1060 | loss: 1420182528.00000 - R2: 0.9960 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1061  | total loss: \u001b[1m\u001b[32m1278370816.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1061 | loss: 1278370816.00000 - R2: 0.9963 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1062  | total loss: \u001b[1m\u001b[32m1235019904.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1062 | loss: 1235019904.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1063  | total loss: \u001b[1m\u001b[32m1111724544.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1063 | loss: 1111724544.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1064  | total loss: \u001b[1m\u001b[32m1089301760.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1064 | loss: 1089301760.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1065  | total loss: \u001b[1m\u001b[32m980578176.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1065 | loss: 980578176.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1066  | total loss: \u001b[1m\u001b[32m943132480.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1066 | loss: 943132480.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1067  | total loss: \u001b[1m\u001b[32m849025856.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1067 | loss: 849025856.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1068  | total loss: \u001b[1m\u001b[32m854471424.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1068 | loss: 854471424.00000 - R2: 0.9957 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1069  | total loss: \u001b[1m\u001b[32m769230912.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1069 | loss: 769230912.00000 - R2: 0.9960 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1070  | total loss: \u001b[1m\u001b[32m781356096.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1070 | loss: 781356096.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1071  | total loss: \u001b[1m\u001b[32m703427072.00000\u001b[0m\u001b[0m | time: 1.032s\n",
      "| SGD | epoch: 1071 | loss: 703427072.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1072  | total loss: \u001b[1m\u001b[32m1045557888.00000\u001b[0m\u001b[0m | time: 1.014s\n",
      "| SGD | epoch: 1072 | loss: 1045557888.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1073  | total loss: \u001b[1m\u001b[32m941208704.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1073 | loss: 941208704.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1074  | total loss: \u001b[1m\u001b[32m910476736.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1074 | loss: 910476736.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1075  | total loss: \u001b[1m\u001b[32m819635648.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1075 | loss: 819635648.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1076  | total loss: \u001b[1m\u001b[32m992235264.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1076 | loss: 992235264.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1077  | total loss: \u001b[1m\u001b[32m893218368.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1077 | loss: 893218368.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1078  | total loss: \u001b[1m\u001b[32m1126343936.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1078 | loss: 1126343936.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1079  | total loss: \u001b[1m\u001b[32m1013916160.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1079 | loss: 1013916160.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1080  | total loss: \u001b[1m\u001b[32m983594048.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1080 | loss: 983594048.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1081  | total loss: \u001b[1m\u001b[32m885441280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1081 | loss: 885441280.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1082  | total loss: \u001b[1m\u001b[32m1137072384.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1082 | loss: 1137072384.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1083  | total loss: \u001b[1m\u001b[32m1023571776.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1083 | loss: 1023571776.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1084  | total loss: \u001b[1m\u001b[32m1051215936.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1084 | loss: 1051215936.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1085  | total loss: \u001b[1m\u001b[32m946300928.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1085 | loss: 946300928.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1086  | total loss: \u001b[1m\u001b[32m963563584.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1086 | loss: 963563584.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1087  | total loss: \u001b[1m\u001b[32m867413824.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1087 | loss: 867413824.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1088  | total loss: \u001b[1m\u001b[32m1138197248.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1088 | loss: 1138197248.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1089  | total loss: \u001b[1m\u001b[32m1024584128.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1089 | loss: 1024584128.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1090  | total loss: \u001b[1m\u001b[32m922332288.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1090 | loss: 922332288.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1091  | total loss: \u001b[1m\u001b[32m830305664.00000\u001b[0m\u001b[0m | time: 1.026s\n",
      "| SGD | epoch: 1091 | loss: 830305664.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1092  | total loss: \u001b[1m\u001b[32m1178011008.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1092 | loss: 1178011008.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1093  | total loss: \u001b[1m\u001b[32m1060416512.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1093 | loss: 1060416512.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1094  | total loss: \u001b[1m\u001b[32m1002198848.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1094 | loss: 1002198848.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1095  | total loss: \u001b[1m\u001b[32m902185600.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1095 | loss: 902185600.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1096  | total loss: \u001b[1m\u001b[32m892046336.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1096 | loss: 892046336.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1097  | total loss: \u001b[1m\u001b[32m803048320.00000\u001b[0m\u001b[0m | time: 1.014s\n",
      "| SGD | epoch: 1097 | loss: 803048320.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1098  | total loss: \u001b[1m\u001b[32m722950080.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1098 | loss: 722950080.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1099  | total loss: \u001b[1m\u001b[32m650861696.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1099 | loss: 650861696.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1100  | total loss: \u001b[1m\u001b[32m731293312.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1100 | loss: 731293312.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1101  | total loss: \u001b[1m\u001b[32m658370560.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1101 | loss: 658370560.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1102  | total loss: \u001b[1m\u001b[32m647246080.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1102 | loss: 647246080.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1103  | total loss: \u001b[1m\u001b[32m582728064.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1103 | loss: 582728064.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1104  | total loss: \u001b[1m\u001b[32m604685888.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1104 | loss: 604685888.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1105  | total loss: \u001b[1m\u001b[32m544423936.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1105 | loss: 544423936.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1106  | total loss: \u001b[1m\u001b[32m490188160.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1106 | loss: 490188160.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1107  | total loss: \u001b[1m\u001b[32m441375968.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1107 | loss: 441375968.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1108  | total loss: \u001b[1m\u001b[32m468461216.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1108 | loss: 468461216.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1109  | total loss: \u001b[1m\u001b[32m421821696.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1109 | loss: 421821696.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1110  | total loss: \u001b[1m\u001b[32m734945984.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1110 | loss: 734945984.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1111  | total loss: \u001b[1m\u001b[32m661657984.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1111 | loss: 661657984.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1112  | total loss: \u001b[1m\u001b[32m706954880.00000\u001b[0m\u001b[0m | time: 1.036s\n",
      "| SGD | epoch: 1112 | loss: 706954880.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1113  | total loss: \u001b[1m\u001b[32m636465984.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1113 | loss: 636465984.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1114  | total loss: \u001b[1m\u001b[32m683504000.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1114 | loss: 683504000.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1115  | total loss: \u001b[1m\u001b[32m615360192.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1115 | loss: 615360192.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1116  | total loss: \u001b[1m\u001b[32m787338368.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1116 | loss: 787338368.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1117  | total loss: \u001b[1m\u001b[32m708811136.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1117 | loss: 708811136.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1118  | total loss: \u001b[1m\u001b[32m879445248.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1118 | loss: 879445248.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1119  | total loss: \u001b[1m\u001b[32m791707328.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1119 | loss: 791707328.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1120  | total loss: \u001b[1m\u001b[32m1036135744.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1120 | loss: 1036135744.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1121  | total loss: \u001b[1m\u001b[32m932728768.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1121 | loss: 932728768.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1122  | total loss: \u001b[1m\u001b[32m948063808.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1122 | loss: 948063808.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1123  | total loss: \u001b[1m\u001b[32m853464064.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1123 | loss: 853464064.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1124  | total loss: \u001b[1m\u001b[32m844536832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1124 | loss: 844536832.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1125  | total loss: \u001b[1m\u001b[32m760289792.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1125 | loss: 760289792.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1126  | total loss: \u001b[1m\u001b[32m941605568.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1126 | loss: 941605568.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1127  | total loss: \u001b[1m\u001b[32m847651584.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1127 | loss: 847651584.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1128  | total loss: \u001b[1m\u001b[32m876863104.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1128 | loss: 876863104.00000 - R2: 0.9963 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1129  | total loss: \u001b[1m\u001b[32m789383424.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1129 | loss: 789383424.00000 - R2: 0.9966 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1130  | total loss: \u001b[1m\u001b[32m841240256.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1130 | loss: 841240256.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1131  | total loss: \u001b[1m\u001b[32m757322816.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1131 | loss: 757322816.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1132  | total loss: \u001b[1m\u001b[32m681797120.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1132 | loss: 681797120.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1133  | total loss: \u001b[1m\u001b[32m613824000.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1133 | loss: 613824000.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1134  | total loss: \u001b[1m\u001b[32m1544809472.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1134 | loss: 1544809472.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1135  | total loss: \u001b[1m\u001b[32m1390535168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1135 | loss: 1390535168.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1136  | total loss: \u001b[1m\u001b[32m1482882688.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1136 | loss: 1482882688.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1137  | total loss: \u001b[1m\u001b[32m1334801024.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1137 | loss: 1334801024.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1138  | total loss: \u001b[1m\u001b[32m1313870336.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1138 | loss: 1313870336.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1139  | total loss: \u001b[1m\u001b[32m1182689920.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1139 | loss: 1182689920.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1140  | total loss: \u001b[1m\u001b[32m1132512640.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1140 | loss: 1132512640.00000 - R2: 0.9963 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1141  | total loss: \u001b[1m\u001b[32m1019467968.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1141 | loss: 1019467968.00000 - R2: 0.9966 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1142  | total loss: \u001b[1m\u001b[32m1123377152.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1142 | loss: 1123377152.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1143  | total loss: \u001b[1m\u001b[32m1011246016.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1143 | loss: 1011246016.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1144  | total loss: \u001b[1m\u001b[32m1023588928.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1144 | loss: 1023588928.00000 - R2: 0.9961 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1145  | total loss: \u001b[1m\u001b[32m921436672.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1145 | loss: 921436672.00000 - R2: 0.9963 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1146  | total loss: \u001b[1m\u001b[32m954830080.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1146 | loss: 954830080.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1147  | total loss: \u001b[1m\u001b[32m859553664.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1147 | loss: 859553664.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1148  | total loss: \u001b[1m\u001b[32m854175488.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1148 | loss: 854175488.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1149  | total loss: \u001b[1m\u001b[32m768964544.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1149 | loss: 768964544.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1150  | total loss: \u001b[1m\u001b[32m857600960.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1150 | loss: 857600960.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1151  | total loss: \u001b[1m\u001b[32m772047488.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1151 | loss: 772047488.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1152  | total loss: \u001b[1m\u001b[32m1005058560.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1152 | loss: 1005058560.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1153  | total loss: \u001b[1m\u001b[32m904759296.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1153 | loss: 904759296.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1154  | total loss: \u001b[1m\u001b[32m914707072.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1154 | loss: 914707072.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1155  | total loss: \u001b[1m\u001b[32m823442944.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1155 | loss: 823442944.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1156  | total loss: \u001b[1m\u001b[32m853213568.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1156 | loss: 853213568.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1157  | total loss: \u001b[1m\u001b[32m768098816.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1157 | loss: 768098816.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1158  | total loss: \u001b[1m\u001b[32m770662336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1158 | loss: 770662336.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1159  | total loss: \u001b[1m\u001b[32m693802688.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1159 | loss: 693802688.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1160  | total loss: \u001b[1m\u001b[32m1005621760.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1160 | loss: 1005621760.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1161  | total loss: \u001b[1m\u001b[32m905266176.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1161 | loss: 905266176.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1162  | total loss: \u001b[1m\u001b[32m1171007616.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1162 | loss: 1171007616.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1163  | total loss: \u001b[1m\u001b[32m1054113472.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1163 | loss: 1054113472.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1164  | total loss: \u001b[1m\u001b[32m1394397184.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1164 | loss: 1394397184.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1165  | total loss: \u001b[1m\u001b[32m1255164032.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1165 | loss: 1255164032.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1166  | total loss: \u001b[1m\u001b[32m1471064192.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1166 | loss: 1471064192.00000 - R2: 1.0019 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1167  | total loss: \u001b[1m\u001b[32m1324164352.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1167 | loss: 1324164352.00000 - R2: 1.0016 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1168  | total loss: \u001b[1m\u001b[32m1440318720.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1168 | loss: 1440318720.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1169  | total loss: \u001b[1m\u001b[32m1296493440.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1169 | loss: 1296493440.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1170  | total loss: \u001b[1m\u001b[32m1248391424.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1170 | loss: 1248391424.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1171  | total loss: \u001b[1m\u001b[32m1123758848.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1171 | loss: 1123758848.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1172  | total loss: \u001b[1m\u001b[32m1341411456.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1172 | loss: 1341411456.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1173  | total loss: \u001b[1m\u001b[32m1207476864.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1173 | loss: 1207476864.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1174  | total loss: \u001b[1m\u001b[32m1174973184.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1174 | loss: 1174973184.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1175  | total loss: \u001b[1m\u001b[32m1057682496.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1175 | loss: 1057682496.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1176  | total loss: \u001b[1m\u001b[32m1271010304.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1176 | loss: 1271010304.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1177  | total loss: \u001b[1m\u001b[32m1144115840.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1177 | loss: 1144115840.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1178  | total loss: \u001b[1m\u001b[32m1367717760.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1178 | loss: 1367717760.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1179  | total loss: \u001b[1m\u001b[32m1231152640.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1179 | loss: 1231152640.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1180  | total loss: \u001b[1m\u001b[32m1486481536.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1180 | loss: 1486481536.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1181  | total loss: \u001b[1m\u001b[32m1338039936.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1181 | loss: 1338039936.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1182  | total loss: \u001b[1m\u001b[32m1541685888.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1182 | loss: 1541685888.00000 - R2: 1.0033 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1183  | total loss: \u001b[1m\u001b[32m1387723904.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1183 | loss: 1387723904.00000 - R2: 1.0028 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1184  | total loss: \u001b[1m\u001b[32m1314615552.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1184 | loss: 1314615552.00000 - R2: 1.0018 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1185  | total loss: \u001b[1m\u001b[32m1183360640.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1185 | loss: 1183360640.00000 - R2: 1.0015 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1186  | total loss: \u001b[1m\u001b[32m1125651072.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1186 | loss: 1125651072.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1187  | total loss: \u001b[1m\u001b[32m1013292544.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1187 | loss: 1013292544.00000 - R2: 1.0012 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1188  | total loss: \u001b[1m\u001b[32m1175381120.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1188 | loss: 1175381120.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1189  | total loss: \u001b[1m\u001b[32m1058049600.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1189 | loss: 1058049600.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1190  | total loss: \u001b[1m\u001b[32m1046572928.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1190 | loss: 1046572928.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1191  | total loss: \u001b[1m\u001b[32m942122240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1191 | loss: 942122240.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1192  | total loss: \u001b[1m\u001b[32m1184262400.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1192 | loss: 1184262400.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1193  | total loss: \u001b[1m\u001b[32m1066042752.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1193 | loss: 1066042752.00000 - R2: 1.0015 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1194  | total loss: \u001b[1m\u001b[32m1041122368.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1194 | loss: 1041122368.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1195  | total loss: \u001b[1m\u001b[32m937216768.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1195 | loss: 937216768.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1196  | total loss: \u001b[1m\u001b[32m906372160.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1196 | loss: 906372160.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1197  | total loss: \u001b[1m\u001b[32m815941568.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1197 | loss: 815941568.00000 - R2: 1.0012 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1198  | total loss: \u001b[1m\u001b[32m840472384.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1198 | loss: 840472384.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1199  | total loss: \u001b[1m\u001b[32m756631744.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1199 | loss: 756631744.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1200  | total loss: \u001b[1m\u001b[32m805953216.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1200 | loss: 805953216.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1201  | total loss: \u001b[1m\u001b[32m725564480.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1201 | loss: 725564480.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1202  | total loss: \u001b[1m\u001b[32m967797568.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1202 | loss: 967797568.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1203  | total loss: \u001b[1m\u001b[32m871224448.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1203 | loss: 871224448.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1204  | total loss: \u001b[1m\u001b[32m1805093376.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1204 | loss: 1805093376.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1205  | total loss: \u001b[1m\u001b[32m1624790656.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1205 | loss: 1624790656.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1206  | total loss: \u001b[1m\u001b[32m1544954368.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1206 | loss: 1544954368.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1207  | total loss: \u001b[1m\u001b[32m1390665472.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1207 | loss: 1390665472.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1208  | total loss: \u001b[1m\u001b[32m1335484160.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1208 | loss: 1335484160.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1209  | total loss: \u001b[1m\u001b[32m1202142336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1209 | loss: 1202142336.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1210  | total loss: \u001b[1m\u001b[32m1186868480.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1210 | loss: 1186868480.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1211  | total loss: \u001b[1m\u001b[32m1068388224.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1211 | loss: 1068388224.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1212  | total loss: \u001b[1m\u001b[32m1026707840.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1212 | loss: 1026707840.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1213  | total loss: \u001b[1m\u001b[32m924243648.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1213 | loss: 924243648.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1214  | total loss: \u001b[1m\u001b[32m931263104.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1214 | loss: 931263104.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1215  | total loss: \u001b[1m\u001b[32m838343424.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1215 | loss: 838343424.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1216  | total loss: \u001b[1m\u001b[32m1136451968.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1216 | loss: 1136451968.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1217  | total loss: \u001b[1m\u001b[32m1023013376.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 1217 | loss: 1023013376.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1218  | total loss: \u001b[1m\u001b[32m1351583232.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1218 | loss: 1351583232.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1219  | total loss: \u001b[1m\u001b[32m1216631552.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1219 | loss: 1216631552.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1220  | total loss: \u001b[1m\u001b[32m1416442496.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1220 | loss: 1416442496.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1221  | total loss: \u001b[1m\u001b[32m1275004800.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1221 | loss: 1275004800.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1222  | total loss: \u001b[1m\u001b[32m1568383232.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1222 | loss: 1568383232.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1223  | total loss: \u001b[1m\u001b[32m1411751552.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1223 | loss: 1411751552.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1224  | total loss: \u001b[1m\u001b[32m1322585600.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1224 | loss: 1322585600.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1225  | total loss: \u001b[1m\u001b[32m1190533632.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1225 | loss: 1190533632.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1226  | total loss: \u001b[1m\u001b[32m1170406912.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1226 | loss: 1170406912.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1227  | total loss: \u001b[1m\u001b[32m1053572800.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1227 | loss: 1053572800.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1228  | total loss: \u001b[1m\u001b[32m1013920128.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1228 | loss: 1013920128.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1229  | total loss: \u001b[1m\u001b[32m912734720.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1229 | loss: 912734720.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1230  | total loss: \u001b[1m\u001b[32m906329472.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1230 | loss: 906329472.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1231  | total loss: \u001b[1m\u001b[32m815903104.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1231 | loss: 815903104.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1232  | total loss: \u001b[1m\u001b[32m985969216.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1232 | loss: 985969216.00000 - R2: 1.0042 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1233  | total loss: \u001b[1m\u001b[32m887578880.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1233 | loss: 887578880.00000 - R2: 1.0037 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1234  | total loss: \u001b[1m\u001b[32m886255424.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1234 | loss: 886255424.00000 - R2: 1.0029 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1235  | total loss: \u001b[1m\u001b[32m797836480.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1235 | loss: 797836480.00000 - R2: 1.0025 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1236  | total loss: \u001b[1m\u001b[32m1102451456.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1236 | loss: 1102451456.00000 - R2: 1.0016 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1237  | total loss: \u001b[1m\u001b[32m992412928.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1237 | loss: 992412928.00000 - R2: 1.0013 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1238  | total loss: \u001b[1m\u001b[32m1148171904.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1238 | loss: 1148171904.00000 - R2: 1.0016 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1239  | total loss: \u001b[1m\u001b[32m1033561344.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1239 | loss: 1033561344.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1240  | total loss: \u001b[1m\u001b[32m1025143936.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1240 | loss: 1025143936.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1241  | total loss: \u001b[1m\u001b[32m922836160.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1241 | loss: 922836160.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1242  | total loss: \u001b[1m\u001b[32m903089472.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1242 | loss: 903089472.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1243  | total loss: \u001b[1m\u001b[32m812987136.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1243 | loss: 812987136.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1244  | total loss: \u001b[1m\u001b[32m804978496.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1244 | loss: 804978496.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1245  | total loss: \u001b[1m\u001b[32m724687232.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1245 | loss: 724687232.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1246  | total loss: \u001b[1m\u001b[32m940577792.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1246 | loss: 940577792.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1247  | total loss: \u001b[1m\u001b[32m846726656.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1247 | loss: 846726656.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1248  | total loss: \u001b[1m\u001b[32m847961792.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 1248 | loss: 847961792.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1249  | total loss: \u001b[1m\u001b[32m763372224.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1249 | loss: 763372224.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1250  | total loss: \u001b[1m\u001b[32m783955904.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1250 | loss: 783955904.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1251  | total loss: \u001b[1m\u001b[32m705766912.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1251 | loss: 705766912.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1252  | total loss: \u001b[1m\u001b[32m863330560.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1252 | loss: 863330560.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1253  | total loss: \u001b[1m\u001b[32m777204096.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1253 | loss: 777204096.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1254  | total loss: \u001b[1m\u001b[32m1011906112.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1254 | loss: 1011906112.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1255  | total loss: \u001b[1m\u001b[32m910922112.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1255 | loss: 910922112.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1256  | total loss: \u001b[1m\u001b[32m922763136.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1256 | loss: 922763136.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1257  | total loss: \u001b[1m\u001b[32m830693440.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1257 | loss: 830693440.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1258  | total loss: \u001b[1m\u001b[32m839509312.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1258 | loss: 839509312.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1259  | total loss: \u001b[1m\u001b[32m755764992.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1259 | loss: 755764992.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1260  | total loss: \u001b[1m\u001b[32m820073984.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1260 | loss: 820073984.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1261  | total loss: \u001b[1m\u001b[32m738273216.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1261 | loss: 738273216.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1262  | total loss: \u001b[1m\u001b[32m786240832.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1262 | loss: 786240832.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1263  | total loss: \u001b[1m\u001b[32m707823360.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1263 | loss: 707823360.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1264  | total loss: \u001b[1m\u001b[32m1708158336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1264 | loss: 1708158336.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1265  | total loss: \u001b[1m\u001b[32m1537549056.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1265 | loss: 1537549056.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1266  | total loss: \u001b[1m\u001b[32m1647162624.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1266 | loss: 1647162624.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1267  | total loss: \u001b[1m\u001b[32m1482652928.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1267 | loss: 1482652928.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1268  | total loss: \u001b[1m\u001b[32m1419930240.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1268 | loss: 1419930240.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1269  | total loss: \u001b[1m\u001b[32m1278143744.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1269 | loss: 1278143744.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1270  | total loss: \u001b[1m\u001b[32m1258381952.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1270 | loss: 1258381952.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1271  | total loss: \u001b[1m\u001b[32m1132750336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1271 | loss: 1132750336.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1272  | total loss: \u001b[1m\u001b[32m1102178048.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1272 | loss: 1102178048.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1273  | total loss: \u001b[1m\u001b[32m992166848.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1273 | loss: 992166848.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1274  | total loss: \u001b[1m\u001b[32m1169569152.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1274 | loss: 1169569152.00000 - R2: 0.9957 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1275  | total loss: \u001b[1m\u001b[32m1052818816.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1275 | loss: 1052818816.00000 - R2: 0.9960 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1276  | total loss: \u001b[1m\u001b[32m1003081472.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1276 | loss: 1003081472.00000 - R2: 0.9950 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1277  | total loss: \u001b[1m\u001b[32m902979904.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1277 | loss: 902979904.00000 - R2: 0.9954 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1278  | total loss: \u001b[1m\u001b[32m877202368.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1278 | loss: 877202368.00000 - R2: 0.9963 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1279  | total loss: \u001b[1m\u001b[32m789688768.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1279 | loss: 789688768.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1280  | total loss: \u001b[1m\u001b[32m786075520.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1280 | loss: 786075520.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1281  | total loss: \u001b[1m\u001b[32m707674560.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1281 | loss: 707674560.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1282  | total loss: \u001b[1m\u001b[32m694206336.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1282 | loss: 694206336.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1283  | total loss: \u001b[1m\u001b[32m624992320.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1283 | loss: 624992320.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1284  | total loss: \u001b[1m\u001b[32m805284928.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1284 | loss: 805284928.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1285  | total loss: \u001b[1m\u001b[32m724963072.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1285 | loss: 724963072.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1286  | total loss: \u001b[1m\u001b[32m742199104.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1286 | loss: 742199104.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1287  | total loss: \u001b[1m\u001b[32m668185792.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1287 | loss: 668185792.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1288  | total loss: \u001b[1m\u001b[32m923689664.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1288 | loss: 923689664.00000 - R2: 0.9960 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1289  | total loss: \u001b[1m\u001b[32m831527296.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1289 | loss: 831527296.00000 - R2: 0.9963 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1290  | total loss: \u001b[1m\u001b[32m1123285888.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1290 | loss: 1123285888.00000 - R2: 0.9955 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1291  | total loss: \u001b[1m\u001b[32m1011163904.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1291 | loss: 1011163904.00000 - R2: 0.9959 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1292  | total loss: \u001b[1m\u001b[32m961983168.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1292 | loss: 961983168.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1293  | total loss: \u001b[1m\u001b[32m865991424.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1293 | loss: 865991424.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1294  | total loss: \u001b[1m\u001b[32m1074909440.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1294 | loss: 1074909440.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1295  | total loss: \u001b[1m\u001b[32m967625088.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1295 | loss: 967625088.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1296  | total loss: \u001b[1m\u001b[32m913115840.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1296 | loss: 913115840.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1297  | total loss: \u001b[1m\u001b[32m822010880.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1297 | loss: 822010880.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1298  | total loss: \u001b[1m\u001b[32m850518656.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1298 | loss: 850518656.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1299  | total loss: \u001b[1m\u001b[32m765673408.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1299 | loss: 765673408.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1300  | total loss: \u001b[1m\u001b[32m956360960.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1300 | loss: 956360960.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1301  | total loss: \u001b[1m\u001b[32m860931456.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1301 | loss: 860931456.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1302  | total loss: \u001b[1m\u001b[32m1094062592.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1302 | loss: 1094062592.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1303  | total loss: \u001b[1m\u001b[32m984862912.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1303 | loss: 984862912.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1304  | total loss: \u001b[1m\u001b[32m1288717568.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1304 | loss: 1288717568.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1305  | total loss: \u001b[1m\u001b[32m1160052352.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1305 | loss: 1160052352.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1306  | total loss: \u001b[1m\u001b[32m1130149760.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1306 | loss: 1130149760.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1307  | total loss: \u001b[1m\u001b[32m1017341376.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1307 | loss: 1017341376.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1308  | total loss: \u001b[1m\u001b[32m1035967744.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1308 | loss: 1035967744.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1309  | total loss: \u001b[1m\u001b[32m932577600.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1309 | loss: 932577600.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1310  | total loss: \u001b[1m\u001b[32m941102400.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1310 | loss: 941102400.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1311  | total loss: \u001b[1m\u001b[32m847198784.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1311 | loss: 847198784.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1312  | total loss: \u001b[1m\u001b[32m844736576.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1312 | loss: 844736576.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1313  | total loss: \u001b[1m\u001b[32m760469504.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 1313 | loss: 760469504.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1314  | total loss: \u001b[1m\u001b[32m912330880.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1314 | loss: 912330880.00000 - R2: 1.0021 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1315  | total loss: \u001b[1m\u001b[32m821304384.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1315 | loss: 821304384.00000 - R2: 1.0018 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1316  | total loss: \u001b[1m\u001b[32m822070272.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1316 | loss: 822070272.00000 - R2: 1.0026 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1317  | total loss: \u001b[1m\u001b[32m740069888.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1317 | loss: 740069888.00000 - R2: 1.0023 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1318  | total loss: \u001b[1m\u001b[32m864629120.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1318 | loss: 864629120.00000 - R2: 1.0033 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1319  | total loss: \u001b[1m\u001b[32m778372800.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1319 | loss: 778372800.00000 - R2: 1.0029 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1320  | total loss: \u001b[1m\u001b[32m949241536.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1320 | loss: 949241536.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1321  | total loss: \u001b[1m\u001b[32m854523968.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1321 | loss: 854523968.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1322  | total loss: \u001b[1m\u001b[32m860923392.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1322 | loss: 860923392.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1323  | total loss: \u001b[1m\u001b[32m775037696.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1323 | loss: 775037696.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1324  | total loss: \u001b[1m\u001b[32m772885056.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1324 | loss: 772885056.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1325  | total loss: \u001b[1m\u001b[32m695803136.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1325 | loss: 695803136.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1326  | total loss: \u001b[1m\u001b[32m930863488.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1326 | loss: 930863488.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1327  | total loss: \u001b[1m\u001b[32m837983744.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1327 | loss: 837983744.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1328  | total loss: \u001b[1m\u001b[32m1121435904.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1328 | loss: 1121435904.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1329  | total loss: \u001b[1m\u001b[32m1009498944.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1329 | loss: 1009498944.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1330  | total loss: \u001b[1m\u001b[32m1055945280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1330 | loss: 1055945280.00000 - R2: 0.9964 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1331  | total loss: \u001b[1m\u001b[32m950557376.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1331 | loss: 950557376.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1332  | total loss: \u001b[1m\u001b[32m2070479616.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1332 | loss: 2070479616.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1333  | total loss: \u001b[1m\u001b[32m1863638272.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1333 | loss: 1863638272.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1334  | total loss: \u001b[1m\u001b[32m1937398272.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1334 | loss: 1937398272.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1335  | total loss: \u001b[1m\u001b[32m1743865088.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1335 | loss: 1743865088.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1336  | total loss: \u001b[1m\u001b[32m1897383680.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1336 | loss: 1897383680.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1337  | total loss: \u001b[1m\u001b[32m1707851904.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1337 | loss: 1707851904.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1338  | total loss: \u001b[1m\u001b[32m1925034880.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1338 | loss: 1925034880.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1339  | total loss: \u001b[1m\u001b[32m1732738048.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1339 | loss: 1732738048.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1340  | total loss: \u001b[1m\u001b[32m1609927168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1340 | loss: 1609927168.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1341  | total loss: \u001b[1m\u001b[32m1449140992.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1341 | loss: 1449140992.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1342  | total loss: \u001b[1m\u001b[32m1397219328.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1342 | loss: 1397219328.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1343  | total loss: \u001b[1m\u001b[32m1257703936.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1343 | loss: 1257703936.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1344  | total loss: \u001b[1m\u001b[32m1296347648.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1344 | loss: 1296347648.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1345  | total loss: \u001b[1m\u001b[32m1166919424.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1345 | loss: 1166919424.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1346  | total loss: \u001b[1m\u001b[32m1310326016.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1346 | loss: 1310326016.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1347  | total loss: \u001b[1m\u001b[32m1179500032.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1347 | loss: 1179500032.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1348  | total loss: \u001b[1m\u001b[32m1146941824.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1348 | loss: 1146941824.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1349  | total loss: \u001b[1m\u001b[32m1032454272.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1349 | loss: 1032454272.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1350  | total loss: \u001b[1m\u001b[32m1038190592.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1350 | loss: 1038190592.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1351  | total loss: \u001b[1m\u001b[32m934578112.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1351 | loss: 934578112.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1352  | total loss: \u001b[1m\u001b[32m1145776896.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1352 | loss: 1145776896.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1353  | total loss: \u001b[1m\u001b[32m1031405824.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1353 | loss: 1031405824.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1354  | total loss: \u001b[1m\u001b[32m1040695936.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1354 | loss: 1040695936.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1355  | total loss: \u001b[1m\u001b[32m936832960.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1355 | loss: 936832960.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1356  | total loss: \u001b[1m\u001b[32m939464448.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1356 | loss: 939464448.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1357  | total loss: \u001b[1m\u001b[32m845724608.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1357 | loss: 845724608.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1358  | total loss: \u001b[1m\u001b[32m1002311680.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1358 | loss: 1002311680.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1359  | total loss: \u001b[1m\u001b[32m902287104.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1359 | loss: 902287104.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1360  | total loss: \u001b[1m\u001b[32m1072719040.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1360 | loss: 1072719040.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1361  | total loss: \u001b[1m\u001b[32m965653760.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1361 | loss: 965653760.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1362  | total loss: \u001b[1m\u001b[32m1202752000.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1362 | loss: 1202752000.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1363  | total loss: \u001b[1m\u001b[32m1082683392.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1363 | loss: 1082683392.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1364  | total loss: \u001b[1m\u001b[32m1198550784.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1364 | loss: 1198550784.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1365  | total loss: \u001b[1m\u001b[32m1078902272.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1365 | loss: 1078902272.00000 - R2: 1.0012 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1366  | total loss: \u001b[1m\u001b[32m1052297088.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1366 | loss: 1052297088.00000 - R2: 1.0020 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1367  | total loss: \u001b[1m\u001b[32m947273984.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1367 | loss: 947273984.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1368  | total loss: \u001b[1m\u001b[32m927429376.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1368 | loss: 927429376.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1369  | total loss: \u001b[1m\u001b[32m834893056.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1369 | loss: 834893056.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1370  | total loss: \u001b[1m\u001b[32m951374848.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1370 | loss: 951374848.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1371  | total loss: \u001b[1m\u001b[32m856443968.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1371 | loss: 856443968.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1372  | total loss: \u001b[1m\u001b[32m829606208.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1372 | loss: 829606208.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1373  | total loss: \u001b[1m\u001b[32m746852224.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1373 | loss: 746852224.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1374  | total loss: \u001b[1m\u001b[32m738784384.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1374 | loss: 738784384.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1375  | total loss: \u001b[1m\u001b[32m665112576.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1375 | loss: 665112576.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1376  | total loss: \u001b[1m\u001b[32m991211072.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1376 | loss: 991211072.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1377  | total loss: \u001b[1m\u001b[32m892296576.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1377 | loss: 892296576.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1378  | total loss: \u001b[1m\u001b[32m903425280.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1378 | loss: 903425280.00000 - R2: 1.0022 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1379  | total loss: \u001b[1m\u001b[32m813289344.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1379 | loss: 813289344.00000 - R2: 1.0019 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1380  | total loss: \u001b[1m\u001b[32m802097472.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1380 | loss: 802097472.00000 - R2: 1.0016 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1381  | total loss: \u001b[1m\u001b[32m722094336.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1381 | loss: 722094336.00000 - R2: 1.0013 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1382  | total loss: \u001b[1m\u001b[32m836510144.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1382 | loss: 836510144.00000 - R2: 1.0019 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1383  | total loss: \u001b[1m\u001b[32m753065728.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1383 | loss: 753065728.00000 - R2: 1.0016 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1384  | total loss: \u001b[1m\u001b[32m757663488.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1384 | loss: 757663488.00000 - R2: 1.0026 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1385  | total loss: \u001b[1m\u001b[32m682103744.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1385 | loss: 682103744.00000 - R2: 1.0022 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1386  | total loss: \u001b[1m\u001b[32m1788253184.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1386 | loss: 1788253184.00000 - R2: 1.0019 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1387  | total loss: \u001b[1m\u001b[32m1609634432.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1387 | loss: 1609634432.00000 - R2: 1.0016 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1388  | total loss: \u001b[1m\u001b[32m1685691392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1388 | loss: 1685691392.00000 - R2: 1.0024 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1389  | total loss: \u001b[1m\u001b[32m1517328896.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1389 | loss: 1517328896.00000 - R2: 1.0021 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1390  | total loss: \u001b[1m\u001b[32m1434942080.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1390 | loss: 1434942080.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1391  | total loss: \u001b[1m\u001b[32m1291654400.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1391 | loss: 1291654400.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1392  | total loss: \u001b[1m\u001b[32m1232918016.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1392 | loss: 1232918016.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1393  | total loss: \u001b[1m\u001b[32m1109832832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1393 | loss: 1109832832.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1394  | total loss: \u001b[1m\u001b[32m1138593408.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1394 | loss: 1138593408.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1395  | total loss: \u001b[1m\u001b[32m1024940672.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1395 | loss: 1024940672.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1396  | total loss: \u001b[1m\u001b[32m1136894080.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1396 | loss: 1136894080.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1397  | total loss: \u001b[1m\u001b[32m1023411264.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1397 | loss: 1023411264.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1398  | total loss: \u001b[1m\u001b[32m1020588032.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1398 | loss: 1020588032.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1399  | total loss: \u001b[1m\u001b[32m918735808.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1399 | loss: 918735808.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1400  | total loss: \u001b[1m\u001b[32m1119950464.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1400 | loss: 1119950464.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1401  | total loss: \u001b[1m\u001b[32m1008162048.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1401 | loss: 1008162048.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1402  | total loss: \u001b[1m\u001b[32m973569920.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1402 | loss: 973569920.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1403  | total loss: \u001b[1m\u001b[32m876419520.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1403 | loss: 876419520.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1404  | total loss: \u001b[1m\u001b[32m1226212096.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1404 | loss: 1226212096.00000 - R2: 1.0032 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1405  | total loss: \u001b[1m\u001b[32m1103797504.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1405 | loss: 1103797504.00000 - R2: 1.0028 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1406  | total loss: \u001b[1m\u001b[32m1294584320.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1406 | loss: 1294584320.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1407  | total loss: \u001b[1m\u001b[32m1165332480.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1407 | loss: 1165332480.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1408  | total loss: \u001b[1m\u001b[32m1118653696.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1408 | loss: 1118653696.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1409  | total loss: \u001b[1m\u001b[32m1006994944.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1409 | loss: 1006994944.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1410  | total loss: \u001b[1m\u001b[32m1169661184.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1410 | loss: 1169661184.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1411  | total loss: \u001b[1m\u001b[32m1052901696.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1411 | loss: 1052901696.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1412  | total loss: \u001b[1m\u001b[32m1064841280.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1412 | loss: 1064841280.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1413  | total loss: \u001b[1m\u001b[32m958563776.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1413 | loss: 958563776.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1414  | total loss: \u001b[1m\u001b[32m977694592.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1414 | loss: 977694592.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1415  | total loss: \u001b[1m\u001b[32m880131712.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1415 | loss: 880131712.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1416  | total loss: \u001b[1m\u001b[32m894805568.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1416 | loss: 894805568.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1417  | total loss: \u001b[1m\u001b[32m805531648.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1417 | loss: 805531648.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1418  | total loss: \u001b[1m\u001b[32m1176408448.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1418 | loss: 1176408448.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1419  | total loss: \u001b[1m\u001b[32m1058974208.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1419 | loss: 1058974208.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1420  | total loss: \u001b[1m\u001b[32m1392748288.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1420 | loss: 1392748288.00000 - R2: 1.0013 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1421  | total loss: \u001b[1m\u001b[32m1253680128.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1421 | loss: 1253680128.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1422  | total loss: \u001b[1m\u001b[32m1197401728.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1422 | loss: 1197401728.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1423  | total loss: \u001b[1m\u001b[32m1077868160.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1423 | loss: 1077868160.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1424  | total loss: \u001b[1m\u001b[32m1098350976.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1424 | loss: 1098350976.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1425  | total loss: \u001b[1m\u001b[32m988722496.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1425 | loss: 988722496.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1426  | total loss: \u001b[1m\u001b[32m1206102528.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1426 | loss: 1206102528.00000 - R2: 1.0015 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1427  | total loss: \u001b[1m\u001b[32m1085698816.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1427 | loss: 1085698816.00000 - R2: 1.0013 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1428  | total loss: \u001b[1m\u001b[32m1300789248.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1428 | loss: 1300789248.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1429  | total loss: \u001b[1m\u001b[32m1170916864.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1429 | loss: 1170916864.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1430  | total loss: \u001b[1m\u001b[32m1470462976.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1430 | loss: 1470462976.00000 - R2: 1.0040 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1431  | total loss: \u001b[1m\u001b[32m1323623296.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1431 | loss: 1323623296.00000 - R2: 1.0035 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1432  | total loss: \u001b[1m\u001b[32m1303895424.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1432 | loss: 1303895424.00000 - R2: 1.0037 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1433  | total loss: \u001b[1m\u001b[32m1173712512.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1433 | loss: 1173712512.00000 - R2: 1.0032 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1434  | total loss: \u001b[1m\u001b[32m1127959936.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1434 | loss: 1127959936.00000 - R2: 1.0045 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1435  | total loss: \u001b[1m\u001b[32m1015370560.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1435 | loss: 1015370560.00000 - R2: 1.0040 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1436  | total loss: \u001b[1m\u001b[32m1062254976.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1436 | loss: 1062254976.00000 - R2: 1.0039 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1437  | total loss: \u001b[1m\u001b[32m956236096.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1437 | loss: 956236096.00000 - R2: 1.0034 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1438  | total loss: \u001b[1m\u001b[32m926806784.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1438 | loss: 926806784.00000 - R2: 1.0028 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1439  | total loss: \u001b[1m\u001b[32m834332736.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1439 | loss: 834332736.00000 - R2: 1.0024 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1440  | total loss: \u001b[1m\u001b[32m854321664.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1440 | loss: 854321664.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1441  | total loss: \u001b[1m\u001b[32m769096128.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1441 | loss: 769096128.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1442  | total loss: \u001b[1m\u001b[32m912356992.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1442 | loss: 912356992.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1443  | total loss: \u001b[1m\u001b[32m821327872.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1443 | loss: 821327872.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1444  | total loss: \u001b[1m\u001b[32m830235008.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1444 | loss: 830235008.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1445  | total loss: \u001b[1m\u001b[32m747418112.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1445 | loss: 747418112.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1446  | total loss: \u001b[1m\u001b[32m1048470784.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1446 | loss: 1048470784.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1447  | total loss: \u001b[1m\u001b[32m943830336.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1447 | loss: 943830336.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1448  | total loss: \u001b[1m\u001b[32m947369408.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1448 | loss: 947369408.00000 - R2: 0.9966 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1449  | total loss: \u001b[1m\u001b[32m852839040.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1449 | loss: 852839040.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1450  | total loss: \u001b[1m\u001b[32m1175955840.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1450 | loss: 1175955840.00000 - R2: 0.9957 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1451  | total loss: \u001b[1m\u001b[32m1058566848.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1451 | loss: 1058566848.00000 - R2: 0.9960 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1452  | total loss: \u001b[1m\u001b[32m1055264960.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1452 | loss: 1055264960.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1453  | total loss: \u001b[1m\u001b[32m949945088.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1453 | loss: 949945088.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1454  | total loss: \u001b[1m\u001b[32m1148234112.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1454 | loss: 1148234112.00000 - R2: 0.9957 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1455  | total loss: \u001b[1m\u001b[32m1033617280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1455 | loss: 1033617280.00000 - R2: 0.9960 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1456  | total loss: \u001b[1m\u001b[32m1017601280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1456 | loss: 1017601280.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1457  | total loss: \u001b[1m\u001b[32m916047744.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1457 | loss: 916047744.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1458  | total loss: \u001b[1m\u001b[32m990890880.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1458 | loss: 990890880.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1459  | total loss: \u001b[1m\u001b[32m892008384.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1459 | loss: 892008384.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1460  | total loss: \u001b[1m\u001b[32m970591680.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1460 | loss: 970591680.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1461  | total loss: \u001b[1m\u001b[32m873739136.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1461 | loss: 873739136.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1462  | total loss: \u001b[1m\u001b[32m1793610624.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1462 | loss: 1793610624.00000 - R2: 0.9958 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1463  | total loss: \u001b[1m\u001b[32m1614456192.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1463 | loss: 1614456192.00000 - R2: 0.9961 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1464  | total loss: \u001b[1m\u001b[32m1505428864.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1464 | loss: 1505428864.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1465  | total loss: \u001b[1m\u001b[32m1355092608.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1465 | loss: 1355092608.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1466  | total loss: \u001b[1m\u001b[32m1338824960.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1466 | loss: 1338824960.00000 - R2: 0.9961 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1467  | total loss: \u001b[1m\u001b[32m1205149056.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1467 | loss: 1205149056.00000 - R2: 0.9964 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1468  | total loss: \u001b[1m\u001b[32m1176756864.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1468 | loss: 1176756864.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1469  | total loss: \u001b[1m\u001b[32m1059287808.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1469 | loss: 1059287808.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1470  | total loss: \u001b[1m\u001b[32m1003600384.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1470 | loss: 1003600384.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1471  | total loss: \u001b[1m\u001b[32m903446976.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1471 | loss: 903446976.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1472  | total loss: \u001b[1m\u001b[32m898041600.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1472 | loss: 898041600.00000 - R2: 0.9966 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1473  | total loss: \u001b[1m\u001b[32m808444032.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1473 | loss: 808444032.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1474  | total loss: \u001b[1m\u001b[32m832120832.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1474 | loss: 832120832.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1475  | total loss: \u001b[1m\u001b[32m749115392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1475 | loss: 749115392.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1476  | total loss: \u001b[1m\u001b[32m737496640.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1476 | loss: 737496640.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1477  | total loss: \u001b[1m\u001b[32m663953600.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1477 | loss: 663953600.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1478  | total loss: \u001b[1m\u001b[32m659729344.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1478 | loss: 659729344.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1479  | total loss: \u001b[1m\u001b[32m593963008.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1479 | loss: 593963008.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1480  | total loss: \u001b[1m\u001b[32m725338176.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1480 | loss: 725338176.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1481  | total loss: \u001b[1m\u001b[32m653010944.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1481 | loss: 653010944.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1482  | total loss: \u001b[1m\u001b[32m638535552.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1482 | loss: 638535552.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1483  | total loss: \u001b[1m\u001b[32m574888640.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1483 | loss: 574888640.00000 - R2: 0.9970 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1484  | total loss: \u001b[1m\u001b[32m607277184.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1484 | loss: 607277184.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1485  | total loss: \u001b[1m\u001b[32m546756096.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1485 | loss: 546756096.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1486  | total loss: \u001b[1m\u001b[32m589639296.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1486 | loss: 589639296.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1487  | total loss: \u001b[1m\u001b[32m530881984.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1487 | loss: 530881984.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1488  | total loss: \u001b[1m\u001b[32m557763520.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1488 | loss: 557763520.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1489  | total loss: \u001b[1m\u001b[32m502193792.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1489 | loss: 502193792.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1490  | total loss: \u001b[1m\u001b[32m535848256.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1490 | loss: 535848256.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1491  | total loss: \u001b[1m\u001b[32m482470048.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1491 | loss: 482470048.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1492  | total loss: \u001b[1m\u001b[32m525129632.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1492 | loss: 525129632.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1493  | total loss: \u001b[1m\u001b[32m472823296.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1493 | loss: 472823296.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1494  | total loss: \u001b[1m\u001b[32m615627584.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1494 | loss: 615627584.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1495  | total loss: \u001b[1m\u001b[32m554271424.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1495 | loss: 554271424.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1496  | total loss: \u001b[1m\u001b[32m560347648.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1496 | loss: 560347648.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1497  | total loss: \u001b[1m\u001b[32m504519488.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1497 | loss: 504519488.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1498  | total loss: \u001b[1m\u001b[32m454274144.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1498 | loss: 454274144.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1499  | total loss: \u001b[1m\u001b[32m409053344.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1499 | loss: 409053344.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1500  | total loss: \u001b[1m\u001b[32m446145280.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1500 | loss: 446145280.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1501  | total loss: \u001b[1m\u001b[32m401737376.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1501 | loss: 401737376.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1502  | total loss: \u001b[1m\u001b[32m461674368.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1502 | loss: 461674368.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1503  | total loss: \u001b[1m\u001b[32m415713536.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1503 | loss: 415713536.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1504  | total loss: \u001b[1m\u001b[32m598735296.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1504 | loss: 598735296.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1505  | total loss: \u001b[1m\u001b[32m539068416.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1505 | loss: 539068416.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1506  | total loss: \u001b[1m\u001b[32m568017856.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1506 | loss: 568017856.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1507  | total loss: \u001b[1m\u001b[32m511422688.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1507 | loss: 511422688.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1508  | total loss: \u001b[1m\u001b[32m544553152.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1508 | loss: 544553152.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1509  | total loss: \u001b[1m\u001b[32m490304448.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1509 | loss: 490304448.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1510  | total loss: \u001b[1m\u001b[32m1536973824.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1510 | loss: 1536973824.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1511  | total loss: \u001b[1m\u001b[32m1383483008.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1511 | loss: 1383483008.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1512  | total loss: \u001b[1m\u001b[32m1438219136.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1512 | loss: 1438219136.00000 - R2: 0.9945 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1513  | total loss: \u001b[1m\u001b[32m1294603776.00000\u001b[0m\u001b[0m | time: 1.015s\n",
      "| SGD | epoch: 1513 | loss: 1294603776.00000 - R2: 0.9949 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1514  | total loss: \u001b[1m\u001b[32m1555917568.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1514 | loss: 1555917568.00000 - R2: 0.9937 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1515  | total loss: \u001b[1m\u001b[32m1400532352.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1515 | loss: 1400532352.00000 - R2: 0.9943 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1516  | total loss: \u001b[1m\u001b[32m1627566720.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1516 | loss: 1627566720.00000 - R2: 0.9954 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1517  | total loss: \u001b[1m\u001b[32m1465016576.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1517 | loss: 1465016576.00000 - R2: 0.9958 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1518  | total loss: \u001b[1m\u001b[32m1839479040.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1518 | loss: 1839479040.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1519  | total loss: \u001b[1m\u001b[32m1655737728.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 1519 | loss: 1655737728.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1520  | total loss: \u001b[1m\u001b[32m1904099456.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1520 | loss: 1904099456.00000 - R2: 0.9966 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1521  | total loss: \u001b[1m\u001b[32m1713896064.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1521 | loss: 1713896064.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1522  | total loss: \u001b[1m\u001b[32m1892526720.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1522 | loss: 1892526720.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1523  | total loss: \u001b[1m\u001b[32m1703480576.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1523 | loss: 1703480576.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1524  | total loss: \u001b[1m\u001b[32m1661731968.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1524 | loss: 1661731968.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1525  | total loss: \u001b[1m\u001b[32m1495765376.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1525 | loss: 1495765376.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1526  | total loss: \u001b[1m\u001b[32m1401774848.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1526 | loss: 1401774848.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1527  | total loss: \u001b[1m\u001b[32m1261804032.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1527 | loss: 1261804032.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1528  | total loss: \u001b[1m\u001b[32m1366205312.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1528 | loss: 1366205312.00000 - R2: 0.9963 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1529  | total loss: \u001b[1m\u001b[32m1229791360.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1529 | loss: 1229791360.00000 - R2: 0.9966 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1530  | total loss: \u001b[1m\u001b[32m1370474368.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1530 | loss: 1370474368.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1531  | total loss: \u001b[1m\u001b[32m1233633536.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1531 | loss: 1233633536.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1532  | total loss: \u001b[1m\u001b[32m1294214912.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1532 | loss: 1294214912.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1533  | total loss: \u001b[1m\u001b[32m1165000064.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1533 | loss: 1165000064.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1534  | total loss: \u001b[1m\u001b[32m1115595776.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1534 | loss: 1115595776.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1535  | total loss: \u001b[1m\u001b[32m1004242816.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1535 | loss: 1004242816.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1536  | total loss: \u001b[1m\u001b[32m995541184.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1536 | loss: 995541184.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1537  | total loss: \u001b[1m\u001b[32m896193664.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1537 | loss: 896193664.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1538  | total loss: \u001b[1m\u001b[32m1093952000.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1538 | loss: 1093952000.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1539  | total loss: \u001b[1m\u001b[32m984763392.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1539 | loss: 984763392.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1540  | total loss: \u001b[1m\u001b[32m928409984.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1540 | loss: 928409984.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1541  | total loss: \u001b[1m\u001b[32m835775616.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1541 | loss: 835775616.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1542  | total loss: \u001b[1m\u001b[32m969773888.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1542 | loss: 969773888.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1543  | total loss: \u001b[1m\u001b[32m873003136.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1543 | loss: 873003136.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1544  | total loss: \u001b[1m\u001b[32m1192702208.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1544 | loss: 1192702208.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1545  | total loss: \u001b[1m\u001b[32m1073638592.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1545 | loss: 1073638592.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1546  | total loss: \u001b[1m\u001b[32m1110711040.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1546 | loss: 1110711040.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1547  | total loss: \u001b[1m\u001b[32m999846528.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1547 | loss: 999846528.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1548  | total loss: \u001b[1m\u001b[32m974179520.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1548 | loss: 974179520.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1549  | total loss: \u001b[1m\u001b[32m876968192.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1549 | loss: 876968192.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1550  | total loss: \u001b[1m\u001b[32m1022191744.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1550 | loss: 1022191744.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1551  | total loss: \u001b[1m\u001b[32m920179200.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1551 | loss: 920179200.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1552  | total loss: \u001b[1m\u001b[32m935405440.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1552 | loss: 935405440.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1553  | total loss: \u001b[1m\u001b[32m842071488.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1553 | loss: 842071488.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1554  | total loss: \u001b[1m\u001b[32m1656073984.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1554 | loss: 1656073984.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1555  | total loss: \u001b[1m\u001b[32m1490673152.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1555 | loss: 1490673152.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1556  | total loss: \u001b[1m\u001b[32m1437240320.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1556 | loss: 1437240320.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1557  | total loss: \u001b[1m\u001b[32m1293722880.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1557 | loss: 1293722880.00000 - R2: 1.0005 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1558  | total loss: \u001b[1m\u001b[32m1252294272.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1558 | loss: 1252294272.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1559  | total loss: \u001b[1m\u001b[32m1127271424.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1559 | loss: 1127271424.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1560  | total loss: \u001b[1m\u001b[32m1162926080.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1560 | loss: 1162926080.00000 - R2: 1.0023 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1561  | total loss: \u001b[1m\u001b[32m1046840064.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1561 | loss: 1046840064.00000 - R2: 1.0019 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1562  | total loss: \u001b[1m\u001b[32m1424393472.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1562 | loss: 1424393472.00000 - R2: 1.0029 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1563  | total loss: \u001b[1m\u001b[32m1282160768.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1563 | loss: 1282160768.00000 - R2: 1.0025 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1564  | total loss: \u001b[1m\u001b[32m1154151296.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1564 | loss: 1154151296.00000 - R2: 1.0022 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1565  | total loss: \u001b[1m\u001b[32m1038942784.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1565 | loss: 1038942784.00000 - R2: 1.0019 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1566  | total loss: \u001b[1m\u001b[32m988523776.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1566 | loss: 988523776.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1567  | total loss: \u001b[1m\u001b[32m889878016.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1567 | loss: 889878016.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1568  | total loss: \u001b[1m\u001b[32m892655488.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1568 | loss: 892655488.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1569  | total loss: \u001b[1m\u001b[32m803596544.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1569 | loss: 803596544.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1570  | total loss: \u001b[1m\u001b[32m826145152.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1570 | loss: 826145152.00000 - R2: 1.0009 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1571  | total loss: \u001b[1m\u001b[32m743737216.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1571 | loss: 743737216.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1572  | total loss: \u001b[1m\u001b[32m1004653120.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1572 | loss: 1004653120.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1573  | total loss: \u001b[1m\u001b[32m904394432.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1573 | loss: 904394432.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1574  | total loss: \u001b[1m\u001b[32m852234624.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1574 | loss: 852234624.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1575  | total loss: \u001b[1m\u001b[32m767217792.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1575 | loss: 767217792.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1576  | total loss: \u001b[1m\u001b[32m757006464.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1576 | loss: 757006464.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1577  | total loss: \u001b[1m\u001b[32m681512448.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1577 | loss: 681512448.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1578  | total loss: \u001b[1m\u001b[32m933360640.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1578 | loss: 933360640.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1579  | total loss: \u001b[1m\u001b[32m840231168.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1579 | loss: 840231168.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1580  | total loss: \u001b[1m\u001b[32m947132736.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1580 | loss: 947132736.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1581  | total loss: \u001b[1m\u001b[32m852626048.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1581 | loss: 852626048.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1582  | total loss: \u001b[1m\u001b[32m1040650368.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1582 | loss: 1040650368.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1583  | total loss: \u001b[1m\u001b[32m936791936.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1583 | loss: 936791936.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1584  | total loss: \u001b[1m\u001b[32m976537984.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1584 | loss: 976537984.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1585  | total loss: \u001b[1m\u001b[32m879090816.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1585 | loss: 879090816.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1586  | total loss: \u001b[1m\u001b[32m907795328.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1586 | loss: 907795328.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1587  | total loss: \u001b[1m\u001b[32m817222400.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1587 | loss: 817222400.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1588  | total loss: \u001b[1m\u001b[32m735706752.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1588 | loss: 735706752.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1589  | total loss: \u001b[1m\u001b[32m662342656.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1589 | loss: 662342656.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1590  | total loss: \u001b[1m\u001b[32m694261568.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1590 | loss: 694261568.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1591  | total loss: \u001b[1m\u001b[32m625042048.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1591 | loss: 625042048.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1592  | total loss: \u001b[1m\u001b[32m873369856.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1592 | loss: 873369856.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1593  | total loss: \u001b[1m\u001b[32m786239488.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1593 | loss: 786239488.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1594  | total loss: \u001b[1m\u001b[32m860901632.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1594 | loss: 860901632.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1595  | total loss: \u001b[1m\u001b[32m775018112.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1595 | loss: 775018112.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1596  | total loss: \u001b[1m\u001b[32m772109888.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1596 | loss: 772109888.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1597  | total loss: \u001b[1m\u001b[32m695105536.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1597 | loss: 695105536.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1598  | total loss: \u001b[1m\u001b[32m722748544.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1598 | loss: 722748544.00000 - R2: 0.9962 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1599  | total loss: \u001b[1m\u001b[32m650680320.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1599 | loss: 650680320.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1600  | total loss: \u001b[1m\u001b[32m706903744.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1600 | loss: 706903744.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1601  | total loss: \u001b[1m\u001b[32m636419968.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1601 | loss: 636419968.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1602  | total loss: \u001b[1m\u001b[32m890573632.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1602 | loss: 890573632.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1603  | total loss: \u001b[1m\u001b[32m801722880.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1603 | loss: 801722880.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1604  | total loss: \u001b[1m\u001b[32m836284416.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1604 | loss: 836284416.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1605  | total loss: \u001b[1m\u001b[32m752862592.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1605 | loss: 752862592.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1606  | total loss: \u001b[1m\u001b[32m785339200.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1606 | loss: 785339200.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1607  | total loss: \u001b[1m\u001b[32m707011904.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1607 | loss: 707011904.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1608  | total loss: \u001b[1m\u001b[32m737580608.00000\u001b[0m\u001b[0m | time: 1.018s\n",
      "| SGD | epoch: 1608 | loss: 737580608.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1609  | total loss: \u001b[1m\u001b[32m664029184.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1609 | loss: 664029184.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1610  | total loss: \u001b[1m\u001b[32m686476352.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1610 | loss: 686476352.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1611  | total loss: \u001b[1m\u001b[32m618035328.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1611 | loss: 618035328.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1612  | total loss: \u001b[1m\u001b[32m660086720.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1612 | loss: 660086720.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1613  | total loss: \u001b[1m\u001b[32m594284672.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1613 | loss: 594284672.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1614  | total loss: \u001b[1m\u001b[32m805160064.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1614 | loss: 805160064.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1615  | total loss: \u001b[1m\u001b[32m724850688.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1615 | loss: 724850688.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1616  | total loss: \u001b[1m\u001b[32m678756032.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1616 | loss: 678756032.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1617  | total loss: \u001b[1m\u001b[32m611087040.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1617 | loss: 611087040.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1618  | total loss: \u001b[1m\u001b[32m865300224.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1618 | loss: 865300224.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1619  | total loss: \u001b[1m\u001b[32m778976832.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1619 | loss: 778976832.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1620  | total loss: \u001b[1m\u001b[32m788019584.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1620 | loss: 788019584.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1621  | total loss: \u001b[1m\u001b[32m709424256.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1621 | loss: 709424256.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1622  | total loss: \u001b[1m\u001b[32m889798144.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1622 | loss: 889798144.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1623  | total loss: \u001b[1m\u001b[32m801024960.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1623 | loss: 801024960.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1624  | total loss: \u001b[1m\u001b[32m852793792.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1624 | loss: 852793792.00000 - R2: 0.9972 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1625  | total loss: \u001b[1m\u001b[32m767721024.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1625 | loss: 767721024.00000 - R2: 0.9974 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1626  | total loss: \u001b[1m\u001b[32m716807296.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1626 | loss: 716807296.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1627  | total loss: \u001b[1m\u001b[32m645333184.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1627 | loss: 645333184.00000 - R2: 0.9975 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1628  | total loss: \u001b[1m\u001b[32m581006464.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1628 | loss: 581006464.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1629  | total loss: \u001b[1m\u001b[32m523112448.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1629 | loss: 523112448.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1630  | total loss: \u001b[1m\u001b[32m548845312.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1630 | loss: 548845312.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1631  | total loss: \u001b[1m\u001b[32m494167392.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1631 | loss: 494167392.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1632  | total loss: \u001b[1m\u001b[32m491004640.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1632 | loss: 491004640.00000 - R2: 0.9991 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1633  | total loss: \u001b[1m\u001b[32m442110784.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1633 | loss: 442110784.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1634  | total loss: \u001b[1m\u001b[32m452950976.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1634 | loss: 452950976.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1635  | total loss: \u001b[1m\u001b[32m407862496.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1635 | loss: 407862496.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1636  | total loss: \u001b[1m\u001b[32m861796352.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1636 | loss: 861796352.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1637  | total loss: \u001b[1m\u001b[32m775823360.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1637 | loss: 775823360.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1638  | total loss: \u001b[1m\u001b[32m807746304.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1638 | loss: 807746304.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1639  | total loss: \u001b[1m\u001b[32m727178304.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1639 | loss: 727178304.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1640  | total loss: \u001b[1m\u001b[32m725917952.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1640 | loss: 725917952.00000 - R2: 0.9969 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1641  | total loss: \u001b[1m\u001b[32m653532800.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1641 | loss: 653532800.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1642  | total loss: \u001b[1m\u001b[32m667950080.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1642 | loss: 667950080.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1643  | total loss: \u001b[1m\u001b[32m601361664.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1643 | loss: 601361664.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1644  | total loss: \u001b[1m\u001b[32m576000192.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1644 | loss: 576000192.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1645  | total loss: \u001b[1m\u001b[32m518606784.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1645 | loss: 518606784.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1646  | total loss: \u001b[1m\u001b[32m574320896.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1646 | loss: 574320896.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1647  | total loss: \u001b[1m\u001b[32m517095424.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1647 | loss: 517095424.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1648  | total loss: \u001b[1m\u001b[32m785623552.00000\u001b[0m\u001b[0m | time: 1.032s\n",
      "| SGD | epoch: 1648 | loss: 785623552.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1649  | total loss: \u001b[1m\u001b[32m707267840.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1649 | loss: 707267840.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1650  | total loss: \u001b[1m\u001b[32m745788928.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1650 | loss: 745788928.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1651  | total loss: \u001b[1m\u001b[32m671416640.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1651 | loss: 671416640.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1652  | total loss: \u001b[1m\u001b[32m737563904.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1652 | loss: 737563904.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1653  | total loss: \u001b[1m\u001b[32m664014144.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1653 | loss: 664014144.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1654  | total loss: \u001b[1m\u001b[32m674341568.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1654 | loss: 674341568.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1655  | total loss: \u001b[1m\u001b[32m607114048.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1655 | loss: 607114048.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1656  | total loss: \u001b[1m\u001b[32m637260480.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1656 | loss: 637260480.00000 - R2: 0.9963 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1657  | total loss: \u001b[1m\u001b[32m573741056.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1657 | loss: 573741056.00000 - R2: 0.9966 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1658  | total loss: \u001b[1m\u001b[32m782471872.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1658 | loss: 782471872.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1659  | total loss: \u001b[1m\u001b[32m704431296.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1659 | loss: 704431296.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1660  | total loss: \u001b[1m\u001b[32m793692928.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1660 | loss: 793692928.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1661  | total loss: \u001b[1m\u001b[32m714530240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1661 | loss: 714530240.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1662  | total loss: \u001b[1m\u001b[32m885924800.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1662 | loss: 885924800.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1663  | total loss: \u001b[1m\u001b[32m797538944.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1663 | loss: 797538944.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1664  | total loss: \u001b[1m\u001b[32m806302720.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1664 | loss: 806302720.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1665  | total loss: \u001b[1m\u001b[32m725879040.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1665 | loss: 725879040.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1666  | total loss: \u001b[1m\u001b[32m743329088.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1666 | loss: 743329088.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1667  | total loss: \u001b[1m\u001b[32m669202816.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1667 | loss: 669202816.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1668  | total loss: \u001b[1m\u001b[32m658269696.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1668 | loss: 658269696.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1669  | total loss: \u001b[1m\u001b[32m592649344.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1669 | loss: 592649344.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1670  | total loss: \u001b[1m\u001b[32m744654976.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1670 | loss: 744654976.00000 - R2: 0.9943 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1671  | total loss: \u001b[1m\u001b[32m670396096.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1671 | loss: 670396096.00000 - R2: 0.9948 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1672  | total loss: \u001b[1m\u001b[32m684962560.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1672 | loss: 684962560.00000 - R2: 0.9948 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1673  | total loss: \u001b[1m\u001b[32m616672896.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1673 | loss: 616672896.00000 - R2: 0.9952 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1674  | total loss: \u001b[1m\u001b[32m660526848.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1674 | loss: 660526848.00000 - R2: 0.9960 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1675  | total loss: \u001b[1m\u001b[32m594680768.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1675 | loss: 594680768.00000 - R2: 0.9963 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1676  | total loss: \u001b[1m\u001b[32m638854080.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1676 | loss: 638854080.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1677  | total loss: \u001b[1m\u001b[32m575175296.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1677 | loss: 575175296.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1678  | total loss: \u001b[1m\u001b[32m792980480.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1678 | loss: 792980480.00000 - R2: 0.9951 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1679  | total loss: \u001b[1m\u001b[32m713889024.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1679 | loss: 713889024.00000 - R2: 0.9955 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1680  | total loss: \u001b[1m\u001b[32m768110336.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1680 | loss: 768110336.00000 - R2: 0.9954 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1681  | total loss: \u001b[1m\u001b[32m691505920.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1681 | loss: 691505920.00000 - R2: 0.9957 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1682  | total loss: \u001b[1m\u001b[32m730201664.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1682 | loss: 730201664.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1683  | total loss: \u001b[1m\u001b[32m657388096.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1683 | loss: 657388096.00000 - R2: 0.9967 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1684  | total loss: \u001b[1m\u001b[32m911052672.00000\u001b[0m\u001b[0m | time: 1.013s\n",
      "| SGD | epoch: 1684 | loss: 911052672.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1685  | total loss: \u001b[1m\u001b[32m820153984.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1685 | loss: 820153984.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1686  | total loss: \u001b[1m\u001b[32m842157888.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1686 | loss: 842157888.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1687  | total loss: \u001b[1m\u001b[32m758148736.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1687 | loss: 758148736.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1688  | total loss: \u001b[1m\u001b[32m879273344.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1688 | loss: 879273344.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1689  | total loss: \u001b[1m\u001b[32m791552640.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1689 | loss: 791552640.00000 - R2: 0.9986 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1690  | total loss: \u001b[1m\u001b[32m824402496.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1690 | loss: 824402496.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1691  | total loss: \u001b[1m\u001b[32m742168832.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1691 | loss: 742168832.00000 - R2: 0.9990 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1692  | total loss: \u001b[1m\u001b[32m780109696.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1692 | loss: 780109696.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1693  | total loss: \u001b[1m\u001b[32m702305344.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1693 | loss: 702305344.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1694  | total loss: \u001b[1m\u001b[32m847174080.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1694 | loss: 847174080.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1695  | total loss: \u001b[1m\u001b[32m762663296.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1695 | loss: 762663296.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1696  | total loss: \u001b[1m\u001b[32m758635904.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1696 | loss: 758635904.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1697  | total loss: \u001b[1m\u001b[32m682978944.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1697 | loss: 682978944.00000 - R2: 0.9995 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1698  | total loss: \u001b[1m\u001b[32m714746176.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1698 | loss: 714746176.00000 - R2: 0.9994 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1699  | total loss: \u001b[1m\u001b[32m643478144.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1699 | loss: 643478144.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1700  | total loss: \u001b[1m\u001b[32m840347968.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1700 | loss: 840347968.00000 - R2: 1.0012 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1701  | total loss: \u001b[1m\u001b[32m756519808.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1701 | loss: 756519808.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1702  | total loss: \u001b[1m\u001b[32m1001582848.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1702 | loss: 1001582848.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1703  | total loss: \u001b[1m\u001b[32m901631168.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1703 | loss: 901631168.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1704  | total loss: \u001b[1m\u001b[32m1134236160.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1704 | loss: 1134236160.00000 - R2: 1.0003 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1705  | total loss: \u001b[1m\u001b[32m1021019136.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1705 | loss: 1021019136.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1706  | total loss: \u001b[1m\u001b[32m996135680.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1706 | loss: 996135680.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1707  | total loss: \u001b[1m\u001b[32m896728704.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1707 | loss: 896728704.00000 - R2: 1.0012 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1708  | total loss: \u001b[1m\u001b[32m878996096.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1708 | loss: 878996096.00000 - R2: 1.0021 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1709  | total loss: \u001b[1m\u001b[32m791303104.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1709 | loss: 791303104.00000 - R2: 1.0018 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1710  | total loss: \u001b[1m\u001b[32m778920896.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1710 | loss: 778920896.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1711  | total loss: \u001b[1m\u001b[32m701235392.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1711 | loss: 701235392.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1712  | total loss: \u001b[1m\u001b[32m912106240.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1712 | loss: 912106240.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1713  | total loss: \u001b[1m\u001b[32m821102208.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1713 | loss: 821102208.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1714  | total loss: \u001b[1m\u001b[32m1904595072.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1714 | loss: 1904595072.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1715  | total loss: \u001b[1m\u001b[32m1714342144.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1715 | loss: 1714342144.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1716  | total loss: \u001b[1m\u001b[32m1632977920.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1716 | loss: 1632977920.00000 - R2: 0.9976 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1717  | total loss: \u001b[1m\u001b[32m1469886720.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1717 | loss: 1469886720.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1718  | total loss: \u001b[1m\u001b[32m1323104640.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1718 | loss: 1323104640.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1719  | total loss: \u001b[1m\u001b[32m1191000832.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1719 | loss: 1191000832.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1720  | total loss: \u001b[1m\u001b[32m1206968960.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1720 | loss: 1206968960.00000 - R2: 0.9962 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1721  | total loss: \u001b[1m\u001b[32m1086478720.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1721 | loss: 1086478720.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1722  | total loss: \u001b[1m\u001b[32m1067681152.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1722 | loss: 1067681152.00000 - R2: 0.9981 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1723  | total loss: \u001b[1m\u001b[32m961119616.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1723 | loss: 961119616.00000 - R2: 0.9982 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1724  | total loss: \u001b[1m\u001b[32m1142225792.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1724 | loss: 1142225792.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1725  | total loss: \u001b[1m\u001b[32m1028209792.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1725 | loss: 1028209792.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1726  | total loss: \u001b[1m\u001b[32m1011039872.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1726 | loss: 1011039872.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1727  | total loss: \u001b[1m\u001b[32m910142464.00000\u001b[0m\u001b[0m | time: 1.026s\n",
      "| SGD | epoch: 1727 | loss: 910142464.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1728  | total loss: \u001b[1m\u001b[32m1075233920.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1728 | loss: 1075233920.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1729  | total loss: \u001b[1m\u001b[32m967917120.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1729 | loss: 967917120.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1730  | total loss: \u001b[1m\u001b[32m940495168.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1730 | loss: 940495168.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1731  | total loss: \u001b[1m\u001b[32m846652288.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1731 | loss: 846652288.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1732  | total loss: \u001b[1m\u001b[32m856514752.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1732 | loss: 856514752.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1733  | total loss: \u001b[1m\u001b[32m771069888.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1733 | loss: 771069888.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1734  | total loss: \u001b[1m\u001b[32m1056748736.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1734 | loss: 1056748736.00000 - R2: 1.0033 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1735  | total loss: \u001b[1m\u001b[32m951280448.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1735 | loss: 951280448.00000 - R2: 1.0029 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1736  | total loss: \u001b[1m\u001b[32m935172736.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1736 | loss: 935172736.00000 - R2: 1.0020 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1737  | total loss: \u001b[1m\u001b[32m841862080.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1737 | loss: 841862080.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1738  | total loss: \u001b[1m\u001b[32m808430272.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1738 | loss: 808430272.00000 - R2: 1.0014 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1739  | total loss: \u001b[1m\u001b[32m727793856.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1739 | loss: 727793856.00000 - R2: 1.0011 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1740  | total loss: \u001b[1m\u001b[32m913743296.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1740 | loss: 913743296.00000 - R2: 1.0025 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1741  | total loss: \u001b[1m\u001b[32m822575552.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1741 | loss: 822575552.00000 - R2: 1.0021 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1742  | total loss: \u001b[1m\u001b[32m1939022592.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1742 | loss: 1939022592.00000 - R2: 1.0027 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1743  | total loss: \u001b[1m\u001b[32m1745326848.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1743 | loss: 1745326848.00000 - R2: 1.0023 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1744  | total loss: \u001b[1m\u001b[32m1843372032.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1744 | loss: 1843372032.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1745  | total loss: \u001b[1m\u001b[32m1659241472.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1745 | loss: 1659241472.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1746  | total loss: \u001b[1m\u001b[32m1576873600.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1746 | loss: 1576873600.00000 - R2: 1.0002 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1747  | total loss: \u001b[1m\u001b[32m1419392768.00000\u001b[0m\u001b[0m | time: 1.034s\n",
      "| SGD | epoch: 1747 | loss: 1419392768.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1748  | total loss: \u001b[1m\u001b[32m1651029760.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1748 | loss: 1651029760.00000 - R2: 0.9965 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1749  | total loss: \u001b[1m\u001b[32m1486133376.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1749 | loss: 1486133376.00000 - R2: 0.9968 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1750  | total loss: \u001b[1m\u001b[32m1814262784.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1750 | loss: 1814262784.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1751  | total loss: \u001b[1m\u001b[32m1633043072.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1751 | loss: 1633043072.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1752  | total loss: \u001b[1m\u001b[32m1564854016.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1752 | loss: 1564854016.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1753  | total loss: \u001b[1m\u001b[32m1408575232.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1753 | loss: 1408575232.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1754  | total loss: \u001b[1m\u001b[32m1267924352.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1754 | loss: 1267924352.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1755  | total loss: \u001b[1m\u001b[32m1141338496.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1755 | loss: 1141338496.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1756  | total loss: \u001b[1m\u001b[32m1098584576.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1756 | loss: 1098584576.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1757  | total loss: \u001b[1m\u001b[32m988932736.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1757 | loss: 988932736.00000 - R2: 0.9993 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1758  | total loss: \u001b[1m\u001b[32m890246080.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1758 | loss: 890246080.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1759  | total loss: \u001b[1m\u001b[32m801428096.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1759 | loss: 801428096.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1760  | total loss: \u001b[1m\u001b[32m994870272.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1760 | loss: 994870272.00000 - R2: 1.0007 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1761  | total loss: \u001b[1m\u001b[32m895589824.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1761 | loss: 895589824.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1762  | total loss: \u001b[1m\u001b[32m891123776.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1762 | loss: 891123776.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1763  | total loss: \u001b[1m\u001b[32m802217984.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1763 | loss: 802217984.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1764  | total loss: \u001b[1m\u001b[32m1113424000.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1764 | loss: 1113424000.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1765  | total loss: \u001b[1m\u001b[32m1002288192.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1765 | loss: 1002288192.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1766  | total loss: \u001b[1m\u001b[32m1139181184.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1766 | loss: 1139181184.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1767  | total loss: \u001b[1m\u001b[32m1025469696.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1767 | loss: 1025469696.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1768  | total loss: \u001b[1m\u001b[32m992756608.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1768 | loss: 992756608.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1769  | total loss: \u001b[1m\u001b[32m893687552.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1769 | loss: 893687552.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1770  | total loss: \u001b[1m\u001b[32m881808960.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1770 | loss: 881808960.00000 - R2: 1.0001 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1771  | total loss: \u001b[1m\u001b[32m793834688.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1771 | loss: 793834688.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1772  | total loss: \u001b[1m\u001b[32m814679808.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1772 | loss: 814679808.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1773  | total loss: \u001b[1m\u001b[32m733418432.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1773 | loss: 733418432.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1774  | total loss: \u001b[1m\u001b[32m749474496.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1774 | loss: 749474496.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1775  | total loss: \u001b[1m\u001b[32m674733632.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1775 | loss: 674733632.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1776  | total loss: \u001b[1m\u001b[32m888255872.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1776 | loss: 888255872.00000 - R2: 1.0010 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1777  | total loss: \u001b[1m\u001b[32m799636864.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1777 | loss: 799636864.00000 - R2: 1.0008 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1778  | total loss: \u001b[1m\u001b[32m719879808.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1778 | loss: 719879808.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1779  | total loss: \u001b[1m\u001b[32m648098432.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1779 | loss: 648098432.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1780  | total loss: \u001b[1m\u001b[32m646471040.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1780 | loss: 646471040.00000 - R2: 1.0020 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1781  | total loss: \u001b[1m\u001b[32m582030528.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1781 | loss: 582030528.00000 - R2: 1.0017 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 1782  | total loss: \u001b[1m\u001b[32m626922752.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1782 | loss: 626922752.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1783  | total loss: \u001b[1m\u001b[32m564437120.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1783 | loss: 564437120.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1784  | total loss: \u001b[1m\u001b[32m796841216.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1784 | loss: 796841216.00000 - R2: 1.0030 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1785  | total loss: \u001b[1m\u001b[32m717363712.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1785 | loss: 717363712.00000 - R2: 1.0026 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1786  | total loss: \u001b[1m\u001b[32m763403456.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1786 | loss: 763403456.00000 - R2: 1.0000 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1787  | total loss: \u001b[1m\u001b[32m687269696.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1787 | loss: 687269696.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1788  | total loss: \u001b[1m\u001b[32m707051008.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1788 | loss: 707051008.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1789  | total loss: \u001b[1m\u001b[32m636552512.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1789 | loss: 636552512.00000 - R2: 0.9996 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1790  | total loss: \u001b[1m\u001b[32m958689536.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1790 | loss: 958689536.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1791  | total loss: \u001b[1m\u001b[32m863027200.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1791 | loss: 863027200.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1792  | total loss: \u001b[1m\u001b[32m890841536.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1792 | loss: 890841536.00000 - R2: 1.0006 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1793  | total loss: \u001b[1m\u001b[32m801963968.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1793 | loss: 801963968.00000 - R2: 1.0004 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1794  | total loss: \u001b[1m\u001b[32m1069312832.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1794 | loss: 1069312832.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1795  | total loss: \u001b[1m\u001b[32m962588160.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1795 | loss: 962588160.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1796  | total loss: \u001b[1m\u001b[32m966140160.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1796 | loss: 966140160.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1797  | total loss: \u001b[1m\u001b[32m869732736.00000\u001b[0m\u001b[0m | time: 1.012s\n",
      "| SGD | epoch: 1797 | loss: 869732736.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1798  | total loss: \u001b[1m\u001b[32m1108181760.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1798 | loss: 1108181760.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1799  | total loss: \u001b[1m\u001b[32m997570176.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1799 | loss: 997570176.00000 - R2: 0.9988 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1800  | total loss: \u001b[1m\u001b[32m975188480.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1800 | loss: 975188480.00000 - R2: 0.9983 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1801  | total loss: \u001b[1m\u001b[32m877876224.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1801 | loss: 877876224.00000 - R2: 0.9984 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1802  | total loss: \u001b[1m\u001b[32m909796224.00000\u001b[0m\u001b[0m | time: 1.016s\n",
      "| SGD | epoch: 1802 | loss: 909796224.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1803  | total loss: \u001b[1m\u001b[32m819023232.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1803 | loss: 819023232.00000 - R2: 0.9989 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1804  | total loss: \u001b[1m\u001b[32m832206080.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1804 | loss: 832206080.00000 - R2: 0.9977 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1805  | total loss: \u001b[1m\u001b[32m749192064.00000\u001b[0m\u001b[0m | time: 1.018s\n",
      "| SGD | epoch: 1805 | loss: 749192064.00000 - R2: 0.9978 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1806  | total loss: \u001b[1m\u001b[32m1329433344.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1806 | loss: 1329433344.00000 - R2: 0.9971 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1807  | total loss: \u001b[1m\u001b[32m1196696576.00000\u001b[0m\u001b[0m | time: 1.010s\n",
      "| SGD | epoch: 1807 | loss: 1196696576.00000 - R2: 0.9973 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1808  | total loss: \u001b[1m\u001b[32m1468566272.00000\u001b[0m\u001b[0m | time: 1.006s\n",
      "| SGD | epoch: 1808 | loss: 1468566272.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1809  | total loss: \u001b[1m\u001b[32m1321916288.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1809 | loss: 1321916288.00000 - R2: 0.9992 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1810  | total loss: \u001b[1m\u001b[32m1407817728.00000\u001b[0m\u001b[0m | time: 1.039s\n",
      "| SGD | epoch: 1810 | loss: 1407817728.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1811  | total loss: \u001b[1m\u001b[32m1267242496.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1811 | loss: 1267242496.00000 - R2: 0.9987 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1812  | total loss: \u001b[1m\u001b[32m1199865856.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1812 | loss: 1199865856.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1813  | total loss: \u001b[1m\u001b[32m1080085888.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1813 | loss: 1080085888.00000 - R2: 0.9997 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1814  | total loss: \u001b[1m\u001b[32m1056591424.00000\u001b[0m\u001b[0m | time: 1.008s\n",
      "| SGD | epoch: 1814 | loss: 1056591424.00000 - R2: 0.9979 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1815  | total loss: \u001b[1m\u001b[32m951138880.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1815 | loss: 951138880.00000 - R2: 0.9980 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1816  | total loss: \u001b[1m\u001b[32m944002496.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1816 | loss: 944002496.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1817  | total loss: \u001b[1m\u001b[32m849808832.00000\u001b[0m\u001b[0m | time: 1.009s\n",
      "| SGD | epoch: 1817 | loss: 849808832.00000 - R2: 0.9985 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1818  | total loss: \u001b[1m\u001b[32m823534656.00000\u001b[0m\u001b[0m | time: 1.011s\n",
      "| SGD | epoch: 1818 | loss: 823534656.00000 - R2: 0.9999 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n",
      "Training Step: 1819  | total loss: \u001b[1m\u001b[32m741387776.00000\u001b[0m\u001b[0m | time: 1.007s\n",
      "| SGD | epoch: 1819 | loss: 741387776.00000 - R2: 0.9998 | val_loss: 798799.81250 - val_acc: 0.9979 -- iter: 1167/1167\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "tff.reset_default_graph()\n",
    "r2 = tf.R2()\n",
    "net = tf.input_data(shape=[None, train_data.shape[1]])\n",
    "net = tf.fully_connected(net, 30, activation='linear')\n",
    "net = tf.fully_connected(net, 10, activation = 'linear')\n",
    "net = tf.fully_connected(net, 1, activation = 'linear')\n",
    "sgd = tf.SGD(learning_rate=0.1, lr_decay = 0.01, decay_step=100)\n",
    "net = tf.regression(net, optimizer=sgd, loss='mean_square', metric=r2)\n",
    "model = tf.DNN(net)\n",
    "\n",
    "model.fit(train_data, train_labels, show_metric=True, validation_set=0.2, shuffle = True, batch_size=1460, n_epoch=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_DNN = model.predict(test_data)\n",
    "preds_DNN = preds_DNN.reshape(-1,)\n",
    "output = pd.DataFrame({\"id\":ids})\n",
    "output[\"SalePrice\"] = preds_DNN\n",
    "#output_id = pd.DataFrame({\"id\":ids})\n",
    "output.to_csv('output2.csv', index=False)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
